\section{Causal Bayesian Networks}

A causal Bayesian network is an example of an identifiable causal theory. Given a measurable space $(X,\mathcal{X})$ where $X=\times_{i\in [N]} X^i$, a decision space $D = \times_{i\in [N]} X^i\cup\{*\}$ and a causal graph $\mathcal{G}$ over nodes $\mathbf{V}=\{V^i|i\in [N]\}$, the graph $\mathcal{G}$ maps each probability distribution $\mu\in \Delta(\mathcal{X})$ to a unique consequence $D\to \Delta(\mathcal{X})$.

The CBN convention is to call the elements of the decision space $D$ ``interventions'' and denote then with $P(\cdot|do(\cdot\cdot))$ notation. In the first definition below, we instead denote interventions with indices on the probability distribution, and introduce the special $*$ object to denote a passive intervention.

\begin{definition}[Causal Bayesian Network]\label{def:CBN}
The definition here follows \cite{pearl_causality:_2009}.

Consider a directed acyclic graph $\mathcal{G}$ with nodes $\mathbf{X}=\{\RV{X}^i|i\in [N]\}$, a measurable space $(E,\mathcal{E})$ and a set of random variables $\RV{X}^i:E\to X^i$ and let $X=\times_{i\in[N]} X^i$. For each $x\in \times_{i\in [N]} X^i\cup \{*\}$ suppose we have an \emph{interventional distribution} $P^{x}\in \Delta(\mathcal{E})$, and let the set of all such distributions be denoted $\mathbf{P}^{X\cup\{*\}}$. Let $P^*$ be the passive distribution given by the intervention $x = (*,...,*)$.

Given any $x\in X\cup\{*\}$ let $S(x)\subset[N]$ be the set of all indices $i$ such that $x^i\neq *$. The graph $\mathcal{G}$ is a causal Bayesian network compatible with $\mathbf{P}^{X\cup\{*\}}$ iff for all $x\in X$ and $S(x)\subset [N]$:
\begin{enumerate}
    \item $P^{x}_{\RV{X}}$ is compatible with $\mathcal{G}$ for all $x\in \cup_{i\in [N]} X^i\cup \{*\}$
    \item $P^x_{\RV{X}}(\RV{X}^{S(x)})=\delta_{x^{S(x)}}(\RV{X}^{S(x)})$
    \item For $i\in S^C$, $P^x_{\RV{X}}(\RV{X}^i|\PA{\mathcal{G}}{\RV{X}^i})=P^*_\RV{X}(\RV{X}^i|\PA{\mathcal{G}}{\RV{X}^i})$, $P^x$-almost surely
\end{enumerate}
\end{definition}

The above three conditions are sufficient that, given some graph $\mathcal{G}$ and $P^*_{\RV{X}}\in \Delta(\mathcal{X})$, one obtains a set of interventional distributions $\mathbf{P}^{X\cup\{*\}}$ that is unique if $\mathcal{E}$ is equal to the sigma algebra generated by the joint variable $\RV{X}$ (this follows from the truncated factorisation property given by \cite{pearl_causality:_2009}). That is, identifying the decision set $D$ with $\times_{i\in [N]} X^i\cup\{*\}$, a causal graph $\mathcal{G}$ induces a set of pairs of the form $(\mu,\kappa)$ where $\mu\in \Delta(\mathcal{X})$ and $\kappa\in \Delta(\mathcal{X})^D$. Theorem \ref{th:cbn_MK} establishes that the object $\kappa$ is a Markov kernel, so these pairs indeed  form a causal theory.

\begin{theorem}\label{th:cbn_MK}
Given a graph $\mathcal{G}$, a measurable set $(E,\mathcal{E})$ and a decision set $D=\times_{i\in [N]} X^i\cup\{*\}$, let $P^x\in \Delta(\mathcal{E})$ be an interventional distribution compatible with $\mathcal{G}$ as defined in Definition \ref{def:CBN}. 

Then the map $\kappa_{\mathcal{G}}:D\to \Delta(\mathcal{X})$ given by $x\mapsto P^x$ is a Markov kernel.
\end{theorem}

The proof is given in Appendix \ref{app:cbn_ct}.

\subsection{Extension of CBNs}

The causal theory associated with a given graph $\mathcal{G}$ is in general incomplete in that it does not associate a consequence with some distributions on $\Delta(\mathcal{X})$. In particular, if certain conditional independences are implied by $\mathcal{G}$, then probability distributions for which these conditional independences do not hold do not appear in the associated causal theory - only \emph{compatible} distributions are represented. This follows from condition 1 of Definition \ref{def:CBN}. While a theory doesn't have to feature a causal state for every possible distribution, it usually isn't reasonable to assume \emph{a priori} that certain conditional independences hold. In this case, it is necessary to extend the causal theory associated with $\mathcal{G}$ to cover distributions that are incompatible with $\mathcal{G}$.

The choice of how to extend the theory can be important. For discrete and jointly Gaussian distributions, conditional independences are associated with lower dimensional subspaces of the set of distributions\cite{meek_strong_1995}. Given causal theory $\mathscr{T}\subset$ and a $\sigma$-algebra $\mathcal{T}$, if we have a prior $\xi\in \Delta(\mathscr{H}\times \mathscr{K})$ such that the marginal $P^\xi(A)$ for $A\in \Delta(\mathscr{H})$ admits a density $P^\xi(A) = \int_A p^\xi(x)dx$ then this prior will assign 0 weight to any subset of $\mathscr{T}$ for which some conditional independence holds, and so for any $\mathcal{G}$ which is not fully connected the Bayes risk of any decision function $J$ is determined entirely by the extension of $\mathscr{T}$ to the set of distributions incompatible with $\mathcal{G}$.

An example follows. Suppose we have the following graph $\mathcal{G}$:

\begin{figure}
    \centering
     \begin{tikzpicture}[-latex,auto ,node distance =1 cm and 2cm ,on grid ,
    semithick ,
    vb/.style ={ circle ,top color =white , 
    draw , text=blue , minimum width =0.6 cm},
    kernel/.style={rectangle,draw}
    ]

    \node[vb] (A) {$A$};
    \node[vb] (B) [right = of A] {$B$};
    \node[vb] (C) [right = of B] {$C$};
    \draw (A) -- (B);
    \draw (B) -- (C);
    \end{tikzpicture}
    \caption{Simple causal Bayesian network $\mathcal{G}$}
    \label{fig:simple_cbn}
\end{figure}

Associated with this graph is the sample space $(E,\mathcal{E})=(\{0,1\}^3,\mathscr{P}(\{0,1\}^3)$ where $\mathscr{P}$ denotes the power set, and random variables $\RV{A},\RV{B}$ and $\RV{C}$ taking values in $\{0,1\}$. The set of possible distributions $\Delta(\mathcal{E})$ can be identified with the probability simplex in $\mathbb{R}^8$. For simplicity, suppose that only $A$ can be intervened on; that is, the decision space $D=A\cup\{*\}$ with the decision $\RV{D}_A=x$ for $x\in A$ having the usual interpretation as a hard intervention on $A$. We could alternatively assign infinite costs to interventions on $B$ and $C$, but this adds unnecessary complexity.

$\mathcal{G}$ implies $\RV{A}\CI \RV{C} | \RV{B}$. We have a theory $\mathscr{T}_{\mathcal{G}}$ associated with the graph $\mathcal{G}$ containing the state
\begin{align}
    (\nu,x\mapsto P^\nu(\RV{B}|\RV{A}=x) ) \label{eq:ocbn}
\end{align}
for every compatible $\nu$ . 

Consider two options for extending this to distributions $\nu$ incompatible with $\mathcal{G}$:
\begin{itemize}
    \item $\mathscr{T}_{\mathcal{G}}'$ assigns the causal states given by the union over all DAGs on the set of nodes $\{A, B, C\}$
    \item $\mathscr{T}_{\mathcal{G}}^\square$ assigns the causal states given by the union over all supergraphs of $\mathcal{G}$ on $\{A, B, C\}$
    \item $\mathscr{T}_{\mathcal{G}}^\circ$ assigns the causal states given by $\mathcal{G}$ and ignores the inconsistency of $\nu$
\end{itemize}

In the first theory, for every DAG featuring $A\to B$ there is a DAG featuring $B\to A$; in addition, there are a number of DAGs with no arrow between $A$ and $B$. Therefore any prior $\xi$ that admits a density over $\Delta(\mathcal{E})$ and assigns equal weight to each causal state in $\mathcal{T}$ featuring the same distribution will generate a posterior that assigns a weight of more than 0.5 to the possibility that the marginal distribution of $\RV{B}$ is independent of whatever decision $\RV{D}_A$ is chosen. This remains true even if the observed data consist of a very large number of IID observations distributed according to some $\pi$ for which it is indeed holds that $\RV{A} \CI_\pi \RV{C} | \RV{B}$.

The second theory, on the other hand, yields a set consequences that are ``close'' to the consequence given by the original graph $\mathcal{G}$ provided the distribution $\mu$ is sufficiently ``close'' to a distribution $\nu$ for which $\RV{A} \CI_\nu \RV{C} | \RV{B}$. Note that, marginalising over $\RV{A}$ and $\RV{C}$ and ignoring the passive action, the theory $\mathscr{T}^\square_{\mathcal{G}}$ associates two consequences with every incompatible $\mu$:
\begin{align}
    &(\mu,x\mapsto P^\mu(\RV{B}|\RV{A}=x) )\label{eq:cbn_s1}\\
    &(\mu,x \mapsto \sum_c P^\mu(\RV{B}|\RV{A}=x,\RV{C}=c)P^\mu(\RV{C}=c)) \label{eq:cbn_s2}
\end{align}

Note that \ref{eq:cbn_s1} matches the pattern for states in the original graph \ref{eq:ocbn}. Define the consequences $\kappa^\circ := x\mapsto P^\mu(\RV{B}|\RV{A}=x)$ and $\kappa^\square:= x \mapsto \sum_c P^\mu(\RV{B}|\RV{A}=x,\RV{C}=c)P^\mu(\RV{C}=c))$, leaving the $\mu$-dependence implicit.

Consider some $\mu$ for which $\RV{A} \CI_\mu \RV{C} | \RV{B}$ holds approximately. That is, for some $\epsilon>0$
\begin{align}
    \left|\sum_c P^\mu(\RV{B}|\RV{A}=x,\RV{C}=c)P^\mu(\RV{C}=c)-P^\mu(B|A=x)\right| &< \epsilon \label{eq:app_ci}
\end{align}

Suppose that we have some loss such that $L(\rho)= \mathbb{E}_\rho[\RV{B}] + k$. Noting that $\mathscr{T}^\circ_\mathcal{G}$ and $\mathscr{T}^\square_\mathcal{G}$ agree on $\kappa^\circ$, we can bound the disagreement between the two theories with
\begin{align}
    |R(J,\kappa^\square,\mu) - R(J,\kappa^\circ,\mu)| &=  \left|\mathbb{E}_{\mu J \kappa^\square} [\RV{B}] - \mathbb{E}_{\mu J \kappa^\circ} [\RV{B}]\right|\\
        &\leq \max_x \left|\sum_c P^\mu(\RV{B}|\RV{A}=x,\RV{C}=c)P^\mu(\RV{C}=c)-P^\mu(B|A=x) \right|\\
        &< \epsilon
\end{align}

The stipulation that the prior $\xi$ was such that the marginal distribution over $\Delta(\mathcal{E})$ admitted a density may be controversial. It is consistent with the notion that ``there are no true parametric zeros'' endorsed by many statisticians (for example  \cite{gelman_bayesian_2010,meehl_theory-testing_1967,berkson_difficulties_1938}). Furthermore, if conditional independences rarely hold precisely, then learners based on the theory $\mathscr{T}_{\mathcal{G}}'$ may converge to a state of skepticism even given a prior that assigns nonzero weight to the set of compatible probability distributions because the data are drawn from a distribution from which this independence does not hold.

\section{Potential Outcomes}

Potential outcomes provide an alternative means to discuss causal effects 