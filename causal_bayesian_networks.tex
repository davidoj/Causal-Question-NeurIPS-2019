\section{Causal Bayesian Networks}

There is a natural identification of a causal Bayesian network (CBN) associated with a graph $\mathcal{G}$ and a causal theory $\mathscr{T}_{\mathcal{G}}$ where we consider \emph{do}-interventions defined with respect to the CBN to correspond to decisions in the causal theory. The CBN convention is to denote an interventional distribution with $P(\cdot|do(\RV{X}^i=a))$ notation. Here we associate every allowable intervention with the decision space $(D,\mathcal{D})$ equipped with random variables $\{\RV{D}^i\}_{i\in[N]}$ such that for $y\in D$, $P^y(\cdot) := P(\cdot|[do(\RV{X}^j=\RV{D}^i(y))]_{i\in N})$. The special element $*$ corresponds to a passive intervention which is denoted by the absence of a $do()$ statement in regular CBN notation.

\begin{definition}[Causal Bayesian Network]\label{def:CBN}
Consider a directed acyclic graph $\mathcal{G}$ with nodes $\mathbf{X}=\{\RV{X}^i|i\in [N]\}$, a measurable space $(E,\mathcal{E})$ and a set of random variables $\RV{X}^i:E\to X^i$ and $X=\times_{i\in[N]} X^i$ along with decision space $(D,\mathcal{D})$ and random variables $\{\RV{D}^i\}_{i\in [N]}$ where $\RV{D}^i:D\to X^i\cup\{*\}$.

Given any $y\in D$ let $S(y)\subset[N]$ be the set of all indices $i$ such that $\RV{D}^i(y)\neq *$. Let $\mathscr{H}_\mathcal{G}\subset\Delta(\mathcal{X})$ be the set of distributions compatible with $\mathcal{G}$. Given arbitrary $\mu\in \mathscr{H}_\mathcal{G}$ and $y\in D$ the $\mathcal{G},\mu,y$-interventional distribution denoted $\mu^{\mathcal{G},y}$ is given by the following two conditions:
\begin{enumerate}
    \item $\mu^{\mathcal{G},y}\splitter{0.15}(\otimes_{i\in S(y)} F_{\RV{X}^i})=\prod_{i\in S(y)} \delta_{\RV{D}^i(y)} F_{\RV{X}^i}$
    \item $\mu^{\mathcal{G},y}_{\PA{\mathcal{G}}{\RV{X}^i}} \splitter{0.15}(\otimes_{i\not\in S(y)} F_{\RV{X}^i})=\mu_{\PA{\mathcal{G}}{\RV{X}^i}}\splitter{0.15}(\otimes_{i\not\in S(y)} F_{\RV{X}^i}) $, $\mu^{\mathcal{G},y}$-almost surely
\end{enumerate}
\end{definition}

$\PA{\mathcal{G}}{\RV{X}^i}$ are the parents of $\RV{X}^i$ with respect to the graph $\mathcal{G}$ and $\mu_{\PA{\mathcal{G}}{\RV{X}^i}}$ is the conditional probability with respect to $\mu$ and the $\sigma$-algebra generated by the set $\PA{\mathcal{G}}{\RV{X}^i}$. Recall that $\mu \splitter{0.15}(\otimes_{i\not\in S(y)} F_{\RV{X}^i})$ is the joint distribution of $\{\RV{X}^i|i\in S(y)\}$.

To establish that the map $\kappa^{\mathcal{G},\mu}:D\to \Delta(\mathcal{X})$ given by $y\mapsto \mu^{\mathcal{G},y}$ is a consequence map, we must shown that it is measurable with respect to the $\sigma$-algebra generated by the set of variables $\RV{D}^i$; this is shown by Theorem \ref{th:cbn_MK}. 

Defining $\mathscr{H}_{\mathcal{G}}\subset\Delta(\mathcal{X})$ to be the set of distributions compatible with $\mathcal{G}$, the set of pairs $\{(\mu, \kappa^\mu)|\mu\in \mathscr{H}_{\mathcal{G}}\}$ is the causal theory $\mathscr{T}_\mathcal{G}$.

\begin{theorem}\label{th:cbn_MK}
Given a graph $\mathcal{G}$, a measurable set $(E,\mathcal{E})$ and a decision set $(D,\mathcal{D})$, let $\mu^D_\mathcal{G}$ be the set of interventional distributions compatible with $\mathcal{G}$ as defined in Definition \ref{def:CBN}. 

Then the map $\kappa_{\mathcal{G}}:D\to \Delta(\mathcal{X})$ given by $y\mapsto \mu^y$ is a Markov kernel.
\end{theorem}

The proof is given in Appendix \ref{app:cbn_ct}.



\subsection{Extending the theory induced by a CBN}

The causal theory $T_{\mathcal{G}}$ defined above associates a consequence with every probability distribution compatible with $\mathcal{G}$ but not every probability distribution in $\Delta(\mathcal{X})$ (this follows from condition 1 of Definition \ref{def:CBN}). It may be the case that it is not considered reasonable to assume \emph{a priori} that the conditional independences implied by $\mathcal{G}$ hold in the observed data - indeed, such an assumption is usually considered unreasonable.

We therefore need to extend the theory $\mathscr{T}_{\mathcal{G}}$ to account for the possibility of incompatible distributions. There are many choices for how this may be done, none are obviously correct and different extensions will generally induce very different risk sets, as shown by Example \ref{ex:extn_cbn}.

\begin{example}[Extension of a CBN]\label{ex:extn_cbn}

A graph $\mathcal{G}$ is given in figure \ref{fig:simple_cbn}. Suppose the three associated random variables $\RV{A}$, $\RV{B}$ and $\RV{C}$ each take values in $\{0,1\}$ and suppose (unrealistically) we know the following holds for all $\mu$ in the set of possible joint distributions $\mathscr{H}$:
\begin{align}
    \mu_{\{\RV{A}\}} F_{\RV{B}} &= \iota \qquad \text{for some }\iota:\{0,1\}^3\to \Delta(\mathcal{B})\\
    \max_{x\in\{0,1\}^3,y\in\{0,1\}}\left|\mu_{\{\RV{A},\RV{C}\}} F_{\RV{B}}(x;\{y\}) - \iota(x;\{y\}) \right| &< \epsilon \label{eq:app_ci}
\end{align}

That is, we know the conditional probability of $\RV{B}$ given $\RV{A}$ and we know that $\RV{B}$ is ``almost'' independent of $\RV{A}$ given $\RV{C}$.

\begin{figure}
    \centering
     \begin{tikzpicture}[-latex,auto ,node distance =1 cm and 2cm ,on grid ,
    semithick ,
    vb/.style ={ circle ,top color =white , 
    draw , text=blue , minimum width =0.6 cm},
    kernel/.style={rectangle,draw}
    ]

    \node[vb] (A) {$A$};
    \node[vb] (B) [right = of A] {$B$};
    \node[vb] (C) [right = of B] {$C$};
    \draw (A) -- (B);
    \draw (B) -- (C);
    \end{tikzpicture}
    \caption{Simple causal Bayesian network $\mathcal{G}$}
    \label{fig:simple_cbn}
\end{figure}

Suppose that only interventions on $\RV{A}$ are possible and only $\RV{B}$ is relevant to the utility. We restrict our attention to the subset of decisions $D'=\{y|\RV{D}_\RV{B}(y)=\RV{D}_\RV{C}(y)=*\}$ and define the causal theory $\mathscr{T}_{\mathcal{G}}$ from $\mathcal{G}$ and the set of compatible distributions $\mathscr{H}_\mathcal{G}\subset\mathscr{H}$

\begin{align}
    (\mu,y\mapsto \mu_{\mathcal{G}}^x ) \qquad \mu\in\Delta(\mathcal{E}):\RV{A}\CI_{\mu}\RV{B}|\RV{C} \label{eq:ocbn}
\end{align}

Consider three options for extending this to distributions $\nu$ incompatible with $\mathcal{G}$, noting that one could imagine many additional possibilities:
\begin{itemize}
    \item $\mathscr{T}_{\mathcal{G}}^\circ$ assigns the causal states given by the union over all DAGs on the set of nodes $\{A, B, C\}$
    \item $\mathscr{T}_{\mathcal{G}}^\subset$ assigns the causal states given by the union over all graphs $\mathcal{G}'$ on $\{A, B, C\}$ such that of $\mathcal{G}\subset \mathcal{G}'$ (that is, every edge in $\mathcal{G}$ is also in $\mathcal{G}'$)
    \item $\mathscr{T}_{\mathcal{G}}^\mathrm{stubborn}$ assigns the causal states given by $\mathcal{G}$ to all distributions in $\Delta(\mathcal{E})$
\end{itemize}

Finally, we will suppose some prior $\xi$ on $\Delta(\mathcal{E})$ that is absolutely continuous with respect to the Lebesgue measure on the probability simplex $\{[p_1,..p_8]|\sum_i p_i=1,0\leq p_i \leq 1\}\subset\mathbb{R}^8$ that parametrises $\Delta(\mathcal{E})$ and, whenever a theory admits multiple consequences for a given distribution, assigns these consequences equal weight. Note that the set of distributions $\mathscr{H}_{\mathcal{G}}$ for which $\RV{A}\CI \RV{C} | \RV{B}$ holds therefore has measure 0 with respect to $xi$\cite{meek_strong_1995}. The Bayes risk of any decision function $J\in \mathscr{J}$, therefore, will depend entirely on the chosen theory's behaviour on $\mathscr{H}_{\mathcal{G}}^c$.

Noe that the theory $\mathscr{T}_{\mathcal{G}}^\circ$ admits consequences on $\mathscr{H}_{\mathcal{G}}^c$ associated with:
\begin{enumerate}
    \item DAGs featuring $A\to B$
    \item DAGs featuring $B\to A$ (in equal number to the first set of DAGs)
    \item DAGs featuring no arrow between $B$ and $A$ (featuring one additional DAG to the first two types)
\end{enumerate} 
Therefore the calculation for the Bayes risk given by $\mathscr{T}_{\mathcal{G}}^\circ$ will always feature a weight of more than $\tfrac{2}{3}$ on the possibility that the marginal distribution of $\RV{B}$ is independent of whatever decision $\RV{D}_A$ is chosen. For many priors this will be very different to $\mathscr{T}_{\mathcal{G}}^\mathrm{stubborn}$.

The theory $\mathscr{T}_{\mathcal{G}}^\subset$, on the other hand, yields a set consequences that are ``close'' to the consequence admitted by the original theory $\mathscr{T}_{\mathcal{G}}$ if the distribution $\nu$ is sufficiently ``close'' to a distribution $\mu$ for which $\RV{A} \CI_\mu \RV{C} | \RV{B}$. Marginalising over $\RV{A}$ and $\RV{C}$ and ignoring the passive action, the theory $\mathscr{T}^\subset_{\mathcal{G}}$ associates exactly two consequences with every incompatible $\nu$:
\begin{align}
    &(\nu,x\mapsto \nu(\RV{B}|\RV{A}=x) )\label{eq:cbn_s1}\\
    &(\nu,x \mapsto \sum_c \nu(\RV{B}|\RV{A}=x,\RV{C}=c)\nu(\RV{C}=c)) \label{eq:cbn_s2}
\end{align}

Note that \ref{eq:cbn_s1} is the same pattern as the states given by $\mathscr{T}_{\mathcal{G}}$ - see Eq. \ref{eq:ocbn}. Define the consequences $\kappa^\mathcal{G}_\nu := x\mapsto \nu(\RV{B}|\RV{A}=x)$ and $\kappa^\subset_\nu:= x \mapsto \sum_c \nu(\RV{B}|\RV{A}=x,\RV{C}=c)\nu(\RV{C}=c))$.

Consider some $\nu$ for which $\RV{A} \CI_\nu \RV{C} | \RV{B}$ holds approximately. That is, for some $\epsilon>0$ and all $x\in\{0,1\}$
\begin{align}
    \left|\sum_{c\in\{0,1\}} \nu(\RV{B}|\RV{A}=x,\RV{C}=c)\nu(\RV{C}=c)-\nu(B|A=x)\right| &< \epsilon \label{eq:app_ci}
\end{align}

Recalling the utility $U(\zeta)=\mathbb{E}_\zeta[\RV{B}]$, and that $\mathscr{T}^\subset_\mathcal{G}$ and $\mathscr{T}^\mathrm{stubborn}_\mathcal{G}$ agree on $\kappa^\mathcal{G}$, we can bound the disagreement between the two theories with
\begin{align}
    |R(J,\kappa^\subset,\nu) - R(J,\kappa^\mathcal{G},\nu)| &=  \left|\mathbb{E}_{\nu J \kappa^\subset} [\RV{B}] - \mathbb{E}_{\nu J \kappa^\mathcal{G}} [\RV{B}]\right|\\
        &\leq \max_x \left|\sum_c P^\nu(\RV{B}|\RV{A}=x,\RV{C}=c)P^\nu(\RV{C}=c)-P^\nu(B|A=x) \right|\\
        &< \epsilon
\end{align}

Given the assumption of a prior $\xi$ that is uniformly continuous with respect to the Lebesgue measure, $\mathscr{T}_{\mathcal{G}}^\subset$ but not $\mathscr{T}_{\mathcal{G}}^\circ$ gives us the property that for distributions where $\RV{A} \CI_\nu \RV{C} | \RV{B}$ almost holds, the theory $\mathscr{T}_{\mathscr{G}}^{\mathrm{stubborn}}$ can yield ``almost'' correct results. Note that $\mathscr{T}_{\mathscr{G}}^{\mathrm{stubborn}}$ is precisely the theory that actually uses the graph $\mathcal{G}$ to calculate consequences.
\end{example}

This example is somewhat contrived. The stipulation that the prior $\xi$ is absolutely continuous with respect to the Lebesgue measure may be considered strong, as may the assumption of equal weight given to different consequences where multiple consequences were allowed. Bayesian methods of graph learning, for example, may use priors that are not absolutely continuous with respect to the Lebesgue measure \cite{chickering_optimal_2003}. At the same time, it is not obviously unreasonable; it is consistent with the notion that ``there are no true parametric zeros'' endorsed by many statisticians (for example  \cite{gelman_bayesian_2010,meehl_theory-testing_1967,berkson_difficulties_1938}) and a similar assumption is invoked in support of the faithfulness hypothesis in causal graph learning \cite{meek_strong_1995}. A prior that assigns nonzero weight to the set of distributions for which $\RV{A} \CI_\nu \RV{C} | \RV{B}$ will also not resolve the question of whether or not it is reasonable to calculate consequences according to $\mathcal{G}$ if $\RV{A} \CI_\nu \RV{C} | \RV{B}$ is in fact violated by a very small amount.

Even in the apparently simple case of a known causal graph $\mathcal{G}$, determining the associated causal theory is not a simple matter. The standard practice of computing consequences according to $\mathcal{G}$ if we do not reject the hypothesis that the observed data are compatible with $\mathcal{G}$ appears to rely on a prior with particular properties, a particular extension of the associated theory $\mathscr{T}_{\mathcal{G}}$, or both.