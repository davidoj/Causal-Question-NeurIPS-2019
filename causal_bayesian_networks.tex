%!TEX root = main.tex

\section{Causal Bayesian Networks}

We formulate causal Bayesian networks following \citet{pearl_causality:_2009}, relying on the source to define the key concept of \emph{compatibility} as it is not a central concern here. We argue for the following claim: a causal Bayesian network represents a causal theory. 

Suppose we have a set of ``interventions'' $R$ which factorises as $R=\times_{i\in [n]} \{\#\}\cup X^i$ for some $n\in \mathbb{N}$, collection of sets $\{X^i\}_{i\in [n]}$ and distinguished element $\#\not\in R^i$ for any $i$. Suppose we also have a measurable space $E$ and set of random variables $\{\RV{X}^i|i\in \mathbb{N}\}$ such that $\RV{X}^i:E\to X^i$. To explain this setup, an element $(x^0,\#,...,\#,\#)\in R$ identifies a do intervention where the only non-$\#$ component $x^0$ is the ``active'' element and the components taking the value $\#$ are ``do-nothing'' elements. Thus $(x^0,\#,...,\#,\#)$ corresponds to $do(\RV{X}^0=x^0)$ where occurrences of the passive elements are ommitted. Denote by $\underline{\#}$ the element of $R$ consisting entirely of $\#$ (equivalently, $do()$).

For $n\in \mathbb{N}$, a directed acyclic graph (DAG) of degree $n$ is a graph $\mathcal{G}=(V,A)$ where $V$ is a set of vertices such that $|V|=n$ and $A\subset V\times V$ is a set of directed edges (``arrows'') such that $A$ induces no cycles (for a definition of cycles see \citet{pearl_causality:_2009}). 

Strictly, we are considering labeled graphs $\mathcal{G}$ and sets $\{\RV{X}^i\}_{[n]}$ of random variables. That is, we suppose there is a bijective correspondence between graph nodes and random variables in $\{\RV{X}^i\}_{[n]}$ and, leaning on this correspondence, we simply label nodes of $\mathcal{G}$ with the random variables.

We also suppose we have surjective $h:R\to \mathscr{P}([n])$ such that $h:(x^0,...,x^n)\mapsto \{i|x^i\neq \#\}$. That is, $h$ picks out the active components of $r$. Define $\RV{X}^{i\prime}:R\to \{\#\}\cup X^i$ by the function returning the $i$-th element of $r$ for $r\in R$. We identify primed and unprimed random variables in the obvious way.

Let $\Pi_{\RV{X}^i} := \delta_{\pi_{\RV{X}^i}}$ be the Markov kernel that takes the marginal distribution of $\RV{X}^i$, and denote by $\mathbf{P}_{r|A}\Pi_{\RV{X}^i}:E\to \Delta(\mathcal{X}^i)$ the conditional probability of $\RV{X}^i$ given the set of random variables $A$.

\begin{definition}[Causal Bayesian Network]\label{def:CBN}

Given discrete $R$, $E$, $\mathbf{P}:R\to \Delta(\mathcal{E})$ and $\{\RV{X}^i\}_{i\in [n]}$, a Causal Bayesian Network (CBN) compatible with $\mathbf{P}$ is a directed acyclic graph (DAG) $\mathcal{G}$ of degree $n$ such that for all $r\in R$

\begin{enumerate}
    \item $\mathbf{P}_r$ is compatible with $\mathcal{G}$ (see \citet{pearl_causality:_2009})
    \item For all $i\in h(r)$, $\mathbf{P}_r \Pi_{\RV{X}^i}=\delta_{\RV{X}^{i\prime}(r)}$
    \item For all $i\not \in h(r)$, $\mathbf{P}_{r|\PA{\mathcal{G}}{\RV{X}^i}} \Pi_{\RV{X}^i}=\mathbf{P}_{\underline{\#}|\PA{\mathcal{G}}{\RV{X}^i}}\Pi_{\RV{X}^i} $, $\mathbf{P}_{\underline{\#}}$-almost surely
\end{enumerate}
\end{definition}

This definition differs slightly from \citet{pearl_causality:_2009} in that $\mathbf{P}$ is a Markov kernel rather than a set of labeled elements of $\Delta(\mathcal{E})$.

Given a graph $\mathcal{G}$ and a measure $\mu\in \Delta(\mathcal{E})$ compatible with $\mathcal{G}$ we can define a class of stochastic maps $\mathscr{K}\subset \Delta(\mathcal{E})^V$ such that every $\mathbf{P}\in \mathscr{K}$ is compatible with $\mathcal{G}$ and $\mathbf{P}(\#)=\mu$. Let the notation $\mathcal{G}(\mu)$ stand for the set $\mathscr{K}$ as defined here; note that $\mathcal{G}(\mu)$ is in general set-valued. 

We have from this definition for any $r\in V$ the \emph{truncated factorisation} property:
\begin{align}
	&P_r F_{\mathbf{X}}(A) =\nonumber\\
	 &\prod_{i\in h(r)} \delta_{\RV{X}^{i\prime} (r)} (\RV{X}^i(A)) \sum_{a\in A} \prod_{i\not \in h(r)} \mathbf{P}_{\#|\PA{\mathcal{G}}{\RV{X}^i}} \Pi_{\RV{X}^i} (a;\{\RV{X}^i(a)\})\label{eq:trunc_fac}
\end{align}
As a consequence of the existence of conditional probability for standard measurable spaces, provided $\mu$ is compatible with $\mathcal{G}$ we have that the right hand side of\ref{eq:trunc_fac} exists, and so $\mathcal{G}(\mu)$ is non-empty. If $\mu$ is positive definite this relationship is functional; in such a case we could treat $\mathcal{G}(\mu)$ as a function from $\Delta(\mathcal{E})$ to interventional maps $\mathbf{P}$.

Suppose we define some arbitrary hypothesis class $\mathscr{H}^{\mathcal{G}}\subset\Delta(\mathcal{E})$ of possible observed distributions. Then let $\Theta:=\{(\mu,\mathbf{P})|\mu\in\mathscr{H},\mathbf{P}\in \mathcal{G}(\mu)\}$ and define $\mathbf{T}^\mathscr{G}:\Theta\times R \to \Delta(\mathcal{E}^2)$ by $(\mu,\mathbf{P},r)\mapsto \mu\otimes \mathbf{P}_r$. It is natural to consider $\mathbf{T}^\mathscr{G}$ the causal theory represented by $\mathcal{G}$ for two reasons: first, it is a natural construction from the definition of a CBN if we take the set of possible $do$-interventions to be the set of decisions for $\mathbf{T}$. Secondly, if we take $\mathscr{H}=\Delta(\mathcal{E})$ then the map from DAGs to causal theories is injective (this is in contrast to, for example, the map from DAGs to probability distributions as in ordinary Bayesian networks\citep{bishop_pattern_2006}).

\begin{theorem}[The map $\mathcal{G}\mapsto \mathbf{T}^\mathcal{G}$ is injective]
For DAGs $\mathcal{G}$, $\mathcal{G}'$ on the same set of RV's $\{\RV{X}^i\}_{[n]}$, $\mathcal{G}\neq \mathcal{G}'\implies \mathscr{T}^{\mathcal{G}}\neq \mathscr{T}^{\mathcal{G}'}$ if these theories are induced by a complete hypothesis class.
\end{theorem}

\begin{proof}
$\mathcal{G}$ and $\mathcal{G}'$ must disagree on at least one parental set. Suppose this is on the parents of $\RV{X}^i$. Choose some $\mu$ such that $\mu_{|\PA{\mathcal{G}}{\RV{X}^i}}\Pi_{\RV{X}^i} \neq  \mu_{|\PA{\mathcal{G}'}{\RV{X}^i}}\Pi_{\RV{X}^i}$. By the non-equality of these conditional probabilities there are some $r,r'$ such that $h(r) = h(r') = \PA{\mathcal{G}}{\RV{X}^i}\cup \PA{\mathcal{G}'}{\RV{X}^i}$, $\PA{\mathcal{G}}{\RV{X}'^i}(r) = \PA{\mathcal{G}}{\RV{X}'^i}(r')$ but $\mathbf{P}^{\mathcal{G}'}_r \Pi_{\RV{X}^i} \neq \mathbf{P}^{\mathcal{G}'}_{r'} \Pi_{\RV{X}^i}$, but we also have $\mathbf{P}^{\mathcal{G}}_r \Pi_{\RV{X}^i} = \mathbf{P}^{\mathcal{G}}_{r'} \Pi_{\RV{X}^i}$ by equality of $r$ and $r'$ on $\PA{\mathcal{G}}{\RV{X}^i}$. Thus $\mathbf{T}^\mathcal{G}\neq \mathbf{T}^\mathcal{G}$.
\end{proof}

\subsection{Decisions and Interventions in a Causal Bayesian Network}

The causal theory $\mathbf{T}^\mathcal{G}$ associated with a CBN $\mathcal{G}$ features a large number of decisions. Given some utility function $u:\times_{i\in [n]} X^i\to \mathbb{R}$, there is always a decision that fixes the values of all $\RV{X}^i$ deterministically to a value maximising $u$, if such a maximum exists. Clearly, for a practical decision problem $\mathbf{T}^\mathcal{G}$ is inappropriate. There may be some cases where there is an obvious alternative theory $\mathbf{T}^\mathcal{G\prime}$ that features, for example, decisions limited to interventions on particular variables. For now, it is sufficient to note that to actually solve a decision problem, it is necessary to move from the theory $\mathbf{T}^\mathcal{G}$ to a modified theory.