\section{Causal Bayesian Networks}

A causal Bayesian network is an example of an identifiable causal theory. Given a measurable space $(X,\mathcal{X})$ where $X=\times_{i\in [N]} X^i$, a decision space $D = \times_{i\in [N]} X^i\cup\{*\}$ and a causal graph $\mathcal{G}$ over nodes $\mathbf{V}=\{V^i|i\in [N]\}$, the graph $\mathcal{G}$ maps each probability distribution $\mu\in \Delta(\mathcal{X})$ to a unique consequence $D\to \Delta(\mathcal{X})$.

The CBN convention is to call the elements of the decision space $D$ ``interventions'' and denote then with $P(\cdot|do(\cdot\cdot))$ notation. In the first definition below, we instead denote interventions with indices on the probability distribution, and introduce the special $*$ object to denote a passive intervention.

\begin{definition}[Causal Bayesian Network]\label{def:CBN}
The definition here follows \cite{pearl_causality:_2009}.

Consider a directed acyclic graph $\mathcal{G}$ with nodes $\mathbf{X}=\{\RV{X}^i|i\in [N]\}$, a measurable space $(E,\mathcal{E})$ and a set of random variables $\RV{X}^i:E\to X^i$ and let $X=\times_{i\in[N]} X^i$. For each $x\in \times_{i\in [N]} X^i\cup \{*\}$ suppose we have an \emph{interventional distribution} $P^{x}\in \Delta(\mathcal{E})$, and let the set of all such distributions be denoted $\mathbf{P}^{X\cup\{*\}}$. Let $P^*$ be the passive distribution given by the intervention $x = (*,...,*)$.

Given any $x\in X\cup\{*\}$ let $S(x)\subset[N]$ be the set of all indices $i$ such that $x^i\neq *$. The graph $\mathcal{G}$ is a causal Bayesian network compatible with $\mathbf{P}^{X\cup\{*\}}$ iff for all $x\in X$ and $S(x)\subset [N]$:
\begin{enumerate}
    \item $P^{x}_{\RV{X}}$ is compatible with $\mathcal{G}$ for all $x\in \cup_{i\in [N]} X^i\cup \{*\}$
    \item $P^x_{\RV{X}}(\RV{X}^{S(x)})=\delta_{x^{S(x)}}(\RV{X}^{S(x)})$
    \item For $i\in S^C$, $P^x_{\RV{X}}(\RV{X}^i|\PA{\mathcal{G}}{\RV{X}^i})=P^*_\RV{X}(\RV{X}^i|\PA{\mathcal{G}}{\RV{X}^i})$, $P^x$-almost surely
\end{enumerate}
\end{definition}

The above three conditions are sufficient that, given some graph $\mathcal{G}$ and $P^*_{\RV{X}}\in \Delta(\mathcal{X})$, one obtains a unique set of interventional distributions $\mathbf{P}_{\mathcal{G}}^{X\cup\{*\}}$ (this follows from the truncated factorisation property given by \cite{pearl_causality:_2009}). Identifying the decision set $D$ with $\times_{i\in [N]} X^i\cup\{*\}$, the map $\kappa^P_\mathcal{G}:D\to \Delta(\mathcal{X})$ given by $d\mapsto P^d_{\mathcal{G}}$ looks like a consequence map, which in fact it is (as established by Theorem \ref{th:cbn_MK}). Defining $\mathscr{H}_{\mathcal{G}}\subset\Delta(\mathcal{X})$ to be the set of distributions compatible with $\mathcal{G}$, the set of pairs $\{(P^*_{\RV{X}}, \kappa^P_\mathcal{G})|P^*_{\RV{X}}\in \mathscr{H}_{\mathcal{G}}\}$ is then a causal theory $\mathscr{T}_\mathcal{G}$.

\begin{theorem}\label{th:cbn_MK}
Given a graph $\mathcal{G}$, a measurable set $(E,\mathcal{E})$ and a decision set $D=\times_{i\in [N]} X^i\cup\{*\}$, let $P^x\in \Delta(\mathcal{E})$ be an interventional distribution compatible with $\mathcal{G}$ as defined in Definition \ref{def:CBN}. 

Then the map $\kappa_{\mathcal{G}}:D\to \Delta(\mathcal{X})$ given by $x\mapsto P^x$ is a Markov kernel.
\end{theorem}

The proof is given in Appendix \ref{app:cbn_ct}.

\subsection{Extending the theory induced by a CBN}

The causal theory $T_{\mathcal{G}}$ defined above associates a consequence with every probability distribution compatible with $\mathcal{G}$ but not every probability distribution in $\mathcal{G}$ (this follows from condition 1 of Definition \ref{def:CBN}). It may be the case that it is not considered reasonable to assume \emph{a priori} that the conditional independences implied by $\mathcal{G}$ hold in the observed data - indeed, such an assumption is usually considered unreasonable. It has been suggested by \cite{pearl_causality:_2009} that if a conditional independence implied by a graph $\mathcal{G}$ is not considered to hold by a test on the given data, the graph $\mathcal{G}$ should be considered to be false. This is perhaps akin to setting up an SCDP $\langle (\mathscr{T}_{\mathcal{G}},E,\RV{X}),D,(U,F)\rangle$ featuring the theory $\mathscr{T}_{\mathcal{G}}$. Given some $\nu\not\in \mathscr{H}_{\mathcal{G}}$, the question ``what is the risk associated with $J\in\mathscr{J}$ given that we know the data are distributed according to $\nu$'' cannot be answered; the set $\{R(J,\kappa,\nu)|(\nu,\kappa)\in\mathscr{T}_{\mathcal{G}}\}$ is empty. The problem so defined is therefore unsuitable for comparing different decision functions if we believe that it is possible the observed data are not distributed according to any distribution in $\mathscr{H}_{\mathcal{G}}$.

We therefore need to extend the theory $\mathscr{T}_{\mathcal{G}}$ to account for the possibility of incompatible distributions. There are many choices for how this may be done, none are obviously correct and different extensions will generally induce very different risk sets. Example \ref{ex:extn_cbn} is an example of this.

\begin{example}[Extension of a CBN]\label{ex:extn_cbn}

A graph $\mathcal{G}$ is given in figure \ref{fig:simple_cbn}. With this, we will associate the simplified sample space $(E,\mathcal{E})=(\{0,1\}^3,\mathscr{P}(\{0,1\}^3)$ (where $\mathscr{P}$ denotes the power set) and the random variables $\RV{A},\RV{B}$ and $\RV{C}$ taking values in $\{0,1\}$ defined by projections from $E$. The set of distributions $\Delta(\mathcal{E})$ is the categorical distribution with 8 outcomes.

\begin{figure}
    \centering
     \begin{tikzpicture}[-latex,auto ,node distance =1 cm and 2cm ,on grid ,
    semithick ,
    vb/.style ={ circle ,top color =white , 
    draw , text=blue , minimum width =0.6 cm},
    kernel/.style={rectangle,draw}
    ]

    \node[vb] (A) {$A$};
    \node[vb] (B) [right = of A] {$B$};
    \node[vb] (C) [right = of B] {$C$};
    \draw (A) -- (B);
    \draw (B) -- (C);
    \end{tikzpicture}
    \caption{Simple causal Bayesian network $\mathcal{G}$}
    \label{fig:simple_cbn}
\end{figure}

We will define a causal theory $\mathscr{T}_{\mathcal{G}}$ from $\mathcal{G}$ where, for simplicity, we suppose that only $A$ can be intervened on. The decision space $D=A\cup\{*\}$ with the decision $\RV{D}_A=x$ for $x\in A$ having the usual interpretation as a hard intervention on $A$. Suppose also that the utility $U(\zeta)=\mathbb{E}_{\zeta}[\RV{B}]$ for $\zeta\in\Delta(\mathcal{E}\otimes\mathcal{D})$.

$\mathcal{G}$ implies a single conditional independence: $\RV{A}\CI \RV{C} | \RV{B}$. As in the construction above, $\mathscr{T}_{\mathcal{G}}$ is the set of pairs

\begin{align}
    (\mu,x\mapsto \mu_{\mathcal{G}}^x ) \qquad \mu\in\Delta(\mathcal{E}):\RV{A}\CI_{\mu}\RV{B}|\RV{C} \label{eq:ocbn}
\end{align}

Consider three options for extending this to distributions $\nu$ incompatible with $\mathcal{G}$, noting that one could imagine many additional possibilities:
\begin{itemize}
    \item $\mathscr{T}_{\mathcal{G}}^\circ$ assigns the causal states given by the union over all DAGs on the set of nodes $\{A, B, C\}$
    \item $\mathscr{T}_{\mathcal{G}}^\subset$ assigns the causal states given by the union over all graphs $\mathcal{G}'$ on $\{A, B, C\}$ such that of $\mathcal{G}\subset \mathcal{G}'$ (that is, every edge in $\mathcal{G}$ is also in $\mathcal{G}'$)
    \item $\mathscr{T}_{\mathcal{G}}^\mathrm{stubborn}$ assigns the causal states given by $\mathcal{G}$ to all distributions in $\Delta(\mathcal{E})$
\end{itemize}

Finally, we will suppose some prior $\xi$ on $\Delta(\mathcal{E})$ that is absolutely continuous with respect to the Lebesgue measure on the probability simplex $\{[p_1,..p_8]|\sum_i p_i=1,0\leq p_i \leq 1\}\subset\mathbb{R}^8$ that parametrises $\Delta(\mathcal{E})$ and, whenever a theory admits multiple consequences for a given distribution, assigns these consequences equal weight. Note that the set of distributions $\mathscr{H}_{\mathcal{G}}$ for which $\RV{A}\CI \RV{C} | \RV{B}$ holds therefore has measure 0 with respect to $xi$\cite{meek_strong_1995}. The Bayes risk of any decision function $J\in \mathscr{J}$, therefore, will depend entirely on the chosen theory's behaviour on $\mathscr{H}_{\mathcal{G}}^c$.

Noe that the theory $\mathscr{T}_{\mathcal{G}}^\circ$ admits consequences on $\mathscr{H}_{\mathcal{G}}^c$ associated with:
\begin{enumerate}
    \item DAGs featuring $A\to B$
    \item DAGs featuring $B\to A$ (in equal number to the first set of DAGs)
    \item DAGs featuring no arrow between $B$ and $A$ (featuring one additional DAG to the first two types)
\end{enumerate} 
Therefore the calculation for the Bayes risk given by $\mathscr{T}_{\mathcal{G}}^\circ$ will always feature a weight of more than $\tfrac{2}{3}$ on the possibility that the marginal distribution of $\RV{B}$ is independent of whatever decision $\RV{D}_A$ is chosen. For many priors this will be very different to $\mathscr{T}_{\mathcal{G}}^\mathrm{stubborn}$.

The theory $\mathscr{T}_{\mathcal{G}}^\subset$, on the other hand, yields a set consequences that are ``close'' to the consequence admitted by the original theory $\mathscr{T}_{\mathcal{G}}$ if the distribution $\nu$ is sufficiently ``close'' to a distribution $\mu$ for which $\RV{A} \CI_\mu \RV{C} | \RV{B}$. Marginalising over $\RV{A}$ and $\RV{C}$ and ignoring the passive action, the theory $\mathscr{T}^\subset_{\mathcal{G}}$ associates exactly two consequences with every incompatible $\nu$:
\begin{align}
    &(\nu,x\mapsto \nu(\RV{B}|\RV{A}=x) )\label{eq:cbn_s1}\\
    &(\nu,x \mapsto \sum_c \nu(\RV{B}|\RV{A}=x,\RV{C}=c)\nu(\RV{C}=c)) \label{eq:cbn_s2}
\end{align}

Note that \ref{eq:cbn_s1} is the same pattern as the states given by $\mathscr{T}_{\mathcal{G}}$ - see Eq. \ref{eq:ocbn}. Define the consequences $\kappa^\mathcal{G}_\nu := x\mapsto \nu(\RV{B}|\RV{A}=x)$ and $\kappa^\subset_\nu:= x \mapsto \sum_c \nu(\RV{B}|\RV{A}=x,\RV{C}=c)\nu(\RV{C}=c))$.

Consider some $\nu$ for which $\RV{A} \CI_\nu \RV{C} | \RV{B}$ holds approximately. That is, for some $\epsilon>0$ and all $x\in\{0,1\}$
\begin{align}
    \left|\sum_{c\in\{0,1\}} \nu(\RV{B}|\RV{A}=x,\RV{C}=c)\nu(\RV{C}=c)-\nu(B|A=x)\right| &< \epsilon \label{eq:app_ci}
\end{align}

Recalling the utility $U(\zeta)=\mathbb{E}_\zeta[\RV{B}]$, and that $\mathscr{T}^\subset_\mathcal{G}$ and $\mathscr{T}^\mathrm{stubborn}_\mathcal{G}$ agree on $\kappa^\mathcal{G}$, we can bound the disagreement between the two theories with
\begin{align}
    |R(J,\kappa^\subset,\nu) - R(J,\kappa^\mathcal{G},\nu)| &=  \left|\mathbb{E}_{\nu J \kappa^\subset} [\RV{B}] - \mathbb{E}_{\nu J \kappa^\mathcal{G}} [\RV{B}]\right|\\
        &\leq \max_x \left|\sum_c P^\nu(\RV{B}|\RV{A}=x,\RV{C}=c)P^\nu(\RV{C}=c)-P^\nu(B|A=x) \right|\\
        &< \epsilon
\end{align}

Given the assumption of a prior $\xi$ that is uniformly continuous with respect to the Lebesgue measure, $\mathscr{T}_{\mathcal{G}}^\subset$ but not $\mathscr{T}_{\mathcal{G}}^\circ$ gives us the property that for distributions where $\RV{A} \CI_\nu \RV{C} | \RV{B}$ almost holds, the theory $\mathscr{T}_{\mathscr{G}}^{\mathrm{stubborn}}$ can yield ``almost'' correct results. Note that $\mathscr{T}_{\mathscr{G}}^{\mathrm{stubborn}}$ is precisely the theory that actually uses the graph $\mathcal{G}$ to calculate consequences.
\end{example}

This example is somewhat contrived. The stipulation that the prior $\xi$ is absolutely continuous with respect to the Lebesgue measure may be considered strong, as may the assumption of equal weight given to different consequences where multiple consequences were allowed. Bayesian methods of graph learning, for example, may use priors that are not absolutely continuous with respect to the Lebesgue measure \cite{chickering_optimal_2003}. At the same time, it is not obviously unreasonable; it is consistent with the notion that ``there are no true parametric zeros'' endorsed by many statisticians (for example  \cite{gelman_bayesian_2010,meehl_theory-testing_1967,berkson_difficulties_1938}) and a similar assumption is invoked in support of the faithfulness hypothesis in causal graph learning \cite{meek_strong_1995}. A prior that assigns nonzero weight to the set of distributions for which $\RV{A} \CI_\nu \RV{C} | \RV{B}$ will also not resolve the question of whether or not it is reasonable to calculate consequences according to $\mathcal{G}$ if $\RV{A} \CI_\nu \RV{C} | \RV{B}$ is in fact violated by a very small amount.

Even in the apparently simple case of a known causal graph $\mathcal{G}$, determining the associated causal theory is not a simple matter. The standard practice of computing consequences according to $\mathcal{G}$ if we do not reject the hypothesis that the observed data are compatible with $\mathcal{G}$ appears to rely on a prior with particular properties, a particular extension of the associated theory $\mathscr{T}_{\mathcal{G}}$, or both.