
%!TEX root = main.tex

\section{Ignorability Does Not Identify the Average Causal Effect}

There is an error in derivations of the identification of the Average Causal Effect from the assumption of ignorability in Potential Outcomes. I will present a counterexample to the claim that ignorability + positivity leads to the identification of Average Causal Effect. The problem arises because authors assume that the average of an arbitrary sequence of random variables - absent any assumptions of IID or exchangeability - converges to something meaningful. I think the actual condition they want is ``treatment exchangeability''.

For the setup, I refer to \citet{angrist_mastering_2014} \todo[inline]{(although I should probably actually refer to \citet{rubin_causal_2005}).}

\begin{quote}
We use the letter $\RV{Y}$ as shorthand for health, the outcome variable of interest. To make it clear when we’re talking about specific people, we use subscripts as a stand-in for names: $\RV{Y}_i$ is the health of individual $i$. The outcome $\RV{Y}_i$ is recorded in our data. But, facing the choice of whether to pay for health insurance, person $i$ has two potential  outcomes, only one of which is observed. To distinguish one potential outcome from another, we add a second subscript: The road taken without health insurance leads to $\RV{Y}_{0i}$ (read  this  as  “y-zero-i”)  for  person $i$,  while  the  road  with health insurance leads to $\RV{Y}_{1i}$ (read this as “y-one–i”) for person $i$. Potential outcomes lie at the end of each road one might take. The causal effect of insurance on health is the difference between them, written $\RV{Y}_{1i}-\RV{Y}_{0i}$.

[...]

Actual health outcomes: $\RV{Y}_i$, treatment: $\RV{D}_i$

[...]

$\kappa$ is both the individual and average causal effect on health outcomes.

[...]

\begin{align}
	\mathrm{Avg}_n[\RV{Y}_{1i}-\RV{Y}_{0i}] &= \frac{1}{n}\sum_{i=1}^n [\RV{Y}_{1i} - \RV{Y}_{0i}] \label{eq:cond_avg}
\end{align}

[...]

\begin{align}
	\mathrm{Avg}_n[\RV{Y}_{1i}|\RV{D}_i=1] - &\mathrm{Avg}_n[\RV{Y}_{0i}|\RV{D}_i=0] = \nonumber \\&\kappa + \mathrm{Avg}_n[\RV{Y}_{0i}|\RV{D}_i=1] - \mathrm{Avg}_n[\RV{Y}_{0i}|\RV{D}_i=0] \label{eq:angristeq14}
\end{align}

[...]

\paragraph{Random assignment eliminates selection bias} When $\RV{D}_i$ is randomly assigned, $\mathbb{E}[\RV{Y}_{0i}|\RV{D}_i = 1] = \mathbb{E}[\RV{Y}_{0i}|\RV{D}_i=0]$, and the difference in expectations by treatment status captures the causal effect of treatment:

\begin{align}
	\mathbb{E}[\RV{Y}_i|\RV{D}_i=1] - \mathbb{E}[\RV{Y}_i|\RV{D}_i=0] &= \mathbb{E}[\RV{Y}_{1i}|\RV{D}_i=1]-\mathbb{E}[\RV{Y}_{0i}|\RV{D}_i=0]\\
																	  &= \mathbb{E}[\RV{Y}_{0i}+\kappa|\RV{D}_i=1] - \mathbb{E}[\RV{Y}_{0i}|\RV{D}_i=0]\\
																	  &= \kappa + \mathbb{E}[\RV{Y}_{0i}|\RV{D}_i=1] - \mathbb{E}[\RV{Y}_{0i}|\RV{D}_i=0]\\
																	  &= \kappa \label{eq:no_you_cant}
\end{align}

Provided the sample size is large enough for the law of large numbers to work its magic (so we can replace the conditional averages in equation \ref{eq:angristeq14} with conditional expectations), selection bias disappears in a randomized experiment

\end{quote}

The problem is that the expectations in Equation \ref{eq:no_you_cant} \emph{cannot} be replaced with conditional averages as defined in \ref{eq:cond_avg}, even in the infinite limit. From the strong law of large numbers we can deduce that, given IID variables $(\RV{Y}^j_{i0},\RV{D}^j_i)\sim \prob{P}(\RV{Y}_{i0},\RV{D}_i)$ for $j\in \mathbb{N}$,

\begin{align}
	\lim_{n\to\infty}\sum_{j}^n\frac{\RV{Y}^j_{i0}\llbracket\RV{D}^j_i=1\rrbracket}{\sum_j^n\llbracket\RV{D}^j_i=1\rrbracket} \overset{\prob{P}-a.s.}{=} \mathbb{E}[\RV{Y}_{1i}|\RV{D}_i=1]
\end{align}

Note that Angrist and Pischke do \emph{not} assume that $(\RV{Y}^j_{i0},\RV{D}^j_i)$ are given - in their conventions, this would refer to repeated samples of the ``same individual''.

The quantity given by their ``conditional average'' is an average of random variables that share similar names, but are otherwise unrelated:

\begin{align}
	\sum_{i}^n\frac{\RV{Y}_{i0}\llbracket\RV{D}_i=1\rrbracket}{\sum_i^n\llbracket\RV{D}_i=1\rrbracket}\label{eq:avg_stull}
\end{align}

\todo[inline]{This counterexample satisfies the stronger assumptions presented in Rubin 2005, hence it doesn't quite line up with the assumptions from Angrist and Pischke; need to incorporate Rubin.}


Suppose we have random variables $(\vecRV{D},\vecRV{Y}_0,\vecRV{Y}_1):=([\RV{D}_0,\RV{D}_1,...],[\RV{Y}_{00},\RV{Y}_{10},...],[\RV{Y}_{01},\RV{Y}_{11},...]])\in [0,1]^{3\mathbb{N}}$ and

\begin{align}
	\prob{P}(\vecRV{D}=\mathbf{d},\vecRV{Y}_0=\mathbf{y}_0,\vecRV{Y}_1=\mathbf{y}_1) &= \prod_{i\in \mathbb{N}}\left( (1-\epsilon)\delta_{(i\mod 2)}(d_i) +\epsilon\right) \delta_{(i\mod 2)} (y_{i0}) \delta_{(1-i\mod 2)}(y_{i1})
\end{align}

By construction $\vecRV{Y}_1,\vecRV{Y}_0\CI_{\prob{P}} \vecRV{D}$, and $\prob{P}(\RV{D}_i=d_i)>0$ for all $d_i$, which implies for all $i$ $\mathbb{E}[\RV{Y}_{0i}|\RV{D}_i=1]=\mathbb{E}[\RV{Y}_{0i}|\RV{D}_i=0]$. However

\begin{align}
	\lim_{n\to\infty} \sum_{i}^n\frac{\RV{Y}_{i1}\llbracket\RV{D}_i=1\rrbracket}{\sum_i^n\llbracket\RV{D}_i=1\rrbracket} - \lim_{n\to\infty} \sum_{i}^n\frac{\RV{Y}_{i0}\llbracket\RV{D}_i=0\rrbracket}{\sum_i^n\llbracket\RV{D}_i=0\rrbracket}&=1-\frac{\epsilon}{2}-\frac{\epsilon}{2}\label{eq:bad_limit}\\
									  &= 1-\epsilon\\
	\lim_{n\to\infty} \sum_{i}^n\frac{\RV{Y}_{i0}}{n} - \lim_{n\to\infty} \sum_{i}^n\frac{\RV{Y}_{i1}}{n}&=\frac{1}{2}-\frac{1}{2}\\
									  &= 0\\
									  &=\text{``the average causal effect''}\\
									  &\neq \lim_{n\to\infty} \sum_{i}^n\frac{\RV{Y}_{i1}\llbracket\RV{D}_i=1\rrbracket}{\sum_i^n\llbracket\RV{D}_i=1\rrbracket}-\lim_{n\to\infty} \sum_{i}^n\frac{\RV{Y}_{i0}\llbracket\RV{D}_i=0\rrbracket}{\sum_i^n\llbracket\RV{D}_i=0\rrbracket}
\end{align}

Contradicting the claim made by Eq. \ref{eq:no_you_cant}.

I don't know the \emph{necessary} conditions for Equation \ref{eq:no_you_cant} to hold. However, I think the following is sufficient: for any finite permutation $\pi:[0,1]^\mathbb{N}\to[0,1]^\mathbb{N}$:

\begin{align}
	\lim_{n\to\infty} \sum_{i}^n\frac{\RV{Y}_{i1}\llbracket\RV{D}_i=1\rrbracket}{\sum_i^n\llbracket\RV{D}_i=1\rrbracket} = \lim_{n\to\infty} \sum_{i}^n\frac{\RV{Y}_{i1}\llbracket\RV{D}_{\pi(i)}=1\rrbracket}{\sum_i^n\llbracket\RV{D}_{\pi(i)}=1\rrbracket}
\end{align}

(And the limits are finite.)

 	

This corresponds to a condition which I call \emph{treatment-exchangeability}. 
