\section{Causal Decision Problems}

The normal form representation of a two person game is a triple:

\begin{definition}[Two person game (normal form)]
A normal form game is a triple $\langle \mathscr{S}, A, L\rangle$ where $\mathscr{S}$ and $A$ are arbitrary sets and $L:\mathscr{S}\times A\to [0,\infty)$ is a loss function.

By way of interpretation, we will identify the set $\mathscr{S}$ with a set of possible states that the environment may occupy and $A$ with a set of actions some decision maker may take. The decision maker seeks an action in $A$ that minimises the loss $L$.
\end{definition}

The definition presented here does not generally admit a unique best action. A minimax solution chooses an action that minimises the worst case loss:
\begin{align}
    a^*_{mm} = \argmin_{a\in A} [\sup_{s\in \mathscr{S}} L(s,a)]\label{eq:mm_soln}
\end{align}

If the set $\mathscr{S}$ is equipped with a $\sigma$-algebra $\mathcal{S}$, then given a probability measure $\xi\in \Delta(\mathcal{S})$ which we will call a ``prior'', the Bayes solution minimizes the expected risk with respect to $\xi$:
\begin{align}
    a^*_{ba} = \argmin_{a\in A} \int_{\mathcal{S}} L(s,a) \xi(ds)
\end{align}

A statistical decision problem is an instance of a two person zero sum game.

\begin{definition}[Statistical Decision Problem]
A statistical decision problem (SDP) is defined by a tuple $\langle (\mathscr{H},E), D, \RV{X}, L\rangle$.
\begin{itemize}
    \item Given a $\sigma$-algebra $\mathcal{E}$ on $E$, $\mathscr{H}\subset\Delta(\mathcal{E})$ is a hypothesis class.
    \item The set $D$, which is equipped with a $\sigma$-algebra $\mathcal{D}$, is a set of decisions
    \item $\RV{X}:E\to X$ is a random variable representing the information available for the statistician to make a decision
    \item $L:\mathcal{H}\times D\to [0,\infty)$ is a loss function
\end{itemize}

Denote by $\mathscr{D}$ the set of decision kernels $X\to \Delta(\mathcal{D})$. Denote by $K_{\RV{X}}:E\to \Delta(\mathcal{X})$ the kernel given by $x\mapsto \delta_x$. For $J\in \mathscr{D}$ and $\mu\in \mathcal{H}$, the risk is defined as:
\begin{align}
    R(J,\mu) = \int_D L(\mu,y) \mu K_{\RV{X}} J(dy)
\end{align}

Here $\mu K_{\RV{X}} J$ is the measure-kernel-kernel product defined in \ref{def:kernel_product}.

Denoting by $\mathscr{D}$  the set of kernels $X\to \Delta(\mathcal{D})$, the triple $\langle \mathscr{H}, \mathscr{D}, R\rangle$ forms a two player normal form game. The concepts of the minimax and Bayes solutions (Eq. \ref{eq:mm_soln} and \ref{eq:bayes_soln}) apply to this game.
\end{definition}

The loss function $L$ expresses preferences over (state, decision) pairs. However, it may be the case that our preferences don't naturally apply directly to such pairs. For a doctor deciding whether to prescribe a treatment to a patient, it is clear that this patient being healthy in the future is preferable to them being sick. While it may be possible to build a state for which it makes sense to evaluate the desirability of a (state, treatment) pair (for example, such a state may describe both the patient's illness and their responsiveness to treatement), such a description appears to be derived from the more basic preferences over future states of the patient's health.

This motivates the definition of a causal decision problem, which proceeds from a loss defined over outcomes rather than (state, decision) pairs.

\begin{definition}[Consequences]
Given a measurable space $(F,\mathcal{F})$ and a measurable decision set $(D,\mathcal{D})$, a Markov kernel $\kappa:D \to \Delta(\mathcal{F})$ is a consequence mapping, or just a consequence for short.
\end{definition}

\begin{definition}[Causal Theory]\label{def:causal_theory}
Given measurable spaces $(E,\mathcal{E})$, $(F,\mathcal{F})$, a hypothesis class $\mathscr{H}\subset \Delta(\mathcal{E})$ and a measurable decision set $(D,\mathcal{D})$, a causal theory is a map $\tau:\mathscr{H}\to (D \to \Delta(\mathcal{F}))$. A theory $\tau$ maps a distribution $\mu$ to a consequence.
\end{definition}

\begin{definition}[Causal Decision Problem]
A causal decision problem (CDP) is a tuple $\langle (\mathscr{H}, \mathscr{T}, E, F), D, \RV{X}, L \rangle$. The sets $E$, $F$ and $D$ are equipped with $\sigma$-algebras which we leave implicit.

\begin{itemize}
    \item $\mathscr{H}\subset \Delta(\mathcal{E})$ is a hypothesis class
    \item $\mathscr{T}$ is a set of causal theories $\tau:\mathscr{H}\to ( D\to \Delta(\mathcal{F}))$ which we term a \emph{causal prospect}
    \item $(D,\mathcal{D})$ is a measurable decision set
    \item $\RV{X}:E\to X$ is a random variable representing the given information
    \item $L:\Delta(\mathcal{F}\times\mathcal{D})\to [0,\infty)$ is a loss function over distributions on the joint $F\times D$ space
\end{itemize}

Given a decision map $J\in\mathscr{D}$, $\mu\in \mathscr{H}$ and $\tau\in \mathscr{T}$, define 
\begin{align}
    \mu K_{\RV{X}} J\underline{[I_D\otimes \tau^\mu]}(A\times B) = \int_A \tau(\mu)(y;B) \mu K_{\RV{X}} J(dy)
\end{align}
the risk $R(J,\mu,\tau)$ is
\begin{align}
    R(J,\mu,\tau) = L(\mu K_{\RV{X}} J\underline{[I_D\otimes \tau^\mu]}) 
\end{align}

Where $\tau^\mu(d;\cdot)\in \Delta(\mathcal{F})$ is shorthand for $\tau(\mu)(d;\cdot)$.

The triple $\langle \mathscr{H}\times\mathscr{T}, \mathscr{D}, R\rangle$ is a normal form two person game.

Given random variables $\RV{D}:F\times D\to D$ and $\RV{F}:F\times D\to F$ defined by the projections, if the loss has the form $L(\nu) = \mathbb{E}_{\nu}[\ell(\RV{F},\RV{D})]$ for some $\ell:F\times D\to [0,\infty)$ we call it an ordinary utility. In this case, the risk takes the form
\begin{align}
    R(J,\mu,\tau) &= \mathbb{E}_{\mu K_{\RV{X}} J\underline{[I_D\otimes \tau^\mu]}}[\ell(\RV{F},\RV{D})]
\end{align}


\end{definition}

\begin{definition}[SDP and CDP reduction]
Given a statistical decision problem $\alpha$ with an induced game $\langle \mathscr{H}, \mathscr{D}, R\rangle$ and a causal decision problem $\beta$ with induced game $\langle \mathscr{H}'\times \mathscr{T}', \mathscr{D}, R'\rangle$ where the set of decision functions $\mathscr{D}$ is shared, $\alpha$ is reducible to $\beta$ if there is a surjective function $f:\mathscr{H}\to \mathscr{H}'\times\mathscr{T}'$ such that for all $\mu\in \mathscr{H}$ and $J\in \mathscr{D}$ we have $R(J,\mu)=R'(J,f(\mu))$ and $\beta$ is reducible to $\alpha$ if there is some surjective function $g:\mathscr{H}'\times\mathscr{T}'\to \mathscr{H}$ such that for all $(\nu,\tau)\in \mathscr{H}'\times \mathscr{T}'$ we have $R(J,g(\nu,\tau))=R'(J,\nu,\tau)$.
\end{definition}

The intuition behind this definition is that, if one problem can be reduced to another, then for every (decision, state of nature) pair in the first problem there is a state of nature in the second problem assigning the same risk to the decision in question.

The following theorem shows that any statistical decision problem can be reduced to a causal decision problem where decisions have no effect. The intuition is that if we choose a causal theory that always returns the input distribution, then we ``essentially'' have a statistical decision problem, provided we can find a loss that treats the output distribution the same way as the statistical decision problem treats the (input distribution, decision) pair.

\begin{theorem}
Every statistical decision problem $\langle (\mathscr{H},E),D,\RV{X},L\rangle$ can be reduced to a causal decision problem.
\end{theorem}
\begin{proof}
Consider the causal theory $\tau_0$ with the property $\tau_(0)^\mu(d;\cdot)=\mu$ for all $\mu\in\mathscr{H}$, $d\in D$. Take the causal prospect $\mathscr{T}_0=\{\tau_0\}$ and the loss $L':\Delta(\mathcal{E}\otimes\mathcal{D})\to[0,\infty)$ defined by $L'(\nu) = \mathbb{E}_\nu \left[L(\nu_\RV{E},\RV{D})\right]$ to construct the causal decision problem $\langle (\mathscr{H},\mathscr{T}_0,E,E),D,\RV{X},L'\rangle$. We will show that the original problem can be reduced to this.

For the surjective map, take $f:\mathscr{H}\to \mathscr{H}\times\mathscr{T}$ defined by $f(\mu)=(\mu,\tau_0)$.

Denote by $R$ the risk associated with $\langle (\mathscr{H},E),D,\RV{X},L\rangle$ and by $R'$ the risk associated with $\langle (\mathscr{H},\mathscr{T},E,E),D,\RV{X},L\rangle$. Then
\begin{align}
    R'(J,\mu,\tau_0) &= \int_D L(\mu K_{\RV{X}} J\underline{[I_D\otimes \tau^\mu]}_{\RV{E}}, y) \mu K_{\RV{X}} J(dy)\\
                   &= \int_D L(\mu,y) \mu K_{\RV{X}} J(dy)\\
                   &=R(J,f(\nu,\tau_0))
\end{align}
\end{proof}

Whether a causal decision problem can, in general, be reduced to a statistical decision problem is an open question. Theorem \ref{th:cdp_to_sdp} shows that the reduction is possible under restricted conditions. The intuition is that, under appropriate conditions, we can construct a hypothesis class such that every consequence in the causal decision problem corresponds to a conditional distribution in the statistical decision problem.

Caution is required, however. In order to construct the relevant hypothesis class, we must assume a positive definite marginal distribution over the decision space $(D,\mathcal{D})$. As a result, elements of this hypothesis class will generally induce different distributions over ``input'' and ``output'' variables.

\begin{theorem}\label{th:cdp_to_sdp}
A causal decision problem $\langle (\mathscr{H},\mathscr{T},E,F),D,\RV{X},L\rangle$ where $D$ is a denumerable set and the loss $L$ is an ordinary utility can be reduced to a statistical decision problem.
\end{theorem}

\begin{proof}
Take some $\mu\in \Delta(\mathcal{D})$ such that $\mu(\{y\})>0$ for all $y\in D$. Such a $\mu$ exists by the denumerability of $\mathcal{D}$.

Define a function $f:\mathscr{H}\times\mathscr{T}\to \Delta(\mathcal{E}\otimes\mathcal{F}\otimes\mathcal{D})$ such that
\begin{align}
    f(\nu,\tau)(A\times B\times C)&=\int_C \tau^\nu(y;B) \nu(A)\mu(dy)\\
                                  &:=P^{\nu\tau}
\end{align}

Define $L':\Delta(\mathcal{E}\otimes\mathcal{F}\otimes\mathcal{D})\times D\to[0,\infty)$ by
\begin{align}
    L'(\xi,y) = \int_F \ell(z,y) \frac{\xi(E\times dz\times\{y\})}{\mu(\{y\})}
\end{align}

Note that
\begin{align}
    L'(f(\nu,\tau),y) &= \int_{F} \ell(z,y)  \frac{ \tau^{\mu}(y;dz)\mu(\{y\})}{\mu(\{y\})}\\
                      &= \int_F \ell(z,y) \tau^{\mu}(y;dz) \\
\end{align}

Define $\RV{D}:F\times D\to D$, $\RV{F}:F\times D\to F$ by their respective projections.

Then, given the statistical decision problem $\langle(\mathcal{H}',E\times F\times D),D,\RV{X},L'\rangle$, we have the risk
\begin{align}
    R'(J,f(\nu,\tau)) &= \int_D \int_F \ell(z,y) \tau^\mu(y;dz)  \nu K_{\RV{X}} J(x;dy) \\
                      &= \mathbb{E}_{\nu K_{\RV{X}} J\underline{[I_D\otimes \tau^\mu]}}[\ell(\RV{F},\RV{D}]
\end{align}
\end{proof}

A more straightforward reduction can be made if the causal decision problem is \emph{identifiable}. A causal decision problem is identifiable if the risk of a given decision function is unique for any given distribution over the inputs.

\begin{definition}[Identifiability]
A causal decision problem $\langle (\mathscr{H}, \mathscr{T}, E, F), D, \RV{X}, L \rangle$ is identifiable iff for each $\mu\in \mathscr{H}$, $J\in \mathscr{D}$, $|\{R(J,\mu,\tau)|\tau\in \mathscr{T}\}|=1$.
\end{definition}
