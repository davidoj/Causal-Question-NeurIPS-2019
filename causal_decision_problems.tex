\section{Causal Decision Problems}

The normal form representation of a two person game is a triple:

\begin{definition}[Two person game (normal form)]
A normal form game is a triple $\langle \mathscr{S}, A, L\rangle$ where $\mathscr{S}$ and $A$ are arbitrary sets and $L:\mathscr{S}\times A\to [0,\infty)$ is a loss function.

By way of interpretation, we will identify the set $\mathscr{S}$ with a set of possible states that the environment may occupy and $A$ with a set of actions some decision maker may take. The decision maker seeks an action in $A$ that minimises the loss $L$.
\end{definition}

The definition presented here does not generally admit a unique best action. A minimax solution chooses an action that minimises the worst case loss:
\begin{align}
    a^*_{mm} = \argmin_{a\in A} [\sup_{s\in \mathscr{S}} L(s,a)]\label{eq:mm_soln}
\end{align}

If the set $\mathscr{S}$ is equipped with a $\sigma$-algebra $\mathcal{S}$, then given a probability measure $\xi\in \Delta(\mathcal{S})$ which we will call a ``prior'', the Bayes solution minimizes the expected risk with respect to $\xi$:
\begin{align}
    a^*_{ba} = \argmin_{a\in A} \int_{\mathcal{S}} L(s,a) \xi(ds)
\end{align}

A statistical decision problem is an instance of a normal form two person game.

\begin{definition}[Statistical Decision Problem]
A statistical decision problem (SDP) is defined by a tuple $\langle (\mathscr{H},E), D, \RV{X}, L\rangle$.
\begin{itemize}
    \item Given a $\sigma$-algebra $\mathcal{E}$ on $E$, $\mathscr{H}\subset\Delta(\mathcal{E})$ is a hypothesis class.
    \item The set $D$, which is equipped with a $\sigma$-algebra $\mathcal{D}$, is a set of decisions
    \item $\RV{X}:E\to X$ is a random variable representing the information available for the statistician to make a decision
    \item $L:\mathcal{H}\times D\to [0,\infty)$ is a loss function
\end{itemize}

Denote by $\mathscr{D}$ the set of decision kernels $X\to \Delta(\mathcal{D})$. Denote by $K_{\RV{X}}:E\to \Delta(\mathcal{X})$ the kernel given by $x\mapsto \delta_x$. For $J\in \mathscr{D}$ and $\mu\in \mathcal{H}$, the risk is defined as:
\begin{align}
    R(J,\mu) = \int_D L(\mu,y) \mu K_{\RV{X}} J(dy)
\end{align}

Here $\mu K_{\RV{X}} J$ is the measure-kernel-kernel product defined in \ref{def:kernel_product}.

Denoting by $\mathscr{D}$  the set of kernels $X\to \Delta(\mathcal{D})$, the triple $\langle \mathscr{H}, \mathscr{D}, R\rangle$ forms a two player normal form game. The concepts of the minimax and Bayes solutions (Eq. \ref{eq:mm_soln} and \ref{eq:bayes_soln}) apply to this game.
\end{definition}

The loss function $L$ expresses preferences over (state, decision) pairs. However, it may be the case that our preferences don't naturally apply directly to such pairs. For a doctor deciding whether to prescribe a treatment to a patient, it is clear that this patient being healthy in the future is preferable to them being sick. While it may be possible to build a state for which it makes sense to evaluate the desirability of a (state, treatment) pair (for example, such a state may describe both the patient's illness and their responsiveness to treatement), such a description appears to be derived from the more basic preferences over future states of the patient's health.

This motivates the definition of a causal decision problem, which proceeds from a loss defined over outcomes rather than (state, decision) pairs. In order to compute the loss associated with a decision, then, a map from decisions to outcomes is required, which we term a \emph{consequence}. 

\begin{definition}[Consequences]
Given a measurable space $(F,\mathcal{F})$ and a measurable decision set $(D,\mathcal{D})$, a Markov kernel $\kappa:D \to \Delta(\mathcal{F})$ is a consequence mapping, or just a consequence for short.
\end{definition}

\begin{definition}[Causal state]
Given a consequence $\kappa:D\to \Delta(\mathcal{F})$ and some distribution $\mu\in \Delta(\mathcal{E})$, the pair $(\kappa,\mu):=\tau$ is a \emph{causal state}. We refer to $\kappa$ as the consequence and $\mu$ as the observed state.
\end{definition}

\begin{definition}[Causal Theory]\label{def:causal_theory}
A causal theory $\mathscr{T}$ is a set of causal states. Given a set of consequences $\mathscr{K}$ and a test distribution $\pi\in \Delta(\mathcal{D})$, the set $\{(\kappa,\pi\kappa)|\kappa\in \mathscr{K}\}$ is the causal theory generated by $\mathscr{K}$ and $\pi$.
\end{definition}

\begin{definition}[Causal Decision Problem]\label{def:CDP}
A causal decision problem (CDP) is a tuple $\langle (\mathscr{T}, E), D, \RV{X}, L \rangle$. The sets $E$ and $D$ are equipped with $\sigma$-algebras which we leave implicit.

\begin{itemize}
    \item $\mathscr{T}$ is a causal theory
    \item $(D,\mathcal{D})$ is a measurable decision set
    \item $\RV{X}:E\to X$ is a random variable representing the given information
    \item $L:\Delta(\mathcal{E}\times\mathcal{D})\to [0,\infty]$ is a loss function over distributions on the joint $F\times D$ space
\end{itemize}

Given a decision map $J\in\mathscr{D}$ and a state $(\kappa,\mu)\in \mathscr{T}$, define $f:\mathscr{D}\times\mathscr{T}\to\Delta(\mathcal{E}\times\mathcal{D})$ by
\begin{align}
    f(J,\kappa,\mu)(A\times B) = \int_A \kappa (y;B) \mu K_{\RV{X}} J(dy)
\end{align}
the risk $R(J,\kappa,\mu)$ is
\begin{align}
    R(J,\kappa,\mu) = L(f(J,\kappa,\mu)) 
\end{align}

The triple $\langle \mathscr{T}, \mathscr{D}, R\rangle$ is a normal form two person game.

Given random variables $\RV{D}:E\times D\to D$ and $\RV{E}:E\times D\to F$ defined by the respective projections, if the loss has the form $L(\nu) = \mathbb{E}_{\nu}[\ell(\RV{F},\RV{D})]$ for some $\ell:F\times D\to [0,\infty)$ we call it an ordinary utility. In this case, the risk takes the form
\begin{align}
    R(J,\kappa,\mu) &= \mathbb{E}_{f(J,\kappa,\mu)}[\ell(\RV{E},\RV{D})]
\end{align}


\end{definition}

\begin{definition}[SDP and CDP reduction]
Given a statistical decision problem $\alpha$ with an induced game $\langle \mathscr{H}, \mathscr{D}, R\rangle$ and a causal decision problem $\beta$ with induced game $\langle \mathscr{T}, \mathscr{D}, R'\rangle$ where the set of decision functions $\mathscr{D}$ is shared, $\alpha$ is reducible to $\beta$ if there is a surjective function $g:\mathscr{H}\to \mathscr{T}$ such that for all $\mu\in \mathscr{H}$ and $J\in \mathscr{D}$ we have $R(J,\mu)=R'(J,g(\mu))$ and $\beta$ is reducible to $\alpha$ if there is some surjective function $h:\mathscr{T}\to \mathscr{H}$ such that for all $\kappa,\mu\in \mathscr{T}'$ we have $R(J,h(\kappa,\mu))=R'(J,\kappa,\mu)$.
\end{definition}

The intuition behind this definition is that, if one problem can be reduced to another, then for every (decision, state of nature) pair in the first problem there is a state of nature in the second problem assigning the same risk to the same decision.

\begin{definition}[Admissible Action]
Given a normal form two person game $\langle \mathscr{S}, A, L\rangle$, an action $a\in A$ is strictly better than $a'\in A$ iff $L(s,a)\leq L(s,a')$ for all $s\in\mathscr{S}$ and $L(s_0,a)<L(s_0,a')$ for some $s_0\in \mathscr{S}$. If only the first holds, then $a$ is as good as $a'$.

An admissible action is an action $a\in A$ such that there is no action strictly better than $A$.
\end{definition}

\begin{definition}[Complete Class]
A class $C$ of decisions is a complete class if for every $a\not\in C$ there is some $a'\in C$ that is strictly better than $a$.

A class $C$ is an essentially complete class if for every $a\not\in C$ there is some $a'\in C$ that is as good as $a$.
\end{definition}

\begin{lemma}[Reduction and admissibility]
If a causal decision problem $\beta$ with induced game $\langle \mathscr{T},\mathscr{D}, R\rangle$ can be reduced to a statistical decision problem $\alpha$ with induced game $\langle \mathscr{H},\mathscr{D},R' \rangle$ then a decision function $J\in \mathscr{D}$ is admissible in $\beta$ iff it is admissible in $\alpha$.
\end{lemma}


\begin{proof}
Suppose $J\in\mathscr{D}$ is inadmissible in $\alpha$. Then there is some $J'\in\mathscr{D}$, $\mu\in\mathscr{H}$ such that $R'(J',\mu)<R'(J,\mu)$ and $R'(J',\nu)\leq R'(J,\nu)$ for all $\nu\in \mathscr{H}$. Let $h$ be the function that witnesses the reduction. Then we have for all $\tau\in h^{-1}(\mu)$, $R(J',\tau)=R'(J',\mu)<R(J,\tau)=R'(J,\nu)$ and for all $\nu\in \mathscr{H}$, $\chi\in h^{-1}(\nu)$, $R(J',\chi)=R'(J',\nu)\leq R(J,\chi)=R'(J,\nu)$. The set $\bigcup_{\nu\in\mathscr{H}} h^{-1}(\nu)=\mathscr{T}$, so $J$ is inadmissible in $\beta$.

Suppose $J\in \mathscr{D}$ is admissible in $\beta$. Then there is some $J'\in\mathscr{D}$, $\tau\in\mathscr{T}$ such that $R(J',\tau)<R(J,\tau)$ and $R(J',\chi)\leq R(J,\chi)$ for all $\chi\in \mathscr{T}$. Then we have $R'(J',h(\tau))=R(J',\tau)<R(J,\tau)=R'(J,h(\tau))$ and $R'(J',h(\chi))=R(J',\chi)\leq R(J,\chi)=R'(J,h(\chi))$. Because $h$ is surjective, $J$ is admissible in $\alpha$.
\end{proof}

\begin{corollary}[Reduction and completeness]
If a causal decision problem $\beta$ with induced game $\langle \mathscr{T},\mathscr{D}, R\rangle$ can be reduced to a statistical decision problem $\alpha$ with induced game $\langle \mathscr{H},\mathscr{D},R' \rangle$, then an (essentially) complete class with respect to $\alpha$ is (essentially) complete with respect to $\beta$.
\end{corollary}

\begin{definition}[Induced prior]
If a causal decision problem $\beta$ with induced game $\langle \mathscr{T},\mathscr{D}, R\rangle$ can be reduced to a statistical decision problem $\alpha$ with induced game $\langle \mathscr{H},\mathscr{D},R' \rangle$ witnessed by $h:\mathscr{T}\to\mathscr{H}$ then given a prior $\xi$ on $(\mathscr{H},\mathcal{H})$ the induced prior $\xi_h$ on $(\mathscr{T},\sigma(h))$ is defined by $\xi_h(h^{-1}(A)) = \xi(A)$ for $A\in \mathcal{H}$.
\end{definition}[Induced prior]


The following theorem shows that any statistical decision problem can be reduced to a causal decision problem where decisions have no effect. The intuition is that if we choose a causal theory that always returns the input distribution, then we ``essentially'' have a statistical decision problem.

\begin{theorem}
Every statistical decision problem $\langle (\mathscr{H},E),D,\RV{X},L\rangle$ can be reduced to a causal decision problem.
\end{theorem}
\begin{proof}
For each $\mu\in \mathscr{H}$ define the consequence $\kappa_\mu:d\mapsto \mu$ for all $d\in D$. Take the causal theory $\mathscr{T}=\{(\kappa_\mu,\mu)|\mu\in \mathscr{H}\}$ for some $\pi\in \Delta(\mathcal{D})$ and the loss $L':\Delta(\mathcal{E}\otimes\mathcal{D})\to[0,\infty]$ defined by $L'(\nu) = \mathbb{E}_\nu \left[L(P^\nu_\RV{E},\RV{D})\right]$ to construct the causal decision problem $\langle (\mathscr{T},E),D,\RV{X},L'\rangle$. We will show that the original problem can be reduced to this.

For the surjective map, take $g:\mathscr{H}\to \mathscr{T}$ defined by $f(\mu)=\kappa_\mu$.

Take $\RV{D}$ and $\RV{E}$ to be the projections from $D\times E$ to $D$ and $E$ respectively. Note that $P^{f(J,\kappa,\mu)}_{\RV{E}}(B) = \int_D \mu(B) \mu K_{\RV{X}} J(dy) = \mu(B)$.

Denote by $R$ the risk associated with $\langle (\mathscr{H},E),D,\RV{X},L\rangle$ and by $R'$ the risk associated with $\langle (\mathscr{T},E),D,\RV{X},L'\rangle$. Then
\begin{align}
    R'(J,\kappa,\mu) &= \int_D L(P^f(J,\kappa,\mu)_{\RV{E}}, y) \mu K_{\RV{X}} J(dy)\\
                   &= \int_D L(\mu,y) \mu K_{\RV{X}} J(dy)\\
                   &=R(J,g(\kappa,\mu))
\end{align}
\end{proof}

Whether a causal decision problem can, in general, be reduced to a statistical decision problem is an open question. Theorem \ref{th:cdp_to_sdp} shows that the reduction is possible under restricted conditions. The intuition is that, under appropriate conditions, we can construct a hypothesis class such that every causal state in the causal theory is associated with a particular element of the hypothesis class.

\begin{lemma}\label{lem:red_cdp}
Given a causal decision problem $\beta=\langle (\mathscr{T},E),D,\RV{X},L\rangle$ where $L$ has an ordinary utility $\ell$, let $\mathscr{K}=\{\kappa|(\kappa,\mu)\in \mathscr{T}\}$ be the set of consequences. $\beta$ is reducible to a statistical decision problem on some measurable space $(E^2\times D,\mathcal{E}\otimes \mathcal{E}\otimes \mathcal{D})$ if there is some surjective map $m:\Delta(\mathcal{E}\otimes\mathcal{D})\to \mathscr{K}$.
\end{lemma}

\begin{proof}
Let $\mathcal{H}\subset \Delta(\mathcal{E}\otimes \mathcal{E}\otimes \mathcal{D})$ be some hypothesis class and let $m^\dagger$ be a right inverse of $m$. Define $h:\mathscr{T}\to \mathcal{H}$ by $(\kappa,\mu)\mapsto \mu \otimes m^{\dagger}(\kappa)$.

Define $L':\Delta(\mathcal{F})\times D\to [0,\infty)$ by
\begin{align}
    L'(\xi,y) = \int_E \ell(z,y) m(\xi(E\times \cdot))
\end{align}

Note that
\begin{align}
    L'(h(\kappa,\mu),y) = \int_E \ell(z,y) \kappa(y;dz)
\end{align}

Define $\RV{D}:E\times D\to D$, $\RV{E}:E\times D\to E$ by their respective projections. Define $\RV{X}':E\times F\to X$ by $(a,b,c)\mapsto \RV{X}(a)$.

Then, given the statistical decision problem $\langle(\mathcal{H}',E\times F\times D),D,\RV{X}',L'\rangle$, we have the risk
\begin{align}
    R'(J,h(\kappa,\mu)) &= \int_D \int_E \ell(z,y) \kappa(y;dz)  h(\kappa,\mu) K_{\RV{X}'} J(dy) \\
                  &= \int_D \int_E \ell(z,y) \kappa(y;dz) \mu K_{\RV{X}} J(dy)\\
                      &= \mathbb{E}_{f(J,\kappa,\mu)}[\ell(\RV{E},\RV{D})]
\end{align}
\end{proof}

\begin{theorem}\label{th:cdp_to_sdp}
A causal decision problem $\langle (\mathscr{T},E),D,\RV{X},L\rangle$ where $D$ is a denumerable set and the loss $L$ is an ordinary utility can be reduced to a statistical decision problem.
\end{theorem}

\begin{proof}
Take some test distribution $\pi\in \Delta(\mathcal{D})$ such that $\pi(\{y\})>0$ for all $y\in D$. Such a $\pi$ exists by the denumerability of $\mathcal{D}$.

The map $m:\Delta(\mathcal{E}\otimes\mathcal{D})\to \mathscr{K}$ given by $m(\xi) = \frac{\xi(E\times \cdot\times \{y\})}{\pi(\{y\})}$ is surjective. The result follows from Lemma \ref{lem:red_cdp}.

\end{proof}

This reduction is not particularly practically useful, but it is sufficient to lift results from the theory of statistical decision functions.

\begin{theorem}[Complete class theorem (CDP)]

\end{theorem}

A more straightforward reduction can be made if the causal decision problem is \emph{identifiable}. A causal decision problem is identifiable if the risk of a given decision function is unique given a distribution over the observed data.

\begin{definition}[Identifiability]
A causal decision problem $\langle (\mathscr{T}, E), D, \RV{X}, L \rangle$ is risk-identifiable iff for each $J\in \mathscr{D}$ and $\mu\in \Delta(\mathcal{E})$, $|\{R(J,\kappa,\mu)|(\kappa,\mu)\in \mathscr{T}\}|=1$.

A causal theory $\mathscr{T}$ is identifiable iff for each $\mu\in \Delta(\mathcal{E})$, $|\{(\kappa,\mu)|(\kappa,\mu)\in\mathscr{T}\}|=1$.
\end{definition}

\begin{theorem}[Reduction of identifiable problems]
A risk-identifiable causal decision problem $\langle (\mathscr{T}, E), D, \RV{X}, L \rangle$ where the loss is an ordinary utility can be reduced to a statistical decision problem.
\end{theorem}

\begin{proof}


Choose an arbitrary $(\kappa,\mu)\in\mathscr{T}$ and define $L':\mathscr{H}\times D\to [0,\infty)$ by
\begin{align}
    L'(\nu,y) &= L(\delta_y\underline{[I_D\otimes \kappa]})\\
              &= \int_F \ell(z,y) \kappa(y;dz)
\end{align}

Then the risk $R'$ associated with the statistical decision problem $\langle (\mathscr{H},E),D,\RV{X},L'\rangle$ is given by 
\begin{align}
    R'(J,\mu) &= \int_E \int_F \ell(z,y)  \kappa (y;dz) \mu K_{\RV{X}} J(dy)\\
              &= R(J,\mu,\kappa')
\end{align}
For any $\kappa'\in\mathscr{K}$. The map $g:\mathscr{T}\to\mathscr{H}$ given by $(\mu,\kappa)\mapsto \mu$ is the required surjection.
\end{proof}


\begin{definition}[$\mathscr{H}$-Universal conditional independence]\label{def:univ_indep}
Given measurable spaces $(E,\mathcal{E})$ and $(F,\mathcal{F})$, a kernel $K:E\times\mathcal{F}\to[0,1]$ and random variables $\RV{W}:(E\times F,\mathcal{E}\otimes\mathcal{F})\to (W,\mathcal{W})$, $\RV{X}:(E\times F,\mathcal{E}\otimes\mathcal{F})\to (X,\mathcal{X})$ and $\RV{Y}:(E\times F,\mathcal{E}\otimes \mathcal{F})\to (Y,\mathcal{Y})$. $\mathscr{H}\subset\Delta(\mathcal{E})$ is a class of distributions. We say that $\RV{W}$ and $\RV{Y}$ are $\mathscr{H}$-universally independent with respect to $K$ conditional on $\RV{X}$ iff for all $\mu\in \mathscr{H}$, $\RV{W}\CI_{\mu \overline{K}} \RV{Y} |\RV{X}$. We write this $\RV{Y} \CII_K \RV{W} | \RV{X}$. 

If $\RV{X}$ is a constant (i.e. it induces the trivial $\sigma$-algebra) we have $\mathscr{H}$-universal independence, written $\RV{Y}\CII_K \RV{W}$.
\end{definition}