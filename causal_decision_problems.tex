\section{Causal Decision Problems}

We will work with a slightly restricted version of a statistical decision problem where the sequence of observations is fixed in advance.

\begin{definition}[Statistical Decision Problem]
A statistical decision problem (SDP) is a tuple $\langle (\Omega,\mathcal{F},\mu), (D,\mathcal{D}), \RV{X}, \mathscr{H}, \ell\rangle$. 

$(\Omega,\mathcal{F},\mu)$ is a probability space given by the environment where the distribution $\mu$ is unknown, $(D,\mathcal{D})$ is a measurable space of decisions, $\RV{X}:\Omega\to X$ is a random variable representing a fixed sequence of observations, $\mathscr{H}\subseteq\Delta(\mathcal{F})$ is a hypothesis class known to contain $\mu$ and $\ell:\mathscr{H}\times D\to [0,\infty)$ is a loss.

The aim of a statistical decision problem is to find a decision kernel $J:X\to \Delta(\mathcal{D})$ minimising the risk:
\begin{align}
    R(J,\mu) = \int_{\Omega}\int_D \ell(\mu,y) \mu J(dy)
\end{align}
\end{definition}

Here $\mu J$ is the measure-kernel product defined in \ref{def:kernel_product}. As $\mu$ is generally unknown, it is not possible to do this directly. Given a prior $\pi$ on 


A causal decision problem extends a statistical decision problem in that, while an SDP provides us with a loss $D\times \Delta(\mathcal{F})\to [0,\infty)$ that evaluates (decision, state) pairs, a CDP gives us a loss from $\Delta(\mathcal{F})\to [0,\infty)$ that evaluates only consequences. Evaluating the quality of a decsion in an CDP therefore requires an object of the type $D\to \Delta(\mathcal{F})$, which we call a \emph{consequence kernel}.

\begin{definition}[Consequence kernel]
Given a measurable space $(\Omega,\mathcal{F})$ and a measurable decision set $(D,\mathcal{D})$, a consequence kernel is a Markov kernel $\kappa:D \to \Delta(\mathcal{F})$.
\end{definition}

\begin{definition}[Causal Decision Problem]
A causal decision problem (CDP) is a tuple $\langle (\Omega,\mathcal{F},\kappa^*), (D,\mathcal{D}), L, \mathbf{K} \rangle$. The triple $(\Omega,\mathcal{F},\kappa^*)$ is given by the environment, where $\Omega$ is the sample space and $\mathcal{F}$ is a $\sigma$-algebra on $\Omega$. We posit that a true consequence kernel $\kappa^*$ exists which is only known to belong to the class $\mathbf{K}\subset \Delta(\mathcal{F})^D$.

The measurable set $D$ represents the decisions available, and $L:\Delta(\mathcal{F})\to [0,\infty)$ is the loss.

The objective of a causal decision problem is to choose a stochastic decision $\theta\in \Delta(\mathcal{D})$ such that $L(\theta \kappa^*)$ is minimal.
\end{definition}

In a causal decision problem, the class $\mathbf{K}$ represents the given background knowledge about how the world works. Without an informative class $\mathbf{K}$ it is clearly impossible to determine if any decision is preferred to any other decision.

A statistical causal decision problem is a causal decision problem where we are provided some data which we may use to make a decision. A generalised statistical causal decision problem features a causal prospect $\mathscr{T}$ that plays a similar role to to the class of possible consequences $\mathbf{K}$ for a regular causal decision problem.

\begin{definition}[Generalised Causal Theory]\label{def:gen_causal_theory}
Given measurable space $(\Omega,\mathcal{F})$ and $(X,\mathcal{X})$, a random variable $\RV{X}:\Omega\to X$ and a decision set $D$, a generalised causal theory is a Markov kernel $\tau:X\times D \to \Delta(\mathcal{F})$. The map $x\mapsto \tau(\cdot;x,\cdot)$ maps the set $X$ to a consequence kernel.
\end{definition}

\begin{definition}[Generalised Causal Prospect]\label{def:gen_causal_prospect}
A generalised causal prospect is a set of generalised causal theories.
\end{definition}

\begin{definition}[Generalised Statistical Causal Decision Problem]\label{def:gen_scdp}
A generalised statistical causal decision problem (GSCDP) is a tuple $\langle (\Omega,\mathcal{F},\kappa^*,\mu), (D,\mathcal{D}), L, \RV{X}, \mathscr{T} \rangle$.

The elements $(\Omega, \mathcal{F},\kappa^*), (D,\mathcal{D}), L$ are in common with a causal decision problem. $\RV{X}:\Omega\to X$ is a random variable distributed according to $\RV{X}_*\mu$ where $\mu\in \mathscr{H}\subset \Delta(\mathcal{F})$. $\mathscr{T}$ is a generalised causal prospect containing theories $X\times D\to \Delta(\mathcal{F})$.

The objective of a GSCDP is to find a decision kernel $\phi:X\to \Delta(D)$ such that $L(\mu\phi\kappa^*)$ is minimal.

If a GSCDP is realisable, given arbitray $A\in \mathcal{X}$ such that $\mu(A)>0$, the true kernel $\kappa^*$ is assumed to belong to the set $\{\frac{1}{\mu(A)}\int_A \tau(\cdot;x,\cdot)\mu(dx)|\tau\in\mathscr{T}\}$. That is, we assume the causal prospect is sufficiently large to contain a theory mapping to the true kernel with probability 1.
\end{definition}

A GSCDP can be reduced to a regular CDP. The intuition is that a decision kernel for a statistical causal decision problem can be considered to be an ordinary decision for a regular causal decision problem. If the set of available decision kernels is convex and closed, each stochastic decision in the regular causal decision problem corresponds to a decision kernel in the GSCDP, so each problem presents exactly the same set of options.

\begin{theorem}[Reduction from GSCDP to CDP]\label{th:gscdp_to_cdp}
Given a GSCDP $Q=\langle (\Omega,\mathcal{F},\kappa^*,\mu), (D,\mathcal{D}), L, \RV{X}, \mathscr{T} \rangle$, suppose we further have a set of available decision functions $\Phi\subset \Delta(\mathcal{F})^D$ along with some $\sigma$-algebra $\mathcal{E}$ and a hypothesis class $\mathscr{H}\ni \mu$. Then $Q$ can be reduced to a regular causal decision problem $\langle (\Omega,\mathcal{F},\kappa^{*\prime}), (D',\mathcal{D}'), L, \mathbf{K}' \rangle$ with the identification $(D',\mathcal{D}') = (\Phi,\mathcal{E})$, $\kappa^{*\prime}:\phi\mapsto \mu\phi \kappa^*$, $\mathbf{K}' = \{\phi\mapsto \nu\phi\frac{1}{\nu(A)}\int_A\tau(\cdot;x,\cdot)\nu(dx)|\tau\in\mathscr{T},\nu\in \mathscr{H},\forall A:\nu(A)>0\}$.

If $\Phi$ is convex and closed, then each $\theta\in \Delta(\mathcal{E})$ can be identified with some $\phi\in \Phi$ such that $\theta\kappa^{*\prime} = \mu\phi\kappa^*$.
\end{theorem}

\begin{proof}
For the reduction, we must show that $\kappa^*\in \{\frac{1}{\nu(A)}\int_A \tau(\cdot;x,\cdot)\nu(dx)|\tau\in \mathscr{T},\nu\in\mathscr{H},\forall A:\nu(A)>0\}$ implies $\kappa^{*\prime}\in \mathbf{K}'$. This is trivial by the definition of $\mathscr{H}$, $\mathbf{K}'$ and $\kappa^{*\prime}$.

For the identification, we require for each $\theta\in \Delta(\mathcal{E})$ there is a corresponding $\phi\in \Phi$ such that $L(\mu\phi\kappa^*)=L(\theta\kappa^{*\prime})$. Observe that given any $\theta\in \Delta(\mathcal{E})$ by convexity and closure there exists $\phi_0\in \Phi$ such that $\phi_0 = \int_\Phi \phi \theta(d\phi)$. Therefore $\theta\kappa^{*\prime}=\int_\Phi \mu \phi \kappa^* \theta(d\phi) = \mu \int_\Phi \phi \theta(d\phi) \kappa^*=\mu\phi_0\kappa^*$.
\end{proof}