\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2019

% ready for submission
% \usepackage{neurips_2019}

% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
%     \usepackage[preprint]{neurips_2019}

% to compile a camera-ready version, add the [final] option, e.g.:
     \usepackage[final]{neurips_2019}

% to avoid loading the natbib package, add option nonatbib:
%     \usepackage[nonatbib]{neurips_2019}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography

% My packages

\usepackage[mathscr]{euscript}
\usepackage{natbib}
\usepackage{graphicx}
\usepackage {tikz}
\usetikzlibrary {positioning}
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{dsfont}
\usepackage{hyperref}
\usepackage{stmaryrd }
\usepackage{csquotes}
\usepackage{wasysym}

\theoremstyle{plain}
\newtheorem{theorem}{Theorem}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{lemma}[theorem]{Lemma}

\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]
\newtheorem{example}{Example}[section]

\newcommand{\CI}{\mathrel{\text{\scalebox{1.07}{$\perp\mkern-10mu\perp$}}}}
\newcommand{\CII}{\mathrel{\text{\scalebox{1.07}{$\perp\mkern-10mu\perp\mkern-10mu\perp$}}}}
\newcommand{\RV}[1]{\ensuremath{\mathsf{#1}}}
\newcommand{\PA}[2]{\ensuremath{\text{Pa}_{#1}(#2)}}
\newcommand{\ND}[2]{\ensuremath{\text{ND}_{#1}(#2)}}
\newcommand{\CH}[2]{\ensuremath{\text{Ch}_{#1}(#2)}}
\newcommand{\DE}[2]{\ensuremath{\text{De}_{#1}(#2)}}
\makeatletter
\newcommand*\bigcdot{\mathpalette\bigcdot@{.5}}
\newcommand*\bigcdot@[2]{\mathbin{\vcenter{\hbox{\scalebox{#2}{$\m@th#1\bullet$}}}}}
\makeatother

\newcommand\splitter[1]{%
\begin{tikzpicture}[scale=#1]
\draw (0,-1) -- (0,0);
\draw (0,0) to [bend right] (1,1);
\draw (0,0) to [bend left] (-1,1);
\end{tikzpicture}
}

\newcommand\stopper[1]{%
\begin{tikzpicture}[scale=#1]
\draw (0,-1) -- (0,0);
\node (E) at (0,0) {$\bigcdot$};
\end{tikzpicture}
}

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\arginf}{arg\,inf}
\DeclareMathOperator*{\argsup}{arg\,sup}

\newcommand{\cheng}[1]{ {\color{purple}[{\bf Cheng:~{#1}}]} }


\title{How to ask a causal question}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.

\author{%
  David Johnston \\
  College of Engineering and Computer Science\\
  Australian National University and DATA61\\
  ACT, Australia 0200 \\
  \texttt{david.johnston1@anu.edu.au} \\
  % examples of more authors
   \And
  Cheng Soon Ong\\
  DATA61 and Australian National University\\
  % Address \\
  % \texttt{email} \\
   \And
   Robert Williamson \\
   Australian National University and DATA61\\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
}


\begin{document}

\maketitle

\begin{abstract}
There are multiple competing approaches to the handling of causality in statistical inference, including Causal Bayesian Networks and Potential Outcomes which differ in part in their underlying conceptions of causality. In an approach similar to Dawid, we develop the notion of a causal statistical decision problem patterned after the statistical decision theory of Wald. Our approach is motivated by a simple consideration: suppose we have a dataset, some set of available decisions and we know what state we would like the world to occupy, but we are uncertain about how our decisions affect the state of the world. We introduce the notion of \emph{consequence } that relate decisions to states of the world, and \emph{causal theories} that relate observations to consequences. These definitions are not motivated by minimal ``causal'' considerations, save the need to connect observations, decisions and consequences. We connect causal statistical decision problems to statistical decision problems via a reduction that allows results from the latter to sometimes be imported to the former. We show that Causal Bayesian Networks and Potential Outcomes both have a natural mapping to causal theories, and demonstrate a straightforward example of a causal theory that cannot be unambiguously represented by either. We argue by example that, given this more general perspective, the standard understanding of a Causal Bayesian Network is only justified under additional nontrivial assumptions. Finally, we conclude with a long list of open questions raised by this new perspective.
\end{abstract}

\input{story}
\input{markov_kernels}
\input{causal_decision_problems}
\input{causal_bayesian_networks}
\input{counterfactuals}

\section{Conclusion}

We have formally specified a causal statistical decision problem proceeding as a generalisation of statistical decision problems where our preferences over outcomes are known and the distribution over outcomes in consequence of a particular decision is uncertain. CSDPs may be connected to regular SDPs at a high level via a reduction, and this permits the theoretical treatment of risk minimisation in the context of causal problems. 

The existing approaches of Causal Bayesian Networks and Potential Outcomes have natural interpretations as causal theories. We have argued for the utility of a more general approach via two examples. First, we show that a causal theory associated with a CBN $\mathcal{G}$ may be extended to cover distributions incompatible with $\mathcal{G}$ in different ways that yield very different results. Secondly, we show that the possibility of \emph{performance bias} is a natural consideration in CSDPs while it is often treated informally in other systems.

This perspective raises many questions:
\begin{itemize}
    \item Under what conditions do versions of the No-Free Lunch theorems hold for CSDPs?
    \item Example \ref{ex:extn_cbn} deals with a crude notion of ``continuity'' of a causal theory. Generally, what properties may be used to characterise the learnability of a causal theory?
    \item Do alternative approaches to causal inference such as Identifiable Functional Model Classes (IFMOCs) or invariance based methods have natural representations as causal theories?
    \item The notation here borrows heavily from \citep{fong_causal_2013}, who has developed a diagrammatic representation of Markov kernel itself closely related to the directed acyclic graphs associated with CBNs. This raises the question: beyond CBNs, can consequence maps be generically and informatively be represented using DAG-like diagrams?
    \item We have proposed consequence maps and causal theories as ``relatively minimal'' objects to satisfy the need to connect data, decisions and outcomes. Are there strictly more general objects that may be used instead, and if so under what assumptions are consequence maps and causal theories necessary?
\end{itemize}

The general perspective proposed in this paper naturally incorporates the two major causal inference frameworks and, for the first time to our knowledge, allows a range of fundamental questions to be formally posed, such as \emph{what are the characteristics of a causal statistical decision problem that make it ``learnable”?} Whilst we don’t have all the answers, at least we have opened the way to ask such foundational questions!

\bibliographystyle{plainnat}
\bibliography{references}


\include{appendix_MK}
\include{appendix_CSDP}
\include{appendix_CB}

\end{document}
