\section{Introduction}

The decision theoretic approach to statistics plays a role of fundamental importance in modern machine learning; loss functions underpin the development of algorithms, and the analysis of losses is critical to the theoretical treatment of learning algorithms.

Problems of causal inference make different demands to statistical problems, though statistical inference often plays a role in addressing them (\cite{pearl_causality:_2009}). There are two broad approaches to causality, one based on graphical models and one based on the joint distribution of observations and ``potential outcomes''. While both approaches can be applied to decision making problems, such applications require decision theoretic interpretation of notions more basic to the causal frameworks such as ``causal effect'' and ``treatment effect''.

The identification of causal effects under either framework typically requires strong assumptions that are untestable and do not hold generically. This is the case, for example, for \emph{conditional ignorability} under potential outcomes (\cite{gordon_comparison_2018, heckman_randomization_1991}), or for assessing whether an appropriate causal graph admits the identifiability of some effect (\cite{tian2002general}).The evaluation of causal effects, therefore, usually requires expert input in order to assess these assumptions. 

In contrast, substantial progress in machine learning has been the result of developing generic principles and learning techniques that are relevant to many datasets from many domains and are less reliant on the judgement of domain experts. There is substantial interest in discovering and studying analogous generic principles of causal inference. Generic approaches cannot be expected to yield results as strong as approaches that involve expert input. For example, the non-generic assumption of ignorability is often licenced by knowledge of experimental design that is not itself to be found in the data in question, so any learner equipped with only generic assumptions and the data in question must be starting at a disadvantage. Nonetheless, this does not exclude the possibility that generic approaches may yield nontrivial results in the right circumstances.

There are two main assumptions used so far in pursuing generic causal inference: \emph{faithfulness} and the \emph{independence of cause and mechanism}. The assumption of faithfulness (together with the Causal Markov Condition) facilitates the exclusion of some graphical models on the basis of conditional independences found in the data \citep{spirtes_causation_1993} while the principle of independence of cause and mechanism enables the use of a number of special purpose techniques to assess the \emph{algorithmic independence} of marginal and conditional distributions which leads to preferment of some graphical models over others \citep{lemeire_replacing_2013, peters_identifiability_2012}.



Both faithfulness and the independence of cause and mechanism are employed in learning causal Bayesian networks. While these are undoubtedly useful tools for causal reasoning, causal Bayesian networks are not ideal objects for the analysis of causal learning at a very general level:
\begin{itemize}
    \item There are causal models which cannot be captured by a DAG \citep{dawid_beware_2010,bongers_theoretical_2016}
    \item There is controversy over how causal Bayesian networks should be adapted to answer counterfactual questions \citep{richardson2013single}
    \item Under appropriate conditions, different graphs may induce the same set of interventional distributions \citep{peters_structural_2015}
    \item A standard causal Bayesian network posits an intervention operation for every variable under consideration, while the actions to be evaluated may be much more limited. Learning such a graph appears to run afoul of Vapnik's precept: \emph{When solving a given problem, try to avoid solving a more general problem as an intermediate step} \citep{vapnik_nature_2013}
\end{itemize}

We propose a general account of causal inference problems motivated by a decision theoretic approach. While we do not claim that such an approach subsumes every causal question that someone may be interested in, we do claim that a very large number of causal questions have a natural decision theoretic formulation.

Many problems of causal inference are concerned with the consequences of decisions; for example, whether a treatment will promote a patient's recovery, whether a policy will, on average, benefit those affected by it or whether inactivating a particular gene will have a desired effect on the resulting organism. Others may be concerned with the comparison of actual consequences and counterfactual states, such as the test in tort law that asks ``but for the defendant's negligence, would the plaintiff have been injured''? 

We take the view that in both cases, given preferences over decisions, consequences (and, if necessary, counterfactuals), a good decision is a sufficient answer to a causal inference problem. We propose an approach to causal inference based on extending the classic framework of statistical decision theory developed by \cite{wald_statistical_1950}. A classical statistical decision problem arises when we have access to data $\RV{X}$ generated by some state $\mu\in \mathscr{H}$, have some set of available decisions $\RV{D}$ and can articulate preferences over (state,decision) pairs. In contrast, a causal decision problem arises when we cannot easily articulate preferences over (state, decision) pairs but we can articulate preferences over the possible states of the world that follow in consequence from our decision.

In order to connect decisions with preferences, we therefore require a stochastic mapping from decisions $\RV{D}$ to possible future states. We term this mapping a \emph{consequence}, which can be analogized with the data-generating distribution in statistical decision theory. A \emph{causal theory} relates the states that we may observe with possible consequences.

We develop connections between causal decision problems and statistical decision problems, and show that a causal theory is a generalization of a causal Bayesian network. We show that a CBN, in general, leads to an underspecified causal theory with different choices of theory leading to very different behaviour. The counterfactual case requires an extension of the notion of a consequence, which we connect with a number of existing representations of counterfactual distributions including single world intervention graphs and structural equation models.


