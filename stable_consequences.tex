%!TEX root = main.tex

\section{Reproducibility}

A natural assumption suggested by the notion of a CSDP is that of \emph{reproducibility} - that a causal theory $\mathscr{T}:E\times D\rightarrowtriangle E$ permits some decision function that reproduces the distribution of the observed data. Without loss of generality, we assume that for every $(\kappa,\mu)\in \mathscr{T}$ there exists $\gamma_{\kappa,\mu}\in \Delta(\mathcal{D})$ such that
\begin{align}
	\gamma_{\kappa,\mu}\kappa = \mu
\end{align}

A common causal assumption is \emph{variable forcing}: suppose that there exists a variable $\RV{X}:E\to D$ such that, given a decision $y \in D$, the resulting distribution of $\RV{X}$ is $\delta_y$. That is, for all $y\in D$, $\delta_y\kappa F_{\RV{X}} = \delta_y$; this is equivalent to the claim that the Markov kernel $F_{\RV{X}}$ is a right inverse of $\kappa$, which we will subsequently denote $\kappa^\dagger$. This assumption is found explicitly in the Bayesian network literature as ``hard interventions'' and while no such explicit assumption is made in the Potential Outcomes literature, it is typical to posit counterfactual worlds predicated on certain variables being set to certain values.

Given these two assumptions, we have
\begin{align}
	\begin{tikzpicture}[scale=0.8]
		\path (0,0) node[dist] (A) {$\mu$}
		++(0,1) coordinate (C)
		+(1,2) node (F) {$*$}
		++(-1,1) node[kernel] (E) {$\kappa^\dagger$}
		++(0,1) coordinate (G)
		+(1,0.5) node[kernel] (D) {$\kappa$}
		+(0,1) coordinate (H)
		+(1,1) coordinate (I);
		\draw (A) -- (C);
		\draw (C) to [bend right] (F.center);
		\draw (C) to [bend left] (E);
		\draw (E) -- (G);
		\draw (G) -- (H);
		\draw (G) to [bend right] (D);
		\draw (D) to (I);
	\end{tikzpicture}&=
		\begin{tikzpicture}[scale=0.8]
		\path (0,0) node[dist] (A) {$\mu$}
		++(0,1) node[kernel] (E) {$\kappa^\dagger$}
		++(0,0.5) coordinate (G)
		+(1,0.5) node[kernel] (D) {$\kappa$}
		+(-1,1) coordinate (H)
		+(1,1) coordinate (I);
		\draw (A) -- (E);
		\draw (E) -- (G);
		\draw (G) to [bend left] (H);
		\draw (G) to [bend right] (D);
		\draw (D) to (I);
	\end{tikzpicture}\\
	&=
		\begin{tikzpicture}[scale=0.8]
		\path (0,0) node[dist] (A) {$\mu$}
		++(0,1) coordinate (G)
		+(1,1) node[kernel] (B) {$\kappa^\dagger$}
		+(-1,1) node[kernel] (C) {$\kappa^\dagger$}
		+(1,2) node[kernel] (D) {$\kappa$}
		+(-1,3) coordinate (H)
		+(1,3) coordinate (I);
		\draw (A) -- (G);
		\draw (G) to [bend left] (C);
		\draw (G) to [bend right] (B);
		\draw (C) to (H);
		\draw (B) to (D);
		\draw (D) to (I);
	\end{tikzpicture}\\
	&=
	\begin{tikzpicture}[scale=0.8]
		\path (0,0) node[dist] (A) {$\mu$}
		++(0,1) coordinate (B) 
		+(1,2) coordinate (C)
		+(-1,1) node[kernel] (D) {$\kappa^\dagger$}
		+(-1,2) coordinate (E);
		\draw (A) -- (B);
		\draw (B) to [bend right] (C);
		\draw (B) to [bend left] (D);
		\draw (D) -- (E);
	\end{tikzpicture}\\
	&:= \mu^{\dagger|}
\end{align}

Here we recall the definition of a conditional probability: a conditional probability \inkernel{\RV{Y}|\RV{X}}{}{} with respect to $\nu\in \Delta(\mathcal{E})$ and RVs $\RV{X}:E\to X$, $\RV{Y}:E\to Y$ is any Markov kernel $X\to \Delta(\mathcal{Y})$ such that
\begin{align}
	\begin{tikzpicture}[scale=0.7]
	\path (0,0) node[dist,inner sep=2pt] (A) {$\nu$}
	+ (0.5,1) node (B) {$X$}
	+ (-0.5,1) node (C) {$Y$};
	\draw (B) -- (B|- A.north);
	\draw (C) -- (C|- A.north);
	\end{tikzpicture} = 
	\begin{tikzpicture}[scale=0.7]
	\path (0,0) node[dist,inner sep=2pt] (A) {$\nu$}
	+ (-0.5,1) node (C) {$*$}
	++ (0.5,1) coordinate (B)
	+ (-1,1) node[kernel] (D) {$\RV{Y}|\RV{X}$}
	+(0,2) node (E) {$X$}
	+(-1,2) node (F) {$Y$};
	\draw (B) -- (B|- A.north);
	\draw (C.center) -- (C|- A.north);
	\draw (B) -- (E);
	\draw (B) to [bend left] (D);
	\draw (D) -- (F);
	\end{tikzpicture}
\end{align}

Thus we can see that $\kappa$ is a conditional probability \inkernel{\RV{E}|\RV{D}}{}{} with respect to $\mu^{\dagger|}$ the RVs $\RV{D}:E\times D\to D$ and $\RV{E}:E\times D\to E$ given by the respective projections.

This result is in line with two familiar cases from the causal literature: an \emph{ignorable} treatment in potential outcomes or a graphical model with no back-door paths between $\RV{D}$ and $\RV{E}$ both imply that the theory's respective version of ``causal effect'' can be determined directly from the conditional probability. This suggests that reproducibility is in fact quite a strong condition, as it is the exception rather than the norm where causal effects can be determined directly from conditional probabilities. There are two important caveats here, however: firstly we've assumed $\kappa$ has a known right-inverse $\kappa^\dagger$, and secondly this result only allows $\kappa$ to be determined $\mu^{\dagger|}$-almost surely.

\todo[inline]{A notion of ``approximate right-invertibility'' would be very helpful here. Something like $\kappa$ is approximately right invertible if there is some $\tilde{\kappa}^\dagger$ such that $\gamma \kappa\tilde{\kappa}^\dagger$ is always ``close'' to $\gamma$. Intuitively, a consequence that is \emph{not} approximately right invertible has a channel capacity of 0, and \textbf{Conjecture:} a set of kernels that do not collectively admit some approximate right-inverse do not admit preferences between decisions under the minimax rule}

\section{More Complex Cases}

Suppose we retain the right invertibility of $\kappa$ via some known $\kappa^\dagger$, which is consistent with (for example) hard interventions and counterfactual variable forcing. A more common way that there fails to be a memoryless function $J_\theta$ such that $J_\theta\kappa=\mu$ arises from the fact that we may often regard the consequence as dynamic. Assign an index $\theta\in \Theta$ to each causal state, which we now denote $(\kappa_\theta,\mu_\theta)\in \mathscr{T}$ and let $\kappa^\Theta:\Theta\times D\to \Delta(\mathcal{E})$ be defined by $\kappa^\Theta:(\theta,y;A)\mapsto \kappa_\theta(y;A)$ which we will assume is also a Markov kernel (noting that this intoduces additional restrictions on $\mathscr{T}$ in the general case). We may assume, then, for some state-dependent generating distribution $\xi_\theta\in \Delta(\Theta)$ and some state dependent ``passive decision map'' $K_\theta:\Theta\to \Delta(\mathcal{D})$, we have $\mu_\theta = \xi_\theta\splitter{0.1}(I_{(\Theta)}\otimes K_\theta)\kappa^\Theta$. 

\indist{a}{b}

\inkernel{ab}{a}{b}