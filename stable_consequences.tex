%!TEX root = main.tex

\section{The story at a high level}

Take a causal theory $\mathscr{T}$ where we label each pair $\theta:=(\kappa_\theta,\mu_\theta)\in\mathscr{T}$. Define the kernels $\kappa:\mathscr{T}\times D\to \mathcal{E}$ and $\mu:\mathscr{T}\to\mathcal{E}$ by $\kappa:(\theta,y;A)\mapsto \kappa_\theta(y;A)$ and $\mu:(\theta;A)\mapsto \mu_\theta(A)$.

\paragraph{Optmizibility:} I make the claim (unproven) that it is possible to find a ``universally optimal'' decision function if the following identity holds for all decision functions $J:E\to \Delta(\mathcal{D})$:

\begin{align}
\begin{tikzpicture}
\path (0,0) coordinate (A)
	  ++(0.5,0) coordinate (copy0)
	  ++(0.5,0) node[kernel] (mu) {$\mu$}
	  ++(0.5,0) node[kernel] (J) {$J$}
	  ++(0.5,0) node[kernel] (kap) {$\kappa$}
	  ++(0.5,0) node[expectation] (u) {$u$};
\draw (A) -- (mu) -- (J) -- (kap) -- (u);
\draw (copy0) to [bend right = 40] (kap);
\end{tikzpicture}
=
\begin{tikzpicture}
\path (0,0) coordinate (A)
	  ++(0.5,0) node[kernel] (mu) {$\mu$}
	  ++(0.5,0) node[kernel] (be) {$\beta$}
  	  ++(0.5,0) coordinate (copy0)
  	  +(0.5,-0.5) node[kernel] (al) {$\alpha$}
	  ++(0.5,0) node[kernel] (J) {$J$}
	  ++(0.5,0) node[kernel] (kap) {$\kappa$}
	  ++(0.5,0) node[expectation] (u) {$u$};
\draw (A) -- (mu) -- (be) -- (J) -- (kap) -- (u);
\draw (copy0) to [bend right = 40] (al) (al) to [bend right = 40] (kap);
\end{tikzpicture} \label{eq:lift_the_string}
\end{align}

The reason why I think this is optimizable is that the decision function $J$ ``sees'' all the input to $\kappa$.

If the forward direction holds, the reverse direction does not hold - we can take a problem that respects \ref{eq:lift_the_string} and introduce additional dominated decisions that break \ref{eq:lift_the_string} without breaking the ``universal optimizability'' (i.e. decisions we know to be very bad, but exactly how bad depends on the state in a difficult-to-identify manner). It is an open question whether the reverse direction might hold if we exclude such decisions.

\paragraph{Sufficient conditions for optimizibility:} It is easy to show that \ref{eq:lift_the_string} holds if there exists some kernel $^*\mu$ such that the following two identities hold:

\begin{align}
\begin{tikzpicture}
\path (0,0) node[kernel] (kap) {$\kappa$}
 + (-0.8,0.15) node[kernel] (mdm) {$\mu^*\mu$};
\draw ($(kap.west) +(-1.25,-0.15)$) -- ($(kap.west) +(0,-0.15)$);
\draw (kap) -- (0.5,0);
\draw ($(mdm.west)+(-0.2,0)$) -- (mdm) -- ($(kap.west) +(0,0.15)$);
\end{tikzpicture} &=
\begin{tikzpicture}
\path (0,0) node[kernel] (kap) {$\kappa$};
\draw ($(kap.west) +(-0.5,0.15)$) -- ($(kap.west) +(0,0.15)$) ($(kap.west) +(-0.5,-0.15)$) -- ($(kap.west) +(0,-0.15)$);
\draw (kap) -- (0.5,0);
\end{tikzpicture}\label{eq:kappa_factorises}\\
\begin{tikzpicture}
\path (0,0) node (A) {}
++ (0.5,0) node[kernel] (mu0) {$\mu$}
++ (0.5,0) node[kernel] (smu) {$^*\mu$}
++ (0.5,0) coordinate (copy0)
+  (0.5,0.5) coordinate (B)
+  (1,0.5) coordinate (T)
+  (0.5,-0.5) node[kernel] (mu1) {$\mu$}
+  (1,-0.5) coordinate (E);
\draw (A) -- (mu0) -- (smu) -- (copy0);
\draw (copy0) to [bend left] (B);
\draw (B) -- (T);
\draw (copy0) to [bend right] (mu1);
\draw (mu1) -- (E);
\end{tikzpicture}&=
\begin{tikzpicture}
\path (0,0) node (A) {}
++ (0.5,0) coordinate (copy0)
++ (0.5,0.5) node[kernel] (mu0) {$\mu$}
++ (0.5,0) node[kernel] (smu) {$^*\mu$}
++ (0.5,0) coordinate (T);
\path (copy0)
++  (0.5,-0.5) node[kernel] (mu1) {$\mu$}
++  (1,0) coordinate (E);
\draw (A) -- (copy0);
\draw (copy0) to [bend left] (mu0);
\draw (mu0) -- (smu) -- (T);
\draw (copy0) to [bend right] (mu1);
\draw (mu1) -- (E);
\end{tikzpicture}\label{eq:know_mu}
\end{align}

The first condition says that $\kappa$ is fixed on the support of $\mu^*\mu$. 

The second is less obvious. It implies that if we ``guess'' the underlying state via $\mu^*\mu$ this is as good as having the actual underlying state for the purposes of determining the output of $\mu$, but it is stronger than this. In particular, the \emph{joint distribution} between the ``guess'' and the observations must be the same whether we use the guess or the true underlying state as input to $\mu$.

Two sufficient conditions for \ref{eq:know_mu} to obtain are 1) when $\mu$ is deterministic (as $\mu$ then has a left inverse) and 2) if observations are an infinite sequence of binary random variables where each $\mu_\theta$ corresponds to a Bernoulli distribution for a particular parameter $p_\theta$ (via a $^*\mu$ that witnesses the strong law of large numbers).

A more general sufficient graphical condition is available, but it is not presently clear if it is also a necessary one.

These conditions are not necessary for \ref{eq:lift_the_string}; observations may be ``too informative''. For example, if $\mathscr{T}$ contains many different $\mu_\theta$ but only one $\kappa_\theta$, then we can always perform \ref{eq:lift_the_string}, while we do not generally have \ref{eq:know_mu}.

Below, I document additional assumptions that, along with \ref{eq:know_mu} yield \ref{eq:kappa_factorises}.

I'm not sure how interesting the assumptions themselves are. One interesting point about the big picture story is that from one point of view the assumptions boil down to:
\begin{itemize}
	\item We can characterise the input-output behaviour of $\kappa$ for any given state and a small subset of available decisions
	\item $\kappa$ is sufficiently regular that its behaviour on said subset of decisions characterises its complete behaviour
\end{itemize}

\section{Recoverability}

A natural assumption suggested by the notion of a CSDP is that of \emph{recoverability} - that a causal theory $\mathscr{T}:E\times D\rightarrowtriangle E$ permits some decision function that reproduces the distribution of the observed data. That is, we assume that for every $(\kappa_\theta,\mu_\theta):=\theta\in \mathscr{T}$ there exists $\gamma_{\theta}\in \Delta(\mathcal{D})$ such that
\begin{align}
	\gamma_{\theta}\kappa_\theta = \mu_\theta \label{eq:recoverability}
\end{align}

``Traditional'' causal inference doesn't have a strict equivalent of this assumption, though it corresponds roughly to the ``easy'' cases (for example, it is satisfied by a CBN where there are no backdoor paths between the ``intervened'' variable and the ``target'' variable). One reason I think it's interesting is that \emph{randomised recoverability} may be quite a general assumption - that is, there is ``in principle'' a stochastic decision that recovers the observed distribution, but we are practically limited to taking mixed decisions that cannot necessarily accomplish this.

Suppose also that we have some $\kappa^*$ that, for all $\theta \in \mathscr{T}$, is a Bayesian inversion of $\gamma_\theta$ and $\kappa_\theta$; that is:

\begin{align}
\begin{tikzpicture}
	\path (0,0) node[dist] (gam) {$\gamma_\theta$}
	++(0.5,0) coordinate (copy0)
	+(0.5,0.5) node[kernel] (kap) {$\kappa_\theta$}
	+(0.5,-0.5) coordinate (ph)
	+(1,0.5) node (E) {$\RV{E}$}
	+(1,-0.5) node (D) {$\RV{D}$};
	\draw (gam) -- (copy0);
	\draw (copy0) to [bend left] (kap);
	\draw (copy0) to [bend right] (ph);
	\draw (kap) -- (E);
	\draw (ph) -- (D); 
\end{tikzpicture}= \begin{tikzpicture}
	\path (0,0) node[dist] (gam) {$\gamma_\theta$}
	++(0.6,0) node[kernel] (kap) {$\kappa_\theta$}
	++(0.5,0) coordinate (copy0)
	+(0.5,-0.5) node[kernel] (kapdag) {$\kappa^*$}
	+(0.5,0.5) coordinate (ph)
	+(1,0.5) node (E) {$\RV{E}$}
	+(1,-0.5) node (D) {$\RV{D}$};
	\draw (gam) -- (kap);
	\draw (kap) -- (copy0);
	\draw (copy0) to [bend right] (kapdag);
	\draw (copy0) to [bend left] (ph);
	\draw (kapdag) -- (D);
	\draw (ph) -- (E); 
\end{tikzpicture} \label{eq:kappa_BI}
\end{align}

A sufficient condition for the existence of such a $\kappa^*$ is the assumption that decisions correspond to \emph{variable setting} - that is, there is some variable $\RV{X}:E\to X$ such that for all $a\in D$, $\theta\in\mathscr{T}$ we have $\delta_a \kappa_\theta F_{\RV{X}} = \delta_a$ (such an assumption arises in graphical models as hard interventions, and in potential outcomes as ``potential-outcome identifiers''). Indeed $F_{\RV{X}}$ is in this case a candidate for $\kappa^*$. It is not necessary that $\kappa^*$ be deterministic, however - suppose every $\kappa$ ignores $D$. Then choose $\gamma_\theta=\gamma$ for arbitrary $\gamma\in \Delta(\mathcal{D})$ and it can be verified that $\kappa^*:b\mapsto \gamma$ satisfies \ref{eq:kappa_BI}. 

I believe a weaker sufficient condition for the existence of a universal $\kappa^*$ is that every $\kappa_\theta$ factorises as $\kappa_\theta = h \splitter{0.1}(\mathrm{Id}_F\otimes j_\theta)$ for some fixed $h:D\to \Delta(\mathcal{F})$, but I have not yet shown this.

We will proceed somewhat rashly: suppose that by defining $\gamma:\mathscr{T}\to \Delta(\mathcal{D})$, $\mu:\mathscr{T}\to \Delta(\mathcal{E})$ and $\kappa:\mathscr{T}\times D\to \Delta(\mathcal{E}$ by $\gamma:\theta\to \gamma_\theta$, $\mu:\theta\to \mu_\theta$ and $\kappa:(\theta,d)\to \kappa_\theta(d;\cdot)$ that all resulting objects are Markov kernels, and that $\mathscr{T}$ is a standard measurable space.

By previous assumptions, we have the following properties:
\begin{align}
\begin{tikzpicture}
	\path (0,0) coordinate (A)
	++ (0.5,0) node[kernel] (B) {$\mu$}
	++ (0.5,0) coordinate (C);
	\draw (A) -- (B);
	\draw (B) -- (C);
\end{tikzpicture}
&=
\begin{tikzpicture}
	\path (0,0) coordinate (A)
	++ (0.3,0) coordinate (copy0)
	++(0.5,0) node[kernel] (gam) {$\gamma$}
	+ (0,0.3) coordinate (B)
	++(0.5,0) node[kernel] (kap) {$\kappa$}
	++(0.5,0) coordinate (C);
	\draw (A) -- (copy0);
	\draw (copy0) to [bend left] (B);
	\draw (copy0) -- (gam);
	\draw (B) to [bend left] (kap.west);
	\draw (gam) -- (kap);
	\draw (kap) -- (C);
\end{tikzpicture}\label{eq:recoverabilityd}\\
\begin{tikzpicture}
	\path (0,0) coordinate (A)
	++ (0.3,0) coordinate (copy0)
	++ (0.5,0) node[kernel] (gam) {$\gamma$}
	++ (0.5,0) coordinate (copy1)
	+ (0.5,-0.5) node[kernel] (kap) {$\kappa$}
	+ (0.5,0.5) coordinate (B)
	+ (1,-0.5) node (E) {$\RV{E}$}
	+ (1,0.5) node (D) {$\RV{D}$};
	\draw (A) -- (copy0);
	\draw (copy0) -- (gam);
	\draw (copy0) to [bend right] (kap);
	\draw (gam) -- (copy1);
	\draw (copy1) to [bend left] (B);
	\draw (copy1) to [bend right] (kap);
	\draw (kap) to (E);
	\draw (B) to (D);
\end{tikzpicture}
&=
\begin{tikzpicture}
	\path (0,0) coordinate (A)
	++ (0.3,0) coordinate (copy0)
	++ (0.5,0) node[kernel] (gam) {$\gamma$}
	++ (0.5,0) node[kernel] (kap) {$\kappa$}
	++ (0.5,0) coordinate (copy1)
	+ (0.5,0.5) node[kernel] (kapdag) {$\kappa^*$}
	+ (0.5,-0.5) coordinate (B)
	+ (1,-0.5) node (E) {$\RV{E}$}
	+ (1,0.5) node (D) {$\RV{D}$};
	\draw (A) -- (copy0);
	\draw (copy0) -- (gam);
	\draw (copy0) to [bend right=40] (kap);
	\draw (gam) -- (kap);
	\draw (kap) -- (copy1);
	\draw (copy1) to [bend right] (B);
	\draw (copy1) to [bend left] (kapdag);
	\draw (kapdag) to (D);
	\draw (B) to (E);
\end{tikzpicture} \label{eq:general_kapdag}\\
&= 
\begin{tikzpicture}
	\path (0,0) coordinate (A)
	++ (0.3,0) coordinate (copy0)
	++ (0.5,0) node[kernel] (mu) {$\mu$}
	++ (0.5,0) coordinate (copy1)
	+ (0.5,0.5) node[kernel] (kapdag) {$\kappa^*$}
	+ (0.5,-0.5) coordinate (B)
	+ (1,-0.5) node (E) {$\RV{E}$}
	+ (1,0.5) node (D) {$\RV{D}$};
	\draw (A) -- (copy0);
	\draw (copy0) -- (mu);
	\draw (mu) -- (copy1);
	\draw (copy1) to [bend right] (B);
	\draw (copy1) to [bend left] (kapdag);
	\draw (kapdag) to (D);
	\draw (B) to (E);
\end{tikzpicture}\label{eq:gamkap_2_mu}
\end{align}
From \ref{eq:general_kapdag} we also have

\begin{align}
\begin{tikzpicture}
	\path (0,0) coordinate (A)
	++ (0.3,0) coordinate (copy0)
	++ (0.5,0) node[kernel] (gam) {$\gamma$}
	++ (0.5,0) coordinate (copy1)
	+ (0.5,-0.5) node[kernel] (kap) {$\kappa$}
	+ (0.5,0.5) coordinate (B)
	+ (1.1,-0.5) node (E) {}
	+ (1.1,0.5) node (D) {$\RV{D}$};
	\draw (A) -- (copy0);
	\draw (copy0) -- (gam);
	\draw (copy0) to [bend right] (kap);
	\draw (gam) -- (copy1);
	\draw (copy1) to [bend left] (B);
	\draw (copy1) to [bend right] (kap);
	\draw[-{Rays [n=8]}] (kap) to (E);
	\draw (B) to (D);
\end{tikzpicture}
&=
\begin{tikzpicture}
	\path (0,0) coordinate (A)
	++ (0.3,0) coordinate (copy0)
	++ (0.5,0) node[kernel] (gam) {$\gamma$}
	++ (0.5,0) node[kernel] (kap) {$\kappa$}
	++ (0.5,0) coordinate (copy1)
	+ (0.5,0.5) node[kernel] (kapdag) {$\kappa^*$}
	+ (0.5,-0.5) coordinate (B)
	+ (1.1,-0.5) node (E) {}
	+ (1.1,0.5) node (D) {$\RV{D}$};
	\draw (A) -- (copy0);
	\draw (copy0) -- (gam);
	\draw (copy0) to [bend right=40] (kap);
	\draw (gam) -- (kap);
	\draw (kap) -- (copy1);
	\draw (copy1) to [bend right] (B);
	\draw (copy1) to [bend left] (kapdag);
	\draw (kapdag) to (D);
	\draw[-{Rays [n=8]}] (B) to (E);
\end{tikzpicture} \\
\begin{tikzpicture}
	\path (0,0) coordinate (A)
	++ (0.3,0) coordinate (copy0)
	++ (0.5,0) node[kernel] (gam) {$\gamma$}
	+ (0.5,0) node (D) {$\RV{D}$};
	\draw (A) -- (copy0);
	\draw (copy0) -- (gam);
	\draw (gam) -- (D);
\end{tikzpicture}
&=
\begin{tikzpicture}
	\path (0,0) coordinate (A)
	++ (0.5,0) node[kernel] (mu) {$\mu$}
	+ (0.5,0) node[kernel] (kapdag) {$\kappa^*$}
	+ (1.1,0) node (D) {$\RV{D}$};
	\draw (A) -- (mu);
	\draw (mu) --  (kapdag);
	\draw (kapdag) to (D);
\end{tikzpicture} \label{eq:kappa_cancel}
\end{align}
Where \ref{eq:kappa_cancel} follows from \ref{eq:recoverability}.

The following assumption is a formalisation of the notion that ``we can determine $\mu$ precisely from observation'' (alternatively, that we can find an optimal decision for a classical statistical decision problem). Suppose that $\mu$ is characterised by some kernel $^*\mu$. That is,

\begin{align}
\begin{tikzpicture}
\path (0,0) node (A) {}
++ (0.5,0) node[kernel] (mu0) {$\mu$}
++ (0.5,0) node[kernel] (smu) {$^*\mu$}
++ (0.5,0) coordinate (copy0)
+  (0.5,0.5) coordinate (B)
+  (1,0.5) coordinate (T)
+  (0.5,-0.5) node[kernel] (mu1) {$\mu$}
+  (1,-0.5) coordinate (E);
\draw (A) -- (mu0) -- (smu) -- (copy0);
\draw (copy0) to [bend left] (B);
\draw (B) -- (T);
\draw (copy0) to [bend right] (mu1);
\draw (mu1) -- (E);
\end{tikzpicture}=
\begin{tikzpicture}
\path (0,0) node (A) {}
++ (0.5,0) coordinate (copy0)
++ (0.5,0.5) node[kernel] (mu0) {$\mu$}
++ (0.5,0) node[kernel] (smu) {$^*\mu$}
++ (0.5,0) coordinate (T);
\path (copy0)
++  (0.5,-0.5) node[kernel] (mu1) {$\mu$}
++  (1,0) coordinate (E);
\draw (A) -- (copy0);
\draw (copy0) to [bend left] (mu0);
\draw (mu0) -- (smu) -- (T);
\draw (copy0) to [bend right] (mu1);
\draw (mu1) -- (E);
\end{tikzpicture}\label{eq:starmu}
\end{align}

An equivalent condition to \ref{eq:starmu} is that for all $\theta,\theta'\in\mathscr{T}$, $A\in \mathcal{E}$, we have $\mu(\theta;A) = \mu(\theta';A)$, $\mu^*\mu(\theta;\cdot)$-almost surely. More informally,the support of $\mu^*\mu$ for each input $\theta$ divides $\mathscr{T}$ into equivalence classes such that for all $\theta$ in a given equivalence class, $\mu$ maps to the same probability measure on $\mathscr{E}$. 

Note that as a result of \ref{eq:starmu} we also have $\mu^*\mu\mu=\mu$. This weaker condition is not sufficient for the following result.

\todo[inline]{There is a connection between equation \ref{eq:starmu} and the notion of a sufficient statistic}

% A corollary of Lemma \ref{th:rightleft_inverse} is that left inverses have the following property:

% \begin{align}
% \begin{tikzpicture}
%  \path (0,0) node (A) {}
%  ++(0.5,0) node[kernel] (C) {$\;^\dagger A\;$}
%  ++(0.5,0) coordinate (D)
%  +(0.5,-0.5) node[kernel] (X) {$A$}
%  +(1.2,0.5) node (Y) {}
%  +(1.2,-0.5) node (Z) {};
%  \draw (A)--(C);
%  \draw (C) -- (D);
%  \draw (D) to [bend right] (X);
%  \draw (X) -- (Z);
%  \draw (D) to [bend left] (Y);
% \end{tikzpicture} = \begin{tikzpicture}
%  \path (0,0) node (A) {}
%  ++(0.5,0) coordinate (D)
%  +(0.5,0.5) node[kernel] (X) {$\;^\dagger A\;$}
%  +(1,-0.5) node (Y) {}
%  +(1,0.5) node (Z) {};
%  \draw (A)--(D);
%  \draw (D) to [bend left] (X);
%  \draw (X) -- (Z);
%  \draw (D) to [bend right] (Y);
% \end{tikzpicture}\label{eq:rightinverse_commute}
% \end{align}

% And \citet{fong_causal_2013} has shown that for deterministic $A$ we have

% \begin{align}
% \begin{tikzpicture}
%  \path (0,0) node (A) {}
%  ++(0.5,0) node[kernel] (C) {$A$}
%  ++(0.5,0) coordinate (D)
%  +(1.2,0.5) node (Y) {}
%  +(1.2,-0.5) node (Z) {};
%  \draw (A)--(C);
%  \draw (C) -- (D);
%  \draw (D) to [bend right] (Z);
%   \draw (D) to [bend left] (Y);
% \end{tikzpicture} = \begin{tikzpicture}
%  \path (0,0) node (A) {}
%  ++(0.5,0) coordinate (D)
%  +(0.5,0.5) node[kernel] (A1) {$A$}
%  +(0.5,-0.5) node[kernel] (A2) {$A$}
%  +(1,-0.5) node (Y) {}
%  +(1,0.5) node (Z) {};
%  \draw (A)--(D);
%  \draw (D) to [bend right] (A2);
%  \draw (A2) -- (Y);
%  \draw (D) to [bend left] (A1);
%  \draw (A1) -- (Z);
% \end{tikzpicture}\label{eq:det_commute}
% \end{align}

We then have

\begin{align}
\begin{tikzpicture}
 \path (0,0) node (A) {}
 ++(0.3,0) coordinate (copy0)
 ++(0.5,0) node[kernel] (mu) {$\mu$}
 ++(1,0) node[kernel] (kapdag) {$\kappa^*$}
 ++(0.4,0) coordinate (copy1)
 ++(0.5,0) node[kernel] (kap) {$\kappa$}
 +(1,0) node (E) {}
 +(1,-0.5) node (D) {}
 +(1,0.5) node (T) {};
 \draw (A)--(mu) -- (kapdag) -- (kap) -- (E);
 \draw (copy0) to [bend left] (kap) (copy0) to [bend left] (T);
 \draw (copy1) to [bend right] (D);
\end{tikzpicture}
 & \overset{\ref{eq:general_kapdag}}{=}
\begin{tikzpicture}
 \path (0,0) node (A) {}
 ++(0.3,0) coordinate (copy0)
 ++(0.5,0) node[kernel] (mu) {$\mu$}
 ++(1,0) node[kernel] (dagmumu) {$^*\mu\mu$}
 ++(0.4,0) coordinate (copy1)
 ++(0.5,-0.5) node[kernel] (kapsta) {$\kappa^*$}
 +(1,0.5) node (E) {}
 +(1,0) node (D) {}
 +(1,1) node (T) {};
 \draw (A)--(mu) -- (dagmumu) -- (copy1);
 \draw (copy0) to [bend left] (T);
 \draw (copy1) to [bend right] (kapsta) (copy1) to [bend left] (E);
 \draw (kapsta) -- (D);
\end{tikzpicture}\\
&=
\begin{tikzpicture}
 \path (0,0) node (A) {}
 ++(0.3,0) coordinate (copy0)
 ++(0.7,0) node[kernel] (mudagmu) {$\mu^*\mu$}
 ++(0.7,0) coordinate (copy1)
 ++(0.5,0) node[kernel] (mu) {$\mu$}
 ++(0.5,0) node[kernel] (kapsta0) {$\kappa^*$}
 ++(0.5,0) node[kernel] (kap) {$\kappa$}
 ++(0.4,0) coordinate (copy2)
 ++(0.5,-0.5) node[kernel] (kapsta1) {$\kappa^*$}
 +(1,0.5) node (E) {}
 +(1,0) node (D) {}
 +(1,1	) node (T) {};
 \draw (A)--(mudagmu) -- (mu) -- (kapsta0) -- (kap) -- (copy2);
 \draw (copy0) to [bend left] (T);
 \draw (copy1) to [bend left = 40] (kap);
 \draw (copy2) to [bend right] (kapsta1) (copy2) to [bend left] (E);
 \draw (kapsta1) -- (D);
\end{tikzpicture}\\
&\overset{\ref{eq:general_kapdag}}{=} 
\begin{tikzpicture}
 \path (0,0) node (A) {}
 ++(0.3,0) coordinate (copy0)
 ++(0.7,0) node[kernel] (mudagmu) {$\mu^*\mu$}
 ++(0.7,0) coordinate (copy1)
 ++(0.5,0) node[kernel] (mu) {$\mu$}
 ++(0.5,0) node[kernel] (kapsta0) {$\kappa^*$}
 ++(0.4,0) coordinate (copy2)
 ++(0.5,0) node[kernel] (kap) {$\kappa$}
 +(1,0) node (E) {}
 +(1,-0.5) node (D) {}
 +(1,0.5) node (T) {};
 \draw (A)--(mudagmu) -- (mu) -- (kapsta0) -- (kap) -- (copy2);
 \draw (copy0) to [bend left] (T);
 \draw (copy1) to [bend left =40] (kap);
 \draw (copy2) to [bend right] (D) (kap) -- (E);
\end{tikzpicture}\\
&\overset{\ref{eq:rightinverse_commute}}{=}
\begin{tikzpicture}
 \path (0,0) node (A) {}
 ++(0.3,0) coordinate (copy0)
 ++(0.7,0) node[kernel] (mudagmu) {$\mu$}
 ++(0.7,0) coordinate (copy1)
 +(0.5,0.5) node[kernel] (dagmu) {$^*\mu$}
 ++(0.5,0) node[kernel] (kapsta0) {$\kappa^*$}
 ++(0.4,0) coordinate (copy2)
 ++(0.5,0) node[kernel] (kap) {$\kappa$}
 +(1,0) node (E) {}
 +(1,-0.5) node (D) {}
 +(1,1) node (T) {};
 \draw (A)--(mudagmu) -- (kapsta0) -- (kap) -- (copy2);
 \draw (copy0) to [bend left] (T);
 \draw (copy1) to [bend left] (dagmu);
 \draw (dagmu) to [bend left] (kap);
 \draw (copy2) to [bend right] (D) (kap) -- (E);
\end{tikzpicture}\\
&\overset{\ref{eq:starmu}}{=}
\begin{tikzpicture}
 \path (0,0) node (A) {}
 ++(0.3,0) coordinate (copy0)
 +(0.9,0.5) node[kernel] (mu) {$\mu$}
 +(1.5,0.5) node[kernel] (dagmu) {$^*\mu$}
 ++(1.4,0) node[kernel] (kapsta0) {$\mu \kappa^*$}
 ++(0.4,0) coordinate (copy2)
 ++(0.5,0) node[kernel] (kap) {$\kappa$}
 +(1,0) node (E) {}
 +(1,-0.5) node (D) {}
 +(1,1) node (T) {};
 \draw (A)-- (kapsta0) -- (kap) -- (copy2);
 \draw (copy0) to [bend left=40] (T);
 \draw (copy0) to [bend left] (mu);
 \draw (mu) -- (dagmu) to [bend left] (kap);
 \draw (copy2) to [bend right] (D) (kap) -- (E);
\end{tikzpicture}\label{eq:skipmu}
\end{align}

Equation \ref{eq:skipmu} implies that, given any $\xi\in\Delta(\mathscr{T})$, all distributions of the form

\begin{align}
\begin{tikzpicture}
 \path (0,0) node[dist] (A) {$\xi$}
 ++(0.3,0) coordinate (copy0)
 ++(0.5,0) node[kernel] (mu) {$\mu$}
 ++(1,0) node[kernel] (kapdag) {$\kappa^*$}
 ++(0.4,0) coordinate (copy1)
 ++(0.5,0) node[kernel] (kap) {$\kappa$}
 +(1,0) node (E) {$\RV{E}$}
 +(1,-0.5) node (D) {$\RV{D}$}
 +(1,0.5) node (T) {$\RV{T}$};
 \draw (A)--(mu) -- (kapdag) -- (kap) -- (E);
 \draw (copy0) to [bend left] (kap) (copy0) to [bend left] (T);
 \draw (copy1) to [bend right] (D);\label{eq:reference_dist}
\end{tikzpicture}
\end{align}

admit both $\kappa:=$\begin{tikzpicture}
\path (0,0) node[kernel] (kap) {$\kappa^{\,}$};
\draw ($(kap.west) +(-0.5,0.15)$) -- ($(kap.west) +(0,0.15)$) ($(kap.west) +(-0.5,-0.15)$) -- ($(kap.west) +(0,-0.15)$);
\draw (kap) -- (0.5,0);
\end{tikzpicture} and $\kappa_{\mathrm{fac}}:=$\begin{tikzpicture}
\path (0,0) node[kernel] (kap) {$\kappa^{\,}$}
 + (-0.8,0.15) node[kernel] (mdm) {$\mu^*\mu$};
\draw ($(kap.west) +(-1.25,-0.15)$) -- ($(kap.west) +(0,-0.15)$);
\draw (kap) -- (0.5,0);
\draw ($(mdm.west)+(-0.2,0)$) -- (mdm) -- ($(kap.west) +(0,0.15)$);
\end{tikzpicture} as disintegrations from $(\RV{D},\RV{T})\dashrightarrow\RV{E}$. Therefore these $\kappa$ and $\kappa_{\mathrm{fac}}$ agree almost surely with respect to the distribution \ref{eq:reference_dist} for any prior $\xi$. 

However, also by assumption \ref{eq:starmu}, we have that for $\theta,\theta'\in \mathscr{T}$  either $\mu(\theta;A)=\mu(\theta';A)$ for all $A\in \mathcal{E}$, or for any $A\in\mathcal{E}$ $\mu(\theta;A)=0$ or $\mu(\theta';A)=0$. That is, any two states either have the same probability measure or probability measures with disjoint support. This is problematic, as the distribution \ref{eq:reference_dist} then has no support over much of the space $D\times E\times \mathscr{T}$. If $\mu$ were deterministic, for example, and hence associated with some function $f$, while \ref{eq:starmu} would be guaranteed via a left inverse, \ref{eq:reference_dist} would be supported on a subset of $D\times\{(\theta,f(\theta))|\theta\in\mathscr{T}\}$. In particular, we have no guarantee that the desired equality of $\kappa$ and $\kappa_{\mathrm{fac}}$ holds if we take any decision that doesn't reproduce the observed distribution. This isn't totally trivial: we may live in a world where most actions make things worse, in which case knowing how to keep things the same is valuable.

A stronger result can be found if we assume we have an infinite sequence of RVs $\RV{X}_i:E\to W$ and $\RV{D}_i:D\to V$ such that
\begin{itemize}
	\item $W^\mathbb{N}=E$, $V^\mathbb{N}=D$ (i.e. the sequence of all $\RV{X}_i$'s is identified with $E$ and the sequence of all $\RV{D}_i$'s is identified with $D$)
 	\item $\mu = \splitter{0.1} \otimes_{i\in \mathbb{N}} \mu F_{\RV{X}_i}$ (the $\RV{X}_i$'s are ``IID conditional on $\theta$'')\todo{this might be closely related to exchangeability via de Finetti?}
 	\item There exists $\kappa_0$ such that $\kappa = \splitter{0.1}\otimes_{i\in\mathbb{N}} (F_{\RV{D}_i}\otimes \mathrm{Id}_\mathscr{T}) \kappa_0 F_{\RV{X}_i}$ ($\kappa$ is ``IID conditional on $\RV{D},\theta$'')
 \end{itemize} 

Here we define the ``infinite copy map'' $\splitter{0.1}\otimes_{i\in\mathbb{N}}\mu F_{\RV{X}_i}$ to denote the kernel $\theta\mapsto \nu_\theta$ where $\nu_\theta$ the unique distribution such that for all finite $A\subset \mathbb{N}$ and projections $\pi_{A}:E\to \Delta(W^{|A|})$, $\nu_\theta \pi_{A}=\otimes_{i\in A}\mu_\theta F_{\RV{X}_i}$. This distribution is unique via the Kolmogorov extension theorem (the symmetry of the copy map guarantees the required consistency conditions) \citep{tao_introduction_2011}.

\todo[inline]{I assume, for now, that measurability can be worked out in some cases; in particular, that there is a $\sigma$-algebra on infinite sequences that renders the above kernel measurable in the appropriate way.}

\begin{lemma}[``IID'' kernels agree on truncations]\label{lem:agree_on_truncations}
For finite $A\subset\mathbb{N}$, $y,y'\in D$, if $\otimes_{i\in A} \RV{X}_i(y) = \otimes_{i\in A} \RV{X}_i(y')$ and $\kappa:\mathscr{T}\times D\to \Delta(\mathcal{E})$ is ``IID'' in the sense above then for all $\theta\in \mathscr{T}$, $B\in\mathcal{W}^{|A|}$, $\kappa(\theta,y;B) \pi_A = \kappa(\theta,y';B) \pi_A$.
\end{lemma}

\begin{proof}
By definition, we have 
\begin{align}
	\kappa \pi_A (\theta,y;B) &= \otimes_{i\in A} \kappa F_{\RV{X}_i} (\theta,\RV{D}_i(y);B)\\
							 &= \otimes_{i\in A} \kappa F_{\RV{X}_i} (\theta,\RV{D}_i(y');B)\\
							 &= \kappa \pi_A (\theta,y';B)
\end{align}
\end{proof}

Suppose both $\RV{X}_i$ and $\RV{D}_i$ are binary, and that for each $\theta\in \mathscr{T}$ we have recoverability (Eq. \ref{eq:recoverability}) with $\mu_\theta=\gamma_\theta$ (we will conclude that $\RV{X}$ is ``directly controlled'' by $\RV{D}$, but we will not assume this at the outset). $\kappa^*$ is therefore trivial. For each $\theta$, $\RV{X}_i$ are IID Bernoulli variables and so each $\mu_\theta$ is characterised by a single parameter $p$; let $p_\theta$ be the value of this parameter for some given $\theta$. Define $\overline{\RV{X}} := \lim_{n\to\infty} \frac{1}{m}\sum_{i\in[n]} \RV{X}_i$ and $^*\mu$ to be any kernel $E\to \Delta(\mathscr{T})$ such that the support of $^*\mu(x;\cdot)$ is a subset of $\{\theta|p_\theta=\overline{\RV{X}}(x)\}$. Note that for any $\theta,\theta'\in \mathscr{T}$ we have either $p_\theta=p_{\theta'}$ and so $\mu(\theta;A)=\mu(\theta';A)$ for all $A$ or $\theta'$ is not in the support of $\mu^*\mu(\theta;\cdot)$. Thus we have \ref{eq:starmu}, and hence ``almost sure'' equality of $\kappa$ and $\kappa_{\mathrm{fac}}$.

However with the exception of states where $p_\theta=0$ or $1$, almost sure equality is enough for $\kappa_{\mathrm{fac}}\pi_A(\theta,y;B)=\kappa\pi_A(\theta,y;B)$ for all $y\in D$, finite $A\subset\mathbb{N}$ and $B\in \mathcal{W}^{|A|}$. Then by the Kolmogorov extension theorem, we also have $\kappa_{\mathrm{fac}} (\theta,y;B) = \kappa(\theta,y;B)$ for all $y\in D$ and ``almost all'' $\theta\in \mathscr{T}$.

This appears to have similarities to the general case where we are trying to identify a particular function from some set of possible functions and we know the output of that function for a subset of inputs. It still comes down to a question of whether or not the set of functions in question is small enough to be fully characterised by the set of inputs we're allowed to see.

% From \ref{eq:gamkap_2_mu} and \ref{eq:kappa_cancel} we have

% \begin{align}
% \begin{tikzpicture}
% 	\path (0,0) coordinate (A)
% 	+  (0,0.15) coordinate (copy0)
% 	++ (0,0.5) node[kernel] (mu) {$\mu$}
% 	++ (0,0.5) node[kernel] (kapdag) {$\kappa^{\dagger}$}
% 	++ (0,0.5) coordinate (copy1)
% 	+  (0.6,0.5) node[kernel] (kap) {$\kappa$}
% 	+  (-0.5,0.5) coordinate (B) 
% 	+  (0.6	,1) node (E) {$\RV{E}$}
% 	+  (-0.5,1) node (D) {$\RV{D}$};
% 	\draw (A) -- (mu);
% 	\draw (mu) -- (kapdag);
% 	\draw (kapdag) -- (copy1);
% 	\draw (copy1) to [bend left] (B);
% 	\draw (copy1) to [bend right]  (kap);
% 	\draw (kap) to (E);
% 	\draw (B) to (D);
% 	\draw (copy0) to [bend right] (kap);
% \end{tikzpicture} &= 
% \begin{tikzpicture}
% 	\path (0,0) coordinate (A)
% 	++ (0,0.25) coordinate (copy0)
% 	++ (0,0.5) node[kernel] (gam) {$\gamma$}
% 	++ (0,0.5) coordinate (copy1)
% 	+ (0.5,0.5) node[kernel] (kap) {$\kappa$}
% 	+ (-0.5,0.5) coordinate (B)
% 	+ (0.5,1) node (E) {$\RV{E}$}
% 	+ (-0.5,1) node (D) {$\RV{D}$};
% 	\draw (A) -- (copy0);
% 	\draw (copy0) -- (gam);
% 	\draw (copy0) to [bend right] (kap);
% 	\draw (gam) -- (copy1);
% 	\draw (copy1) to [bend left] (B);
% 	\draw (copy1) to [bend right] (kap);
% 	\draw (kap) to (E);
% 	\draw (B) to (D);
% \end{tikzpicture}
% \end{align}

% Note that, given some prior $\xi\in \Delta(\mathscr{T})$, the ``Bayes result'' of invoking some decision rule $J:E\to \Delta(\mathcal{D})$ is given by

% \begin{align}
% \begin{tikzpicture}
%  \path (0,0) node[dist] (A) {$\xi$}
%  ++(0,0.3) coordinate (copy0)
%  ++(0,0.5) node[kernel] (mu) {$\mu$}
%  ++(0,0.5) node[kernel] (kapdag) {$J$}
%  ++(0,0.7) node[kernel] (kap) {$\kappa$}
%  ++(0,0.5) node (E) {$\RV{E}$};
%  \draw (A)--(mu) -- (kapdag) -- (kap) -- (E);
%  \draw (copy0) to [bend right = 45] (kap);
% \end{tikzpicture}
% \end{align}


% Given these two assumptions, we have
% \begin{align}
% 	\begin{tikzpicture}[scale=0.8]
% 		\path (0,0) node[dist] (A) {$\mu$}
% 		++(0,1) coordinate (C)
% 		+(1,2) node (F) {$*$}
% 		++(-1,1) node[kernel] (E) {$\kappa^*$}
% 		++(0,1) coordinate (G)
% 		+(1,0.5) node[kernel] (D) {$\kappa$}
% 		+(0,1) coordinate (H)
% 		+(1,1) coordinate (I);
% 		\draw (A) -- (C);
% 		\draw (C) to [bend right] (F.center);
% 		\draw (C) to [bend left] (E);
% 		\draw (E) -- (G);
% 		\draw (G) -- (H);
% 		\draw (G) to [bend right] (D);
% 		\draw (D) to (I);
% 	\end{tikzpicture}&=
% 		\begin{tikzpicture}[scale=0.8]
% 		\path (0,0) node[dist] (A) {$\mu$}
% 		++(0,1) node[kernel] (E) {$\kappa^*$}
% 		++(0,0.5) coordinate (G)
% 		+(1,0.5) node[kernel] (D) {$\kappa$}
% 		+(-1,1) coordinate (H)
% 		+(1,1) coordinate (I);
% 		\draw (A) -- (E);
% 		\draw (E) -- (G);
% 		\draw (G) to [bend left] (H);
% 		\draw (G) to [bend right] (D);
% 		\draw (D) to (I);
% 	\end{tikzpicture}\\
% 	&=
% 		\begin{tikzpicture}[scale=0.8]
% 		\path (0,0) node[dist] (A) {$\mu$}
% 		++(0,1) coordinate (G)
% 		+(1,1) node[kernel] (B) {$\kappa^*$}
% 		+(-1,1) node[kernel] (C) {$\kappa^*$}
% 		+(1,2) node[kernel] (D) {$\kappa$}
% 		+(-1,3) coordinate (H)
% 		+(1,3) coordinate (I);
% 		\draw (A) -- (G);
% 		\draw (G) to [bend left] (C);
% 		\draw (G) to [bend right] (B);
% 		\draw (C) to (H);
% 		\draw (B) to (D);
% 		\draw (D) to (I);
% 	\end{tikzpicture}\\
% 	&=
% 	\begin{tikzpicture}[scale=0.8]
% 		\path (0,0) node[dist] (A) {$\mu$}
% 		++(0,1) coordinate (B) 
% 		+(1,2) coordinate (C)
% 		+(-1,1) node[kernel] (D) {$\kappa^*$}
% 		+(-1,2) coordinate (E);
% 		\draw (A) -- (B);
% 		\draw (B) to [bend right] (C);
% 		\draw (B) to [bend left] (D);
% 		\draw (D) -- (E);
% 	\end{tikzpicture}\\
% 	&:= \mu^{\dagger|}
% \end{align}

% Here we recall the definition of a conditional probability: a conditional probability \inkernel{\RV{Y}|\RV{X}}{}{} with respect to $\nu\in \Delta(\mathcal{E})$ and RVs $\RV{X}:E\to X$, $\RV{Y}:E\to Y$ is any Markov kernel $X\to \Delta(\mathcal{Y})$ such that
% \begin{align}
% 	\begin{tikzpicture}[scale=0.7]
% 	\path (0,0) node[dist,inner sep=2pt] (A) {$\nu$}
% 	+ (0.5,1) node (B) {$X$}
% 	+ (-0.5,1) node (C) {$Y$};
% 	\draw (B) -- (B|- A.north);
% 	\draw (C) -- (C|- A.north);
% 	\end{tikzpicture} = 
% 	\begin{tikzpicture}[scale=0.7]
% 	\path (0,0) node[dist,inner sep=2pt] (A) {$\nu$}
% 	+ (-0.5,1) node (C) {$*$}
% 	++ (0.5,1) coordinate (B)
% 	+ (-1,1) node[kernel] (D) {$\RV{Y}|\RV{X}$}
% 	+(0,2) node (E) {$X$}
% 	+(-1,2) node (F) {$Y$};
% 	\draw (B) -- (B|- A.north);
% 	\draw (C.center) -- (C|- A.north);
% 	\draw (B) -- (E);
% 	\draw (B) to [bend left] (D);
% 	\draw (D) -- (F);
% 	\end{tikzpicture}
% \end{align}

% Thus we can see that $\kappa$ is a conditional probability \inkernel{\RV{E}|\RV{D}}{}{} with respect to $\mu^{\dagger|}$ the RVs $\RV{D}:E\times D\to D$ and $\RV{E}:E\times D\to E$ given by the respective projections.

% This result is in line with two familiar cases from the causal literature: an \emph{ignorable} treatment in potential outcomes or a graphical model with no back-door paths between $\RV{D}$ and $\RV{E}$ both imply that the theory's respective version of ``causal effect'' can be determined directly from the conditional probability. This suggests that reproducibility is in fact quite a strong condition, as it is the exception rather than the norm where causal effects can be determined directly from conditional probabilities. There are two important caveats here, however: firstly we've assumed $\kappa$ has a known right-inverse $\kappa^*$, and secondly this result only allows $\kappa$ to be determined $\mu^{\dagger|}$-almost surely.

% \todo[inline]{A notion of ``approximate right-invertibility'' would be very helpful here. Something like $\kappa$ is approximately right invertible if there is some $\tilde{\kappa}^\dagger$ such that $\gamma \kappa\tilde{\kappa}^\dagger$ is always ``close'' to $\gamma$. Intuitively, a consequence that is \emph{not} approximately right invertible has a channel capacity of 0, and \textbf{Conjecture:} a set of kernels that do not collectively admit some approximate right-inverse do not admit preferences between decisions under the minimax rule}

% \section{More Complex Cases}

% Suppose we retain the right invertibility of $\kappa$ via some known $\kappa^*$, which is consistent with (for example) hard interventions and counterfactual variable forcing. A more common way that there fails to be a memoryless function $J_\theta$ such that $J_\theta\kappa=\mu$ arises from the fact that we may often regard the consequence as dynamic. Assign an index $\theta\in \Theta$ to each causal state, which we now denote $(\kappa_\theta,\mu_\theta)\in \mathscr{T}$ and let $\kappa^\Theta:\Theta\times D\to \Delta(\mathcal{E})$ be defined by $\kappa^\Theta:(\theta,y;A)\mapsto \kappa_\theta(y;A)$ which we will assume is also a Markov kernel (noting that this intoduces additional restrictions on $\mathscr{T}$ in the general case). We may assume, then, for some state-dependent generating distribution $\xi_\theta\in \Delta(\Theta)$ and some state dependent ``passive decision map'' $K_\theta:\Theta\to \Delta(\mathcal{D})$, we have $\mu_\theta = \xi_\theta\splitter{0.1}(I_{(\Theta)}\otimes K_\theta)\kappa^\Theta$. 

% \indist{a}{b}

% \inkernel{ab}{a}{b}