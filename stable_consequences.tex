%!TEX root = main.tex

\section{The story at a high level}

Take a causal theory $\mathscr{T}$ where we label each pair $\theta:=(\kappa_\theta,\mu_\theta)\in\mathscr{T}$. Define the kernels $\kappa:\mathscr{T}\times D\to \mathcal{E}$ and $\mu:\mathscr{T}\to\mathcal{E}$ by $\kappa:(\theta,y;A)\mapsto \kappa_\theta(y;A)$ and $\mu:(\theta;A)\mapsto \mu_\theta(A)$.

\paragraph{Optmizibility:} I make the claim (unproven) that it is possible to find a ``universally optimal'' decision function if the following identity holds for all decision functions $J:E\to \Delta(\mathcal{D})$:

\begin{align}
\begin{tikzpicture}
\path (0,0) coordinate (A)
	  ++(0.5,0) coordinate (copy0)
	  ++(0.5,0) node[kernel] (mu) {$\mu$}
	  ++(0.5,0) node[kernel] (J) {$J$}
	  ++(0.5,0) node[kernel] (kap) {$\kappa$}
	  ++(0.5,0) node[expectation] (u) {$u$};
\draw (A) -- (mu) -- (J) -- (kap) -- (u);
\draw (copy0) to [bend right = 40] (kap);
\end{tikzpicture}
=
\begin{tikzpicture}
\path (0,0) coordinate (A)
	  ++(0.5,0) node[kernel] (mu) {$\mu$}
  	  ++(0.5,0) coordinate (copy0)
  	  +(0.5,-0.5) node[kernel] (al) {$\alpha$}
	  ++(0.5,0) node[kernel] (J) {$J$}
	  ++(0.5,0) node[kernel] (kap) {$\kappa$}
	  ++(0.5,0) node[expectation] (u) {$u$};
\draw (A) -- (mu) --(J) -- (kap) -- (u);
\draw (copy0) to [bend right = 40] (al) (al) to [bend right = 40] (kap);
\end{tikzpicture} \label{eq:lift_the_string}
\end{align}

I have a proof that this is so in my notebook, but I still have to write it up and check it.

If the forward direction holds, the reverse direction does not hold - we can take a problem that respects \ref{eq:lift_the_string} and introduce additional dominated decisions that break \ref{eq:lift_the_string} without breaking the ``universal optimizability'' (i.e. decisions we know to be very bad, but exactly how bad depends on the state in a difficult-to-identify manner). Again in my notebook, I have an argument that if we only require \ref{eq:lift_the_string} for any complete class of decision functions, then this requirement is necessary. 

\paragraph{Necessary and Sufficient conditions for optimizibility:} 

Setting aside the utility for now, if equation \ref{eq:lift_the_string} holds for all decision functions dominated by $I$ (that is, all $J$ such that for all $x\in E$, $J(x;\cdot) \ll I(x;\cdot)$), this is equivalent to the following:

\begin{align}
\begin{tikzpicture}
\path (0,0) coordinate (A)
	  ++(0.5,0) coordinate (copy0)
	  ++(0.5,0) node[kernel] (mu) {$\mu$}
	  ++(0.5,0) node[kernel] (J) {$I$}
	  ++(0.5,0) coordinate(copy1)
	  ++(0.5,0) node[kernel] (kap) {$\kappa$}
	  ++(0.5,0) node (u) {$E$}
	  ++(0,0.5) node (D) {$D$};
\draw (A) -- (mu) -- (J) -- (kap) -- (u);
\draw (copy0) to [bend right = 40] (kap);
\draw (copy1) to [bend left] (D); 
\end{tikzpicture}
=
\begin{tikzpicture}
\path (0,0) coordinate (A)
	  ++(0.5,0) node[kernel] (mu) {$\mu$}
  	  ++(0.5,0) coordinate (copy0)
  	  +(0.5,-0.5) node[kernel] (al) {$\alpha$}
	  ++(0.5,0) node[kernel] (J) {$I$}
  	  ++(0.5,0) coordinate (copy1)
	  ++(0.5,0) node[kernel] (kap) {$\kappa$}
	  ++(0.5,0) node (u) {$E$}
	  ++(0,0.5) node (D) {$D$};
\draw (A) -- (mu) --(J) -- (kap) -- (u);
\draw (copy0) to [bend right = 40] (al) (al) to [bend right = 40] (kap);
\draw (copy1) to [bend left] (D);
\end{tikzpicture}\label{eq:lts_joint}
\end{align}

The extra copy map allows us to extend from equality ``for $I$'' to equality ``for all kernels dominated by $I$''.

\todo[inline]{We ignore the utility as introducing it moves us out of the world of Markov kernels; need to show that everything still works if we reintroduce it}

Immediate from Eq. \ref{eq:lts_joint} is the fact that there is some kernel $L$ such that:

\begin{align}
\begin{tikzpicture}
	 \path (0,0) coordinate (A)
	  ++(0.5,0) coordinate (copy0)
	  ++(0.5,0) node[kernel] (mu) {$\mu$}
	  ++(0.5,0) node[kernel] (J) {$I$}
	  ++(0.5,0) coordinate(copy1)
	  ++(0.5,0) node[kernel] (kap) {$\kappa$}
	  ++(0.5,0) node (u) {}
	  ++(0,0.5) node (D) {};
\draw (A) -- (mu) -- (J) -- (kap) -- (u);
\draw (copy0) to [bend right = 40] (kap);
\draw (copy1) to [bend left] (D); 
\end{tikzpicture} &=
\begin{tikzpicture}
	\path (0,0) coordinate (A)
	++(0.5,0) node[kernel] (mu) {$\mu$}
	++(0.5,0) node[kernel] (L) {$L$}
	++(1,0.5) node (D) {}
	++(0,-0.5) node (u) {};
	\draw (A) -- (mu) -- (L) -- (u);
	\draw ($(L.east) +(0,0.15)$) to [bend left] (D);
\end{tikzpicture}\label{eq:joint_from_mu}
\end{align}

In addition, if we define $C:\mathscr{T}\times D\to \Delta(\mathcal{E})$ as a \emph{generalised disintegration} of $\mu L$ - i.e. a kernel with the following property:

\begin{align}
\begin{tikzpicture}
\path (0,0) coordinate (A)
	  ++(0.5,0) coordinate (copy0)
	  ++(0.5,0) node[kernel] (mu) {$\mu$}
	  ++(0.5,0) node[kernel] (J) {$I$}
	  ++(0.5,0) coordinate(copy1)
	  ++(0.5,0) node[kernel] (kap) {$\kappa$}
	  ++(0.5,0) node (u) {$E$}
	  ++(0,0.5) node (D) {$D$};
\draw (A) -- (mu) -- (J) -- (kap) -- (u);
\draw (copy0) to [bend right = 40] (kap);
\draw (copy1) to [bend left] (D); 
\end{tikzpicture} &=
\begin{tikzpicture}
\path (0,0) coordinate (A)
	  ++(0.5,0) coordinate (copy0)
	  ++(0.5,0) node[kernel] (mu) {$\mu$}
	  ++(0.5,0) node[kernel] (J) {$I$}
	  ++(0.5,0) coordinate(copy1)
	  ++(0.5,0) node[kernel] (kap) {$\kappa$}
	  ++(0.5,0) node (u) {}
	  +(0,0.5) coordinate (copy2)
	  ++(0.5,0) node[kernel] (C) {$C$}
	  ++(0.7,0) node (E) {$E$}
	  +(0,0.5) node (D) {$D$};
\draw (A) -- (mu) -- (J) -- (kap);
\draw[-{Rays [n=8]}] (kap) -- (u);
\draw (copy0) to [bend right = 35] (kap);
\draw (copy2) to [bend left] (C.west);
\draw (C) -- (E);
\draw (copy1) to [bend left] (copy2) to [bend left] (D); 
\draw (copy0) to [bend right = 40] (C);
\end{tikzpicture}
\end{align}

Then we can see by the fact that $C=\kappa$ ``A.S.'' that 

\begin{align}
\begin{tikzpicture}
\path (0,0) coordinate (A)
	  ++(0.5,0) coordinate (copy0)
	  ++(0.5,0) node[kernel] (mu) {$\mu$}
	  ++(0.5,0) node[kernel] (J) {$I$}
	  ++(0.5,0) coordinate(copy1)
	  ++(0.5,0) node[kernel] (kap) {$C$}
	  ++(0.5,0) node (u) {$E$}
	  ++(0,0.5) node (D) {$D$};
\draw (A) -- (mu) -- (J) -- (kap) -- (u);
\draw (copy0) to [bend right = 40] (kap);
\draw (copy1) to [bend left] (D); 
\end{tikzpicture}
=
\begin{tikzpicture}
\path (0,0) coordinate (A)
	  ++(0.5,0) node[kernel] (mu) {$\mu$}
  	  ++(0.5,0) coordinate (copy0)
  	  +(0.5,-0.5) node[kernel] (al) {$\alpha$}
	  ++(0.5,0) node[kernel] (J) {$I$}
  	  ++(0.5,0) coordinate (copy1)
	  ++(0.5,0) node[kernel] (kap) {$C$}
	  ++(0.5,0) node (u) {$E$}
	  ++(0,0.5) node (D) {$D$};
\draw (A) -- (mu) --(J) -- (kap) -- (u);
\draw (copy0) to [bend right = 40] (al) (al) to [bend right = 40] (kap);
\draw (copy1) to [bend left] (D);
\end{tikzpicture}\label{eq:lts_disint_joint}
\end{align}

The reverse direction also holds: \ref{eq:joint_from_mu} and \ref{eq:lts_disint_joint} imply \ref{eq:lts_joint}. This decomposition is handy, because we can consider \ref{eq:joint_from_mu} to express ``perfect identifiability of $\kappa$'' and \ref{eq:lts_disint_joint} to express ``perfect estimability of $\mu$'', the former being a quintessentially ``causal'' notion and the latter being a ``statistical'' notion (note that this distinction is only based on \emph{typical use} of the words causal and statistical).

The following assumptions are sufficient (but not necessary) for \ref{eq:lts_disint_joint}:
\begin{align}
\exists M:\begin{tikzpicture}
\path (0,0) node (A) {}
++ (0.5,0) node[kernel] (mu) {$\mu$}
++ (0.5,0) coordinate (copy0)
++ (0.5,0.5) node[kernel] (smu) {$M$}
++ (0.5,0) coordinate (T);
\path (copy0)
++  (0.5,-0.5) coordinate (mu1)
++  (0.5,0) coordinate (E);
\draw (A) -- (mu) -- (copy0);
\draw (copy0) to [bend left] (smu);
\draw (smu) -- (T);
\draw (copy0) to [bend right] (mu1);
\draw (mu1) -- (E);
\end{tikzpicture} &=
\begin{tikzpicture}
\path (0,0) node (A) {}
++ (0.5,0) coordinate (copy0)
++ (0.5,0.5) node[kernel] (mu0) {$\mu$}
++ (0.5,0) node[kernel] (smu) {$M$}
++ (0.5,0) coordinate (T);
\path (copy0)
++  (0.5,-0.5) node[kernel] (mu1) {$\mu$}
++  (1,0) coordinate (E);
\draw (A) -- (copy0);
\draw (copy0) to [bend left] (mu0);
\draw (mu0) -- (smu) -- (T);
\draw (copy0) to [bend right] (mu1);
\draw (mu1) -- (E);
\end{tikzpicture}\label{eq:muM_det}\\
\begin{tikzpicture}
\path (0,0) node (A) {}
++ (0.5,0) node[kernel] (mu) {$\mu$}
++ (0.5,0) coordinate (copy0)
++ (0.5,0.5) node[kernel] (smu) {$M$}
++ (0.5,0) coordinate (T);
\path (copy0)
++  (0.5,-0.5) coordinate (mu1)
++  (0.5,0) coordinate (E);
\draw (A) -- (mu) -- (copy0);
\draw (copy0) to [bend left] (smu);
\draw (smu) -- (T);
\draw (copy0) to [bend right] (mu1);
\draw (mu1) -- (E);
\end{tikzpicture}&= \begin{tikzpicture}
\path (0,0) node (A) {}
++ (0.5,0) node[kernel] (mu0) {$\mu$}
++ (0.5,0) node[kernel] (smu) {$M$}
++ (0.5,0) coordinate (copy0)
+  (0.5,0.5) coordinate (B)
+  (1,0.5) coordinate (T)
+  (0.5,-0.5) node[kernel] (mu1) {$\mu$}
+  (1,-0.5) coordinate (E);
\draw (A) -- (mu0) -- (smu) -- (copy0);
\draw (copy0) to [bend left] (B);
\draw (B) -- (T);
\draw (copy0) to [bend right] (mu1);
\draw (mu1) -- (E);
\end{tikzpicture}\label{eq:know_mu}
\end{align}

Sufficient for \ref{eq:muM_det} is the condition that there is some function $\RV{T}$ with associated kernel $F_{\RV{T}}$ such that $\mu F_{\RV{T}}$ is deterministic and for \ref{eq:know_mu} we require that $\RV{T}$ is sufficient for $\{\mu_\theta\}$. An example of this is where $\RV{T}$ is the mean of an infinite sequence of IID Bernoulli variables and $\mu F_{\RV{T}}$ is then deterministic via the strong law of large numbers.

Equation \ref{eq:know_mu} is not necessary for \ref{eq:lift_the_string}, as observations may be ``too informative''. For example, if $\mathscr{T}$ contains many different $\mu_\theta$ but only one $\kappa_\theta$, then we can always perform \ref{eq:lift_the_string} while we do not generally have \ref{eq:know_mu}.

\subsection{IID variables and factorisibility}

Equation \ref{eq:joint_from_mu} is quite general in the sense that, if $\mu$ maps from $\mathscr{T}$ to an infinite sequence of IID variables, we may satisfy it with an $L$ that first estimates $\mu_\theta$ from the sequence of variables and subsequently chooses an appropriate measure on $D\times E$. A particularly interesting case is where $\mu$ maps to an infinite sequence of IID variables and $L$ factorises as follows:

\begin{align}
\begin{tikzpicture}
\path (0,0) coordinate (A)
++ (0.5,0) node[kernel,inner sep=5pt] (L) {$L$}
++ (0.7,0.2) node (D) {}
++ (0,-0.2) node (E) {}
++ (0,-0.2) node (F) {$...$};
\draw ($(A.east) +(0,0)$) -- ($(L.west) +(0,0)$);
\draw ($(A.east) + (0,0.2)$) -- ($(L.west) +(0,0.2)$);
\draw ($(A.east) + (0,-0.2)$) -- ($(L.west) +(0,-0.2)$);
\draw ($(L.east) +(0,0)$) -- (E);
\draw ($(L.east) + (0,0.2)$) -- (D);
\draw ($(L.east) + (0,-0.2)$) -- (F);
\end{tikzpicture}&=
\begin{tikzpicture}
\path (0,0) node[kernel] (LE) {$L_0$}
+ (0,0.5) node[kernel] (LD) {$L_0$}
++ (0.7,0) node (E) {}
+ (0,0.5) node (D) {}
+ (-0.7,-0.3) node (F) {$...$};
\draw ($(LE.west) +(-0.5,0)$) -- (LE);
\draw ($(LD.west) + (-0.5,0)$) -- (LD);
\draw (LE) -- (E);
\draw (LD) -- (D);
\end{tikzpicture}
\end{align}

A special case of this is considered below. We can informally think of this case as ``correlation \emph{is} causation'', as from \emph{each} observed RV we can get an input-output pair via $L_0$. This is a generalisation of the usual case of ``correlation is causation'' as we allow the possibility that the output is randomised from observations rather than insisting it be distributed exactly as observations were. The possibility of post randomisation is helpful, for example, if the sequence of ``result'' variables we expect is of a different length to our sequence of observations.

\subsection{Informal overview of sufficient conditions}

First, we assume that the state-observations map $\mu$ sends a state to an infinite IID sequence generated by the one-shot state-consequence map $\kappa_0$ and some state-decision map $\gamma$:
\begin{align}
	\begin{tikzpicture}
	\path (0,0) node (A) {$\theta$}
	++(0.5,0) node[kernel] (mu) {$\mu$}
	++(0.5,-0.25) node (ell) {...};
	\draw (A) -- (mu);
	\draw ($(mu.east) +(0,0.15)$) -- ($(mu.east) +(0.25,0.15)$);
	\draw ($(mu.east) +(0,0)$) -- ($(mu.east) +(0.25,0)$);
	\draw ($(mu.east) +(0,-0.15)$) -- (ell);
	\end{tikzpicture}
	=
	\begin{tikzpicture}
	\path (0,0) node (A) {$\theta$}
	++(0.35,0) coordinate (copy0)
	++(0.25,0.5) coordinate (copy1)
	++(0.5,0) node[kernel] (g0) {$\gamma$}
	++(0.5,0) node[kernel] (k0) {$\kappa_0$}
	++(0.5,0) coordinate (k0end)
	++(0,-0.5) coordinate (k1end)
	++(-0.5,0) node[kernel] (k1) {$\kappa_0$}
	++(-0.5,0) node[kernel] (g1) {$\gamma$}
	++(-0.5,0) coordinate (copy2)
	++(0.5,-0.5) node (ell) {...};
	\draw (A) -- (copy0) -- (copy2) -- (g1) -- (k1) -- (k1end);
	\draw (copy0) to [bend left] (copy1) -- (g0) -- (k0) -- (k0end);
	\draw (copy0) to [bend right] (ell);
	\draw (copy1) to [bend left = 40] (k0);
	\draw (copy2) to [bend left = 40] (k1);
	\end{tikzpicture}\label{eq:iidmu}
\end{align}

\todo[inline]{Infinite copy maps (indicated by ellipsis) are defined for distributions via the Kolmogorov extension theorem. I don't know if they always exist for kernels.}

\todo[inline]{I think this assumption has a connection with the De Finetti representation theorem}

Second we assume $\kappa_0$ is \emph{globally invertible}. That is, there is a kernel $\kappa_0^*:E\to \Delta(\mathcal{D})$ such that

\begin{align}
\begin{tikzpicture}
	\path (0,0) coordinate (A)
	++ (0.3,0) coordinate (copy0)
	++ (0.5,0) node[kernel] (gam) {$\gamma$}
	++ (0.5,0) coordinate (copy1)
	+ (0.5,-0.5) node[kernel] (kap) {$\kappa_0$}
	+ (0.5,0.5) coordinate (B)
	+ (1,-0.5) node (E) {$\RV{E}$}
	+ (1,0.5) node (D) {$\RV{D}$};
	\draw (A) -- (copy0);
	\draw (copy0) -- (gam);
	\draw (copy0) to [bend right] (kap);
	\draw (gam) -- (copy1);
	\draw (copy1) to [bend left] (B);
	\draw (copy1) to [bend right] (kap);
	\draw (kap) to (E);
	\draw (B) to (D);
\end{tikzpicture}
&=
\begin{tikzpicture}
	\path (0,0) coordinate (A)
	++ (0.3,0) coordinate (copy0)
	++ (0.5,0) node[kernel] (gam) {$\gamma$}
	++ (0.5,0) node[kernel] (kap) {$\kappa_0$}
	++ (0.5,0) coordinate (copy1)
	+ (0.5,0.5) node[kernel] (kapdag) {$\kappa_0^*$}
	+ (0.5,-0.5) coordinate (B)
	+ (1,-0.5) node (E) {$\RV{E}$}
	+ (1,0.5) node (D) {$\RV{D}$};
	\draw (A) -- (copy0);
	\draw (copy0) -- (gam);
	\draw (copy0) to [bend right=40] (kap);
	\draw (gam) -- (kap);
	\draw (kap) -- (copy1);
	\draw (copy1) to [bend right] (B);
	\draw (copy1) to [bend left] (kapdag);
	\draw (kapdag) to (D);
	\draw (B) to (E);
\end{tikzpicture}
\end{align}

In the general, non-globally-invertible case we'd need a string from the ``state'' wire to $\kappa_0^*$.

Under these two conditions plus full support plus sufficient regularity for the strong law of large numbers, we have \ref{eq:kappa_factorises}, \ref{eq:know_mu} and \ref{eq:starmusufficient}.

I think a CBN is a causal theory where the consequence map is a decision randomised version of $\kappa_0$ in Eq. \ref{eq:iidmu} (i.e. the true consequence $\kappa=M\kappa_0$) and these other conditions hold, and is therefore dominated by a theory of the form above. Global invertibility is related to variable setting/hard interventions, and I think it's also related to the ``No Causes in No Causes out'' theorems \ref{th:no_in_out_min} and \ref{th:no_in_out_bayes}.

% I'm not sure how interesting the assumptions themselves are. One interesting point about the big picture story is that from one point of view the assumptions boil down to:
% \begin{itemize}
% 	\item We can characterise the input-output behaviour of $\kappa$ for any given state and a small subset of available decisions
% 	\item $\kappa$ is sufficiently regular that its behaviour on said subset of decisions characterises its complete behaviour
% \end{itemize}




\section{Recoverability}

A natural assumption suggested by the notion of a CSDP is that of \emph{recoverability} - that a causal theory $\mathscr{T}:E\times D\rightarrowtriangle E$ permits some decision function that reproduces the distribution of the observed data. That is, we assume that for every $(\kappa_\theta,\mu_\theta):=\theta\in \mathscr{T}$ there exists $\gamma_{\theta}\in \Delta(\mathcal{D})$ such that
\begin{align}
	\gamma_{\theta}\kappa_\theta = \mu_\theta \label{eq:recoverability}
\end{align}

``Traditional'' causal inference doesn't have a strict equivalent of this assumption, though it corresponds roughly to the ``easy'' cases (for example, it is satisfied by a CBN where there are no backdoor paths between the ``intervened'' variable and the ``target'' variable). One reason I think it's interesting is that \emph{randomised recoverability} may be quite a general assumption - that is, there is ``in principle'' a stochastic decision that recovers the observed distribution, but we are practically limited to taking mixed decisions that cannot necessarily accomplish this.

Suppose also that we have some $\kappa^*$ that, for all $\theta \in \mathscr{T}$, is a Bayesian inversion of $\gamma_\theta$ and $\kappa_\theta$; that is:

\begin{align}
\begin{tikzpicture}
	\path (0,0) node[dist] (gam) {$\gamma_\theta$}
	++(0.5,0) coordinate (copy0)
	+(0.5,0.5) node[kernel] (kap) {$\kappa_\theta$}
	+(0.5,-0.5) coordinate (ph)
	+(1,0.5) node (E) {$\RV{E}$}
	+(1,-0.5) node (D) {$\RV{D}$};
	\draw (gam) -- (copy0);
	\draw (copy0) to [bend left] (kap);
	\draw (copy0) to [bend right] (ph);
	\draw (kap) -- (E);
	\draw (ph) -- (D); 
\end{tikzpicture}= \begin{tikzpicture}
	\path (0,0) node[dist] (gam) {$\gamma_\theta$}
	++(0.6,0) node[kernel] (kap) {$\kappa_\theta$}
	++(0.5,0) coordinate (copy0)
	+(0.5,-0.5) node[kernel] (kapdag) {$\kappa^*$}
	+(0.5,0.5) coordinate (ph)
	+(1,0.5) node (E) {$\RV{E}$}
	+(1,-0.5) node (D) {$\RV{D}$};
	\draw (gam) -- (kap);
	\draw (kap) -- (copy0);
	\draw (copy0) to [bend right] (kapdag);
	\draw (copy0) to [bend left] (ph);
	\draw (kapdag) -- (D);
	\draw (ph) -- (E); 
\end{tikzpicture} \label{eq:kappa_BI}
\end{align}

A sufficient condition for the existence of such a $\kappa^*$ is the assumption that decisions correspond to \emph{variable setting} - that is, there is some variable $\RV{X}:E\to X$ such that for all $a\in D$, $\theta\in\mathscr{T}$ we have $\delta_a \kappa_\theta F_{\RV{X}} = \delta_a$ (such an assumption arises in graphical models as hard interventions, and in potential outcomes as ``potential-outcome identifiers''). Indeed $F_{\RV{X}}$ is in this case a candidate for $\kappa^*$. It is not necessary that $\kappa^*$ be deterministic, however - suppose every $\kappa$ ignores $D$. Then choose $\gamma_\theta=\gamma$ for arbitrary $\gamma\in \Delta(\mathcal{D})$ and it can be verified that $\kappa^*:b\mapsto \gamma$ satisfies \ref{eq:kappa_BI}. 

I believe a weaker sufficient condition for the existence of a universal $\kappa^*$ is that every $\kappa_\theta$ factorises as $\kappa_\theta = h \splitter{0.1}(\mathrm{Id}_F\otimes j_\theta)$ for some fixed $h:D\to \Delta(\mathcal{F})$, but I have not yet shown this.

We will proceed somewhat rashly: suppose that by defining $\gamma:\mathscr{T}\to \Delta(\mathcal{D})$, $\mu:\mathscr{T}\to \Delta(\mathcal{E})$ and $\kappa:\mathscr{T}\times D\to \Delta(\mathcal{E}$ by $\gamma:\theta\to \gamma_\theta$, $\mu:\theta\to \mu_\theta$ and $\kappa:(\theta,d)\to \kappa_\theta(d;\cdot)$ that all resulting objects are Markov kernels, and that $\mathscr{T}$ is a standard measurable space.

By previous assumptions, we have the following properties:
\begin{align}
\begin{tikzpicture}
	\path (0,0) coordinate (A)
	++ (0.5,0) node[kernel] (B) {$\mu$}
	++ (0.5,0) coordinate (C);
	\draw (A) -- (B);
	\draw (B) -- (C);
\end{tikzpicture}
&=
\begin{tikzpicture}
	\path (0,0) coordinate (A)
	++ (0.3,0) coordinate (copy0)
	++(0.5,0) node[kernel] (gam) {$\gamma$}
	+ (0,0.3) coordinate (B)
	++(0.5,0) node[kernel] (kap) {$\kappa$}
	++(0.5,0) coordinate (C);
	\draw (A) -- (copy0);
	\draw (copy0) to [bend left] (B);
	\draw (copy0) -- (gam);
	\draw (B) to [bend left] (kap.west);
	\draw (gam) -- (kap);
	\draw (kap) -- (C);
\end{tikzpicture}\label{eq:recoverabilityd}\\
\begin{tikzpicture}
	\path (0,0) coordinate (A)
	++ (0.3,0) coordinate (copy0)
	++ (0.5,0) node[kernel] (gam) {$\gamma$}
	++ (0.5,0) coordinate (copy1)
	+ (0.5,-0.5) node[kernel] (kap) {$\kappa$}
	+ (0.5,0.5) coordinate (B)
	+ (1,-0.5) node (E) {$\RV{E}$}
	+ (1,0.5) node (D) {$\RV{D}$};
	\draw (A) -- (copy0);
	\draw (copy0) -- (gam);
	\draw (copy0) to [bend right] (kap);
	\draw (gam) -- (copy1);
	\draw (copy1) to [bend left] (B);
	\draw (copy1) to [bend right] (kap);
	\draw (kap) to (E);
	\draw (B) to (D);
\end{tikzpicture}
&=
\begin{tikzpicture}
	\path (0,0) coordinate (A)
	++ (0.3,0) coordinate (copy0)
	++ (0.5,0) node[kernel] (gam) {$\gamma$}
	++ (0.5,0) node[kernel] (kap) {$\kappa$}
	++ (0.5,0) coordinate (copy1)
	+ (0.5,0.5) node[kernel] (kapdag) {$\kappa^*$}
	+ (0.5,-0.5) coordinate (B)
	+ (1,-0.5) node (E) {$\RV{E}$}
	+ (1,0.5) node (D) {$\RV{D}$};
	\draw (A) -- (copy0);
	\draw (copy0) -- (gam);
	\draw (copy0) to [bend right=40] (kap);
	\draw (gam) -- (kap);
	\draw (kap) -- (copy1);
	\draw (copy1) to [bend right] (B);
	\draw (copy1) to [bend left] (kapdag);
	\draw (kapdag) to (D);
	\draw (B) to (E);
\end{tikzpicture} \label{eq:general_kapdag}\\
&= 
\begin{tikzpicture}
	\path (0,0) coordinate (A)
	++ (0.3,0) coordinate (copy0)
	++ (0.5,0) node[kernel] (mu) {$\mu$}
	++ (0.5,0) coordinate (copy1)
	+ (0.5,0.5) node[kernel] (kapdag) {$\kappa^*$}
	+ (0.5,-0.5) coordinate (B)
	+ (1,-0.5) node (E) {$\RV{E}$}
	+ (1,0.5) node (D) {$\RV{D}$};
	\draw (A) -- (copy0);
	\draw (copy0) -- (mu);
	\draw (mu) -- (copy1);
	\draw (copy1) to [bend right] (B);
	\draw (copy1) to [bend left] (kapdag);
	\draw (kapdag) to (D);
	\draw (B) to (E);
\end{tikzpicture}\label{eq:gamkap_2_mu}
\end{align}
From \ref{eq:general_kapdag} we also have

\begin{align}
\begin{tikzpicture}
	\path (0,0) coordinate (A)
	++ (0.3,0) coordinate (copy0)
	++ (0.5,0) node[kernel] (gam) {$\gamma$}
	++ (0.5,0) coordinate (copy1)
	+ (0.5,-0.5) node[kernel] (kap) {$\kappa$}
	+ (0.5,0.5) coordinate (B)
	+ (1.1,-0.5) node (E) {}
	+ (1.1,0.5) node (D) {$\RV{D}$};
	\draw (A) -- (copy0);
	\draw (copy0) -- (gam);
	\draw (copy0) to [bend right] (kap);
	\draw (gam) -- (copy1);
	\draw (copy1) to [bend left] (B);
	\draw (copy1) to [bend right] (kap);
	\draw[-{Rays [n=8]}] (kap) to (E);
	\draw (B) to (D);
\end{tikzpicture}
&=
\begin{tikzpicture}
	\path (0,0) coordinate (A)
	++ (0.3,0) coordinate (copy0)
	++ (0.5,0) node[kernel] (gam) {$\gamma$}
	++ (0.5,0) node[kernel] (kap) {$\kappa$}
	++ (0.5,0) coordinate (copy1)
	+ (0.5,0.5) node[kernel] (kapdag) {$\kappa^*$}
	+ (0.5,-0.5) coordinate (B)
	+ (1.1,-0.5) node (E) {}
	+ (1.1,0.5) node (D) {$\RV{D}$};
	\draw (A) -- (copy0);
	\draw (copy0) -- (gam);
	\draw (copy0) to [bend right=40] (kap);
	\draw (gam) -- (kap);
	\draw (kap) -- (copy1);
	\draw (copy1) to [bend right] (B);
	\draw (copy1) to [bend left] (kapdag);
	\draw (kapdag) to (D);
	\draw[-{Rays [n=8]}] (B) to (E);
\end{tikzpicture} \\
\begin{tikzpicture}
	\path (0,0) coordinate (A)
	++ (0.3,0) coordinate (copy0)
	++ (0.5,0) node[kernel] (gam) {$\gamma$}
	+ (0.5,0) node (D) {$\RV{D}$};
	\draw (A) -- (copy0);
	\draw (copy0) -- (gam);
	\draw (gam) -- (D);
\end{tikzpicture}
&=
\begin{tikzpicture}
	\path (0,0) coordinate (A)
	++ (0.5,0) node[kernel] (mu) {$\mu$}
	+ (0.5,0) node[kernel] (kapdag) {$\kappa^*$}
	+ (1.1,0) node (D) {$\RV{D}$};
	\draw (A) -- (mu);
	\draw (mu) --  (kapdag);
	\draw (kapdag) to (D);
\end{tikzpicture} \label{eq:kappa_cancel}
\end{align}
Where \ref{eq:kappa_cancel} follows from \ref{eq:recoverability}.

The following assumption is a formalisation of the notion that ``we can determine $\mu$ precisely from observation'' (alternatively, that we can find an optimal decision for a classical statistical decision problem). Suppose that $\mu$ is characterised by some kernel $^*\mu$. That is,

\begin{align}
\begin{tikzpicture}
\path (0,0) node (A) {}
++ (0.5,0) node[kernel] (mu0) {$\mu$}
++ (0.5,0) node[kernel] (smu) {$^*\mu$}
++ (0.5,0) coordinate (copy0)
+  (0.5,0.5) coordinate (B)
+  (1,0.5) coordinate (T)
+  (0.5,-0.5) node[kernel] (mu1) {$\mu$}
+  (1,-0.5) coordinate (E);
\draw (A) -- (mu0) -- (smu) -- (copy0);
\draw (copy0) to [bend left] (B);
\draw (B) -- (T);
\draw (copy0) to [bend right] (mu1);
\draw (mu1) -- (E);
\end{tikzpicture}=
\begin{tikzpicture}
\path (0,0) node (A) {}
++ (0.5,0) coordinate (copy0)
++ (0.5,0.5) node[kernel] (mu0) {$\mu$}
++ (0.5,0) node[kernel] (smu) {$^*\mu$}
++ (0.5,0) coordinate (T);
\path (copy0)
++  (0.5,-0.5) node[kernel] (mu1) {$\mu$}
++  (1,0) coordinate (E);
\draw (A) -- (copy0);
\draw (copy0) to [bend left] (mu0);
\draw (mu0) -- (smu) -- (T);
\draw (copy0) to [bend right] (mu1);
\draw (mu1) -- (E);
\end{tikzpicture}\label{eq:starmu}
\end{align}

An equivalent condition to \ref{eq:starmu} is that for all $\theta,\theta'\in\mathscr{T}$, $A\in \mathcal{E}$, we have $\mu(\theta;A) = \mu(\theta';A)$, $\mu^*\mu(\theta;\cdot)$-almost surely. More informally,the support of $\mu^*\mu$ for each input $\theta$ divides $\mathscr{T}$ into equivalence classes such that for all $\theta$ in a given equivalence class, $\mu$ maps to the same probability measure on $\mathscr{E}$. 

Note that as a result of \ref{eq:starmu} we also have $\mu^*\mu\mu=\mu$. This weaker condition is not sufficient for the following result.

\todo[inline]{There is a connection between equation \ref{eq:starmu} and the notion of a sufficient statistic}

% A corollary of Lemma \ref{th:rightleft_inverse} is that left inverses have the following property:

% \begin{align}
% \begin{tikzpicture}
%  \path (0,0) node (A) {}
%  ++(0.5,0) node[kernel] (C) {$\;^\dagger A\;$}
%  ++(0.5,0) coordinate (D)
%  +(0.5,-0.5) node[kernel] (X) {$A$}
%  +(1.2,0.5) node (Y) {}
%  +(1.2,-0.5) node (Z) {};
%  \draw (A)--(C);
%  \draw (C) -- (D);
%  \draw (D) to [bend right] (X);
%  \draw (X) -- (Z);
%  \draw (D) to [bend left] (Y);
% \end{tikzpicture} = \begin{tikzpicture}
%  \path (0,0) node (A) {}
%  ++(0.5,0) coordinate (D)
%  +(0.5,0.5) node[kernel] (X) {$\;^\dagger A\;$}
%  +(1,-0.5) node (Y) {}
%  +(1,0.5) node (Z) {};
%  \draw (A)--(D);
%  \draw (D) to [bend left] (X);
%  \draw (X) -- (Z);
%  \draw (D) to [bend right] (Y);
% \end{tikzpicture}\label{eq:rightinverse_commute}
% \end{align}

% And \citet{fong_causal_2013} has shown that for deterministic $A$ we have

% \begin{align}
% \begin{tikzpicture}
%  \path (0,0) node (A) {}
%  ++(0.5,0) node[kernel] (C) {$A$}
%  ++(0.5,0) coordinate (D)
%  +(1.2,0.5) node (Y) {}
%  +(1.2,-0.5) node (Z) {};
%  \draw (A)--(C);
%  \draw (C) -- (D);
%  \draw (D) to [bend right] (Z);
%   \draw (D) to [bend left] (Y);
% \end{tikzpicture} = \begin{tikzpicture}
%  \path (0,0) node (A) {}
%  ++(0.5,0) coordinate (D)
%  +(0.5,0.5) node[kernel] (A1) {$A$}
%  +(0.5,-0.5) node[kernel] (A2) {$A$}
%  +(1,-0.5) node (Y) {}
%  +(1,0.5) node (Z) {};
%  \draw (A)--(D);
%  \draw (D) to [bend right] (A2);
%  \draw (A2) -- (Y);
%  \draw (D) to [bend left] (A1);
%  \draw (A1) -- (Z);
% \end{tikzpicture}\label{eq:det_commute}
% \end{align}

We then have

\begin{align}
\begin{tikzpicture}
 \path (0,0) node (A) {}
 ++(0.3,0) coordinate (copy0)
 ++(0.5,0) node[kernel] (mu) {$\mu$}
 ++(1,0) node[kernel] (kapdag) {$\kappa^*$}
 ++(0.4,0) coordinate (copy1)
 ++(0.5,0) node[kernel] (kap) {$\kappa$}
 +(1,0) node (E) {}
 +(1,-0.5) node (D) {}
 +(1,0.5) node (T) {};
 \draw (A)--(mu) -- (kapdag) -- (kap) -- (E);
 \draw (copy0) to [bend left] (kap) (copy0) to [bend left] (T);
 \draw (copy1) to [bend right] (D);
\end{tikzpicture}
 & \overset{\ref{eq:general_kapdag}}{=}
\begin{tikzpicture}
 \path (0,0) node (A) {}
 ++(0.3,0) coordinate (copy0)
 ++(0.5,0) node[kernel] (mu) {$\mu$}
 ++(1,0) node[kernel] (dagmumu) {$^*\mu\mu$}
 ++(0.4,0) coordinate (copy1)
 ++(0.5,-0.5) node[kernel] (kapsta) {$\kappa^*$}
 +(1,0.5) node (E) {}
 +(1,0) node (D) {}
 +(1,1) node (T) {};
 \draw (A)--(mu) -- (dagmumu) -- (copy1);
 \draw (copy0) to [bend left] (T);
 \draw (copy1) to [bend right] (kapsta) (copy1) to [bend left] (E);
 \draw (kapsta) -- (D);
\end{tikzpicture}\\
&=
\begin{tikzpicture}
 \path (0,0) node (A) {}
 ++(0.3,0) coordinate (copy0)
 ++(0.7,0) node[kernel] (mudagmu) {$\mu^*\mu$}
 ++(0.7,0) coordinate (copy1)
 ++(0.5,0) node[kernel] (mu) {$\mu$}
 ++(0.5,0) node[kernel] (kapsta0) {$\kappa^*$}
 ++(0.5,0) node[kernel] (kap) {$\kappa$}
 ++(0.4,0) coordinate (copy2)
 ++(0.5,-0.5) node[kernel] (kapsta1) {$\kappa^*$}
 +(1,0.5) node (E) {}
 +(1,0) node (D) {}
 +(1,1	) node (T) {};
 \draw (A)--(mudagmu) -- (mu) -- (kapsta0) -- (kap) -- (copy2);
 \draw (copy0) to [bend left] (T);
 \draw (copy1) to [bend left = 40] (kap);
 \draw (copy2) to [bend right] (kapsta1) (copy2) to [bend left] (E);
 \draw (kapsta1) -- (D);
\end{tikzpicture}\\
&\overset{\ref{eq:general_kapdag}}{=} 
\begin{tikzpicture}
 \path (0,0) node (A) {}
 ++(0.3,0) coordinate (copy0)
 ++(0.7,0) node[kernel] (mudagmu) {$\mu^*\mu$}
 ++(0.7,0) coordinate (copy1)
 ++(0.5,0) node[kernel] (mu) {$\mu$}
 ++(0.5,0) node[kernel] (kapsta0) {$\kappa^*$}
 ++(0.4,0) coordinate (copy2)
 ++(0.5,0) node[kernel] (kap) {$\kappa$}
 +(1,0) node (E) {}
 +(1,-0.5) node (D) {}
 +(1,0.5) node (T) {};
 \draw (A)--(mudagmu) -- (mu) -- (kapsta0) -- (kap) -- (copy2);
 \draw (copy0) to [bend left] (T);
 \draw (copy1) to [bend left =40] (kap);
 \draw (copy2) to [bend right] (D) (kap) -- (E);
\end{tikzpicture}\\
&\overset{\ref{eq:rightinverse_commute}}{=}
\begin{tikzpicture}
 \path (0,0) node (A) {}
 ++(0.3,0) coordinate (copy0)
 ++(0.7,0) node[kernel] (mudagmu) {$\mu$}
 ++(0.7,0) coordinate (copy1)
 +(0.5,0.5) node[kernel] (dagmu) {$^*\mu$}
 ++(0.5,0) node[kernel] (kapsta0) {$\kappa^*$}
 ++(0.4,0) coordinate (copy2)
 ++(0.5,0) node[kernel] (kap) {$\kappa$}
 +(1,0) node (E) {}
 +(1,-0.5) node (D) {}
 +(1,1) node (T) {};
 \draw (A)--(mudagmu) -- (kapsta0) -- (kap) -- (copy2);
 \draw (copy0) to [bend left] (T);
 \draw (copy1) to [bend left] (dagmu);
 \draw (dagmu) to [bend left] (kap);
 \draw (copy2) to [bend right] (D) (kap) -- (E);
\end{tikzpicture}\\
&\overset{\ref{eq:starmu}}{=}
\begin{tikzpicture}
 \path (0,0) node (A) {}
 ++(0.3,0) coordinate (copy0)
 +(0.9,0.5) node[kernel] (mu) {$\mu$}
 +(1.5,0.5) node[kernel] (dagmu) {$^*\mu$}
 ++(1.4,0) node[kernel] (kapsta0) {$\mu \kappa^*$}
 ++(0.4,0) coordinate (copy2)
 ++(0.5,0) node[kernel] (kap) {$\kappa$}
 +(1,0) node (E) {}
 +(1,-0.5) node (D) {}
 +(1,1) node (T) {};
 \draw (A)-- (kapsta0) -- (kap) -- (copy2);
 \draw (copy0) to [bend left=40] (T);
 \draw (copy0) to [bend left] (mu);
 \draw (mu) -- (dagmu) to [bend left] (kap);
 \draw (copy2) to [bend right] (D) (kap) -- (E);
\end{tikzpicture}\label{eq:skipmu}
\end{align}

Equation \ref{eq:skipmu} implies that, given any $\xi\in\Delta(\mathscr{T})$, all distributions of the form

\begin{align}
\begin{tikzpicture}
 \path (0,0) node[dist] (A) {$\xi$}
 ++(0.3,0) coordinate (copy0)
 ++(0.5,0) node[kernel] (mu) {$\mu$}
 ++(1,0) node[kernel] (kapdag) {$\kappa^*$}
 ++(0.4,0) coordinate (copy1)
 ++(0.5,0) node[kernel] (kap) {$\kappa$}
 +(1,0) node (E) {$\RV{E}$}
 +(1,-0.5) node (D) {$\RV{D}$}
 +(1,0.5) node (T) {$\RV{T}$};
 \draw (A)--(mu) -- (kapdag) -- (kap) -- (E);
 \draw (copy0) to [bend left] (kap) (copy0) to [bend left] (T);
 \draw (copy1) to [bend right] (D);\label{eq:reference_dist}
\end{tikzpicture}
\end{align}

admit both $\kappa:=$\begin{tikzpicture}
\path (0,0) node[kernel] (kap) {$\kappa^{\,}$};
\draw ($(kap.west) +(-0.5,0.15)$) -- ($(kap.west) +(0,0.15)$) ($(kap.west) +(-0.5,-0.15)$) -- ($(kap.west) +(0,-0.15)$);
\draw (kap) -- (0.5,0);
\end{tikzpicture} and $\kappa_{\mathrm{fac}}:=$\begin{tikzpicture}
\path (0,0) node[kernel] (kap) {$\kappa^{\,}$}
 + (-0.8,0.15) node[kernel] (mdm) {$\mu^*\mu$};
\draw ($(kap.west) +(-1.25,-0.15)$) -- ($(kap.west) +(0,-0.15)$);
\draw (kap) -- (0.5,0);
\draw ($(mdm.west)+(-0.2,0)$) -- (mdm) -- ($(kap.west) +(0,0.15)$);
\end{tikzpicture} as disintegrations from $(\RV{D},\RV{T})\dashrightarrow\RV{E}$. Therefore these $\kappa$ and $\kappa_{\mathrm{fac}}$ agree almost surely with respect to the distribution \ref{eq:reference_dist} for any prior $\xi$. 

However, also by assumption \ref{eq:starmu}, we have that for $\theta,\theta'\in \mathscr{T}$  either $\mu(\theta;A)=\mu(\theta';A)$ for all $A\in \mathcal{E}$, or for any $A\in\mathcal{E}$ $\mu(\theta;A)=0$ or $\mu(\theta';A)=0$. That is, any two states either have the same probability measure or probability measures with disjoint support. This is problematic, as the distribution \ref{eq:reference_dist} then has no support over much of the space $D\times E\times \mathscr{T}$. If $\mu$ were deterministic, for example, and hence associated with some function $f$, while \ref{eq:starmu} would be guaranteed via a left inverse, \ref{eq:reference_dist} would be supported on a subset of $D\times\{(\theta,f(\theta))|\theta\in\mathscr{T}\}$. In particular, we have no guarantee that the desired equality of $\kappa$ and $\kappa_{\mathrm{fac}}$ holds if we take any decision that doesn't reproduce the observed distribution. This isn't totally trivial: we may live in a world where most actions make things worse, in which case knowing how to keep things the same is valuable.

A stronger result can be found if we assume we have an infinite sequence of RVs $\RV{X}_i:E\to W$ and $\RV{D}_i:D\to V$ such that
\begin{itemize}
	\item $W^\mathbb{N}=E$, $V^\mathbb{N}=D$ (i.e. the sequence of all $\RV{X}_i$'s is identified with $E$ and the sequence of all $\RV{D}_i$'s is identified with $D$)
 	\item $\mu = \splitter{0.1} \otimes_{i\in \mathbb{N}} \mu F_{\RV{X}_i}$ (the $\RV{X}_i$'s are ``IID conditional on $\theta$'')\todo{this might be closely related to exchangeability via de Finetti?}
 	\item There exists $\kappa_0$ such that $\kappa = \splitter{0.1}\otimes_{i\in\mathbb{N}} (F_{\RV{D}_i}\otimes \mathrm{Id}_\mathscr{T}) \kappa_0 F_{\RV{X}_i}$ ($\kappa$ is ``IID conditional on $\RV{D},\theta$'')
 \end{itemize} 

Here we define the ``infinite copy map'' $\splitter{0.1}\otimes_{i\in\mathbb{N}}\mu F_{\RV{X}_i}$ to denote the kernel $\theta\mapsto \nu_\theta$ where $\nu_\theta$ the unique distribution such that for all finite $A\subset \mathbb{N}$ and projections $\pi_{A}:E\to \Delta(W^{|A|})$, $\nu_\theta \pi_{A}=\otimes_{i\in A}\mu_\theta F_{\RV{X}_i}$. This distribution is unique via the Kolmogorov extension theorem (the symmetry of the copy map guarantees the required consistency conditions) \citep{tao_introduction_2011}.

\todo[inline]{I assume, for now, that measurability can be worked out in some cases; in particular, that there is a $\sigma$-algebra on infinite sequences that renders the above kernel measurable in the appropriate way.}

\begin{lemma}[``IID'' kernels agree on truncations]\label{lem:agree_on_truncations}
For finite $A\subset\mathbb{N}$, $y,y'\in D$, if $\otimes_{i\in A} \RV{X}_i(y) = \otimes_{i\in A} \RV{X}_i(y')$ and $\kappa:\mathscr{T}\times D\to \Delta(\mathcal{E})$ is ``IID'' in the sense above then for all $\theta\in \mathscr{T}$, $B\in\mathcal{W}^{|A|}$, $\kappa(\theta,y;B) \pi_A = \kappa(\theta,y';B) \pi_A$.
\end{lemma}

\begin{proof}
By definition, we have 
\begin{align}
	\kappa \pi_A (\theta,y;B) &= \otimes_{i\in A} \kappa F_{\RV{X}_i} (\theta,\RV{D}_i(y);B)\\
							 &= \otimes_{i\in A} \kappa F_{\RV{X}_i} (\theta,\RV{D}_i(y');B)\\
							 &= \kappa \pi_A (\theta,y';B)
\end{align}
\end{proof}

Suppose both $\RV{X}_i$ and $\RV{D}_i$ are binary, and that for each $\theta\in \mathscr{T}$ we have recoverability (Eq. \ref{eq:recoverability}) with $\mu_\theta=\gamma_\theta$ (we will conclude that $\RV{X}$ is ``directly controlled'' by $\RV{D}$, but we will not assume this at the outset). $\kappa^*$ is therefore trivial. For each $\theta$, $\RV{X}_i$ are IID Bernoulli variables and so each $\mu_\theta$ is characterised by a single parameter $p$; let $p_\theta$ be the value of this parameter for some given $\theta$. Define $\overline{\RV{X}} := \lim_{n\to\infty} \frac{1}{m}\sum_{i\in[n]} \RV{X}_i$ and $^*\mu$ to be any kernel $E\to \Delta(\mathscr{T})$ such that the support of $^*\mu(x;\cdot)$ is a subset of $\{\theta|p_\theta=\overline{\RV{X}}(x)\}$. Note that for any $\theta,\theta'\in \mathscr{T}$ we have either $p_\theta=p_{\theta'}$ and so $\mu(\theta;A)=\mu(\theta';A)$ for all $A$ or $\theta'$ is not in the support of $\mu^*\mu(\theta;\cdot)$. Thus we have \ref{eq:starmu}, and hence ``almost sure'' equality of $\kappa$ and $\kappa_{\mathrm{fac}}$.

However with the exception of states where $p_\theta=0$ or $1$, almost sure equality is enough for $\kappa_{\mathrm{fac}}\pi_A(\theta,y;B)=\kappa\pi_A(\theta,y;B)$ for all $y\in D$, finite $A\subset\mathbb{N}$ and $B\in \mathcal{W}^{|A|}$. Then by the Kolmogorov extension theorem, we also have $\kappa_{\mathrm{fac}} (\theta,y;B) = \kappa(\theta,y;B)$ for all $y\in D$ and ``almost all'' $\theta\in \mathscr{T}$.

This appears to have similarities to the general case where we are trying to identify a particular function from some set of possible functions and we know the output of that function for a subset of inputs. It still comes down to a question of whether or not the set of functions in question is small enough to be fully characterised by the set of inputs we're allowed to see.

% From \ref{eq:gamkap_2_mu} and \ref{eq:kappa_cancel} we have

% \begin{align}
% \begin{tikzpicture}
% 	\path (0,0) coordinate (A)
% 	+  (0,0.15) coordinate (copy0)
% 	++ (0,0.5) node[kernel] (mu) {$\mu$}
% 	++ (0,0.5) node[kernel] (kapdag) {$\kappa^{\dagger}$}
% 	++ (0,0.5) coordinate (copy1)
% 	+  (0.6,0.5) node[kernel] (kap) {$\kappa$}
% 	+  (-0.5,0.5) coordinate (B) 
% 	+  (0.6	,1) node (E) {$\RV{E}$}
% 	+  (-0.5,1) node (D) {$\RV{D}$};
% 	\draw (A) -- (mu);
% 	\draw (mu) -- (kapdag);
% 	\draw (kapdag) -- (copy1);
% 	\draw (copy1) to [bend left] (B);
% 	\draw (copy1) to [bend right]  (kap);
% 	\draw (kap) to (E);
% 	\draw (B) to (D);
% 	\draw (copy0) to [bend right] (kap);
% \end{tikzpicture} &= 
% \begin{tikzpicture}
% 	\path (0,0) coordinate (A)
% 	++ (0,0.25) coordinate (copy0)
% 	++ (0,0.5) node[kernel] (gam) {$\gamma$}
% 	++ (0,0.5) coordinate (copy1)
% 	+ (0.5,0.5) node[kernel] (kap) {$\kappa$}
% 	+ (-0.5,0.5) coordinate (B)
% 	+ (0.5,1) node (E) {$\RV{E}$}
% 	+ (-0.5,1) node (D) {$\RV{D}$};
% 	\draw (A) -- (copy0);
% 	\draw (copy0) -- (gam);
% 	\draw (copy0) to [bend right] (kap);
% 	\draw (gam) -- (copy1);
% 	\draw (copy1) to [bend left] (B);
% 	\draw (copy1) to [bend right] (kap);
% 	\draw (kap) to (E);
% 	\draw (B) to (D);
% \end{tikzpicture}
% \end{align}

% Note that, given some prior $\xi\in \Delta(\mathscr{T})$, the ``Bayes result'' of invoking some decision rule $J:E\to \Delta(\mathcal{D})$ is given by

% \begin{align}
% \begin{tikzpicture}
%  \path (0,0) node[dist] (A) {$\xi$}
%  ++(0,0.3) coordinate (copy0)
%  ++(0,0.5) node[kernel] (mu) {$\mu$}
%  ++(0,0.5) node[kernel] (kapdag) {$J$}
%  ++(0,0.7) node[kernel] (kap) {$\kappa$}
%  ++(0,0.5) node (E) {$\RV{E}$};
%  \draw (A)--(mu) -- (kapdag) -- (kap) -- (E);
%  \draw (copy0) to [bend right = 45] (kap);
% \end{tikzpicture}
% \end{align}


% Given these two assumptions, we have
% \begin{align}
% 	\begin{tikzpicture}[scale=0.8]
% 		\path (0,0) node[dist] (A) {$\mu$}
% 		++(0,1) coordinate (C)
% 		+(1,2) node (F) {$*$}
% 		++(-1,1) node[kernel] (E) {$\kappa^*$}
% 		++(0,1) coordinate (G)
% 		+(1,0.5) node[kernel] (D) {$\kappa$}
% 		+(0,1) coordinate (H)
% 		+(1,1) coordinate (I);
% 		\draw (A) -- (C);
% 		\draw (C) to [bend right] (F.center);
% 		\draw (C) to [bend left] (E);
% 		\draw (E) -- (G);
% 		\draw (G) -- (H);
% 		\draw (G) to [bend right] (D);
% 		\draw (D) to (I);
% 	\end{tikzpicture}&=
% 		\begin{tikzpicture}[scale=0.8]
% 		\path (0,0) node[dist] (A) {$\mu$}
% 		++(0,1) node[kernel] (E) {$\kappa^*$}
% 		++(0,0.5) coordinate (G)
% 		+(1,0.5) node[kernel] (D) {$\kappa$}
% 		+(-1,1) coordinate (H)
% 		+(1,1) coordinate (I);
% 		\draw (A) -- (E);
% 		\draw (E) -- (G);
% 		\draw (G) to [bend left] (H);
% 		\draw (G) to [bend right] (D);
% 		\draw (D) to (I);
% 	\end{tikzpicture}\\
% 	&=
% 		\begin{tikzpicture}[scale=0.8]
% 		\path (0,0) node[dist] (A) {$\mu$}
% 		++(0,1) coordinate (G)
% 		+(1,1) node[kernel] (B) {$\kappa^*$}
% 		+(-1,1) node[kernel] (C) {$\kappa^*$}
% 		+(1,2) node[kernel] (D) {$\kappa$}
% 		+(-1,3) coordinate (H)
% 		+(1,3) coordinate (I);
% 		\draw (A) -- (G);
% 		\draw (G) to [bend left] (C);
% 		\draw (G) to [bend right] (B);
% 		\draw (C) to (H);
% 		\draw (B) to (D);
% 		\draw (D) to (I);
% 	\end{tikzpicture}\\
% 	&=
% 	\begin{tikzpicture}[scale=0.8]
% 		\path (0,0) node[dist] (A) {$\mu$}
% 		++(0,1) coordinate (B) 
% 		+(1,2) coordinate (C)
% 		+(-1,1) node[kernel] (D) {$\kappa^*$}
% 		+(-1,2) coordinate (E);
% 		\draw (A) -- (B);
% 		\draw (B) to [bend right] (C);
% 		\draw (B) to [bend left] (D);
% 		\draw (D) -- (E);
% 	\end{tikzpicture}\\
% 	&:= \mu^{\dagger|}
% \end{align}

% Here we recall the definition of a conditional probability: a conditional probability \inkernel{\RV{Y}|\RV{X}}{}{} with respect to $\nu\in \Delta(\mathcal{E})$ and RVs $\RV{X}:E\to X$, $\RV{Y}:E\to Y$ is any Markov kernel $X\to \Delta(\mathcal{Y})$ such that
% \begin{align}
% 	\begin{tikzpicture}[scale=0.7]
% 	\path (0,0) node[dist,inner sep=2pt] (A) {$\nu$}
% 	+ (0.5,1) node (B) {$X$}
% 	+ (-0.5,1) node (C) {$Y$};
% 	\draw (B) -- (B|- A.north);
% 	\draw (C) -- (C|- A.north);
% 	\end{tikzpicture} = 
% 	\begin{tikzpicture}[scale=0.7]
% 	\path (0,0) node[dist,inner sep=2pt] (A) {$\nu$}
% 	+ (-0.5,1) node (C) {$*$}
% 	++ (0.5,1) coordinate (B)
% 	+ (-1,1) node[kernel] (D) {$\RV{Y}|\RV{X}$}
% 	+(0,2) node (E) {$X$}
% 	+(-1,2) node (F) {$Y$};
% 	\draw (B) -- (B|- A.north);
% 	\draw (C.center) -- (C|- A.north);
% 	\draw (B) -- (E);
% 	\draw (B) to [bend left] (D);
% 	\draw (D) -- (F);
% 	\end{tikzpicture}
% \end{align}

% Thus we can see that $\kappa$ is a conditional probability \inkernel{\RV{E}|\RV{D}}{}{} with respect to $\mu^{\dagger|}$ the RVs $\RV{D}:E\times D\to D$ and $\RV{E}:E\times D\to E$ given by the respective projections.

% This result is in line with two familiar cases from the causal literature: an \emph{ignorable} treatment in potential outcomes or a graphical model with no back-door paths between $\RV{D}$ and $\RV{E}$ both imply that the theory's respective version of ``causal effect'' can be determined directly from the conditional probability. This suggests that reproducibility is in fact quite a strong condition, as it is the exception rather than the norm where causal effects can be determined directly from conditional probabilities. There are two important caveats here, however: firstly we've assumed $\kappa$ has a known right-inverse $\kappa^*$, and secondly this result only allows $\kappa$ to be determined $\mu^{\dagger|}$-almost surely.

% \todo[inline]{A notion of ``approximate right-invertibility'' would be very helpful here. Something like $\kappa$ is approximately right invertible if there is some $\tilde{\kappa}^\dagger$ such that $\gamma \kappa\tilde{\kappa}^\dagger$ is always ``close'' to $\gamma$. Intuitively, a consequence that is \emph{not} approximately right invertible has a channel capacity of 0, and \textbf{Conjecture:} a set of kernels that do not collectively admit some approximate right-inverse do not admit preferences between decisions under the minimax rule}

% \section{More Complex Cases}

% Suppose we retain the right invertibility of $\kappa$ via some known $\kappa^*$, which is consistent with (for example) hard interventions and counterfactual variable forcing. A more common way that there fails to be a memoryless function $J_\theta$ such that $J_\theta\kappa=\mu$ arises from the fact that we may often regard the consequence as dynamic. Assign an index $\theta\in \Theta$ to each causal state, which we now denote $(\kappa_\theta,\mu_\theta)\in \mathscr{T}$ and let $\kappa^\Theta:\Theta\times D\to \Delta(\mathcal{E})$ be defined by $\kappa^\Theta:(\theta,y;A)\mapsto \kappa_\theta(y;A)$ which we will assume is also a Markov kernel (noting that this intoduces additional restrictions on $\mathscr{T}$ in the general case). We may assume, then, for some state-dependent generating distribution $\xi_\theta\in \Delta(\Theta)$ and some state dependent ``passive decision map'' $K_\theta:\Theta\to \Delta(\mathcal{D})$, we have $\mu_\theta = \xi_\theta\splitter{0.1}(I_{(\Theta)}\otimes K_\theta)\kappa^\Theta$. 

% \indist{a}{b}

% \inkernel{ab}{a}{b}