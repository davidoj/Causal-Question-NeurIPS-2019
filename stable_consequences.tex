%!TEX root = main.tex

\section{Recoverability}

A natural assumption suggested by the notion of a CSDP is that of \emph{recoverability} - that a causal theory $\mathscr{T}:E\times D\rightarrowtriangle E$ permits some decision function that reproduces the distribution of the observed data. That is, we assume that for every $(\kappa_\theta,\mu_\theta):=\theta\in \mathscr{T}$ there exists $\gamma_{\theta}\in \Delta(\mathcal{D})$ such that
\begin{align}
	\gamma_{\theta}\kappa_\theta = \mu_\theta \label{eq:recoverability}
\end{align}

Suppose also that we have some $\kappa^*$ that, for all $\theta \in \mathscr{T}$, is a Bayesian inversion of $\gamma_\theta$ and $\kappa_\theta$; that is:

\begin{align}
\begin{tikzpicture}
	\path (0,0) node[dist] (gam) {$\gamma_\theta$}
	++(0.5,0) coordinate (copy0)
	+(0.5,0.5) node[kernel] (kap) {$\kappa_\theta$}
	+(0.5,-0.5) coordinate (ph)
	+(1,0.5) node (E) {$\RV{E}$}
	+(1,-0.5) node (D) {$\RV{D}$};
	\draw (gam) -- (copy0);
	\draw (copy0) to [bend left] (kap);
	\draw (copy0) to [bend right] (ph);
	\draw (kap) -- (E);
	\draw (ph) -- (D); 
\end{tikzpicture}= \begin{tikzpicture}
	\path (0,0) node[dist] (gam) {$\gamma_\theta$}
	++(0.6,0) node[kernel] (kap) {$\kappa_\theta$}
	++(0.5,0) coordinate (copy0)
	+(0.5,-0.5) node[kernel] (kapdag) {$\kappa^*$}
	+(0.5,0.5) coordinate (ph)
	+(1,0.5) node (E) {$\RV{E}$}
	+(1,-0.5) node (D) {$\RV{D}$};
	\draw (gam) -- (kap);
	\draw (kap) -- (copy0);
	\draw (copy0) to [bend right] (kapdag);
	\draw (copy0) to [bend left] (ph);
	\draw (kapdag) -- (D);
	\draw (ph) -- (E); 
\end{tikzpicture} \label{eq:kappa_BI}
\end{align}

A sufficient condition for the existence of such a $\kappa^*$ is the assumption that decisions correspond to \emph{variable setting} - that is, there is some variable $\RV{X}:E\to X$ such that for all $a\in D$, $\theta\in\mathscr{T}$ we have $\delta_a \kappa_\theta F_{\RV{X}} = \delta_a$ (such an assumption arises in graphical models as hard interventions, and in potential outcomes as ``potential-outcome identifiers''). Indeed $F_{\RV{X}}$ is in this case a candidate for $\kappa^*$. It is not necessary that $\kappa^*$ be deterministic, however - suppose every $\kappa$ ignores $D$. Then choose $\gamma_\theta=\gamma$ for arbitrary $\gamma\in \Delta(\mathcal{D})$ and it can be verified that $\kappa^*:b\mapsto \gamma$ satisfies \ref{eq:kappa_BI}. 

I believe a weaker sufficient condition for the existence of a universal $\kappa^*$ is that every $\kappa_\theta$ factorises as $\kappa_\theta = h \splitter{0.1}(\mathrm{Id}_F\otimes j_\theta)$ for some fixed $h:D\to \Delta(\mathcal{F})$, but I have not yet shown this.

We will proceed somewhat rashly: suppose that by defining $\gamma:\mathscr{T}\to \Delta(\mathcal{D})$, $\mu:\mathscr{T}\to \Delta(\mathcal{E})$ and $\kappa:\mathscr{T}\times D\to \Delta(\mathcal{E}$ by $\gamma:\theta\to \gamma_\theta$, $\mu:\theta\to \mu_\theta$ and $\kappa:(\theta,d)\to \kappa_\theta(d;\cdot)$ that all resulting objects are Markov kernels, and that $\mathscr{T}$ is a standard measurable space.

By previous assumptions, we have the following properties:
\begin{align}
\begin{tikzpicture}
	\path (0,0) coordinate (A)
	++ (0.5,0) node[kernel] (B) {$\mu$}
	++ (0.5,0) coordinate (C);
	\draw (A) -- (B);
	\draw (B) -- (C);
\end{tikzpicture}
&=
\begin{tikzpicture}
	\path (0,0) coordinate (A)
	++ (0.3,0) coordinate (copy0)
	++(0.5,0) node[kernel] (gam) {$\gamma$}
	+ (0,0.3) coordinate (B)
	++(0.5,0) node[kernel] (kap) {$\kappa$}
	++(0.5,0) coordinate (C);
	\draw (A) -- (copy0);
	\draw (copy0) to [bend left] (B);
	\draw (copy0) -- (gam);
	\draw (B) to [bend left] (kap.west);
	\draw (gam) -- (kap);
	\draw (kap) -- (C);
\end{tikzpicture}\label{eq:recoverabilityd}\\
\begin{tikzpicture}
	\path (0,0) coordinate (A)
	++ (0.3,0) coordinate (copy0)
	++ (0.5,0) node[kernel] (gam) {$\gamma$}
	++ (0.5,0) coordinate (copy1)
	+ (0.5,-0.5) node[kernel] (kap) {$\kappa$}
	+ (0.5,0.5) coordinate (B)
	+ (1,-0.5) node (E) {$\RV{E}$}
	+ (1,0.5) node (D) {$\RV{D}$};
	\draw (A) -- (copy0);
	\draw (copy0) -- (gam);
	\draw (copy0) to [bend right] (kap);
	\draw (gam) -- (copy1);
	\draw (copy1) to [bend left] (B);
	\draw (copy1) to [bend right] (kap);
	\draw (kap) to (E);
	\draw (B) to (D);
\end{tikzpicture}
&=
\begin{tikzpicture}
	\path (0,0) coordinate (A)
	++ (0.3,0) coordinate (copy0)
	++ (0.5,0) node[kernel] (gam) {$\gamma$}
	++ (0.5,0) node[kernel] (kap) {$\kappa$}
	++ (0.5,0) coordinate (copy1)
	+ (0.5,0.5) node[kernel] (kapdag) {$\kappa^*$}
	+ (0.5,-0.5) coordinate (B)
	+ (1,-0.5) node (E) {$\RV{E}$}
	+ (1,0.5) node (D) {$\RV{D}$};
	\draw (A) -- (copy0);
	\draw (copy0) -- (gam);
	\draw (copy0) to [bend right=40] (kap);
	\draw (gam) -- (kap);
	\draw (kap) -- (copy1);
	\draw (copy1) to [bend right] (B);
	\draw (copy1) to [bend left] (kapdag);
	\draw (kapdag) to (D);
	\draw (B) to (E);
\end{tikzpicture} \label{eq:general_kapdag}\\
&= 
\begin{tikzpicture}
	\path (0,0) coordinate (A)
	++ (0.3,0) coordinate (copy0)
	++ (0.5,0) node[kernel] (mu) {$\mu$}
	++ (0.5,0) coordinate (copy1)
	+ (0.5,0.5) node[kernel] (kapdag) {$\kappa^*$}
	+ (0.5,-0.5) coordinate (B)
	+ (1,-0.5) node (E) {$\RV{E}$}
	+ (1,0.5) node (D) {$\RV{D}$};
	\draw (A) -- (copy0);
	\draw (copy0) -- (mu);
	\draw (mu) -- (copy1);
	\draw (copy1) to [bend right] (B);
	\draw (copy1) to [bend left] (kapdag);
	\draw (kapdag) to (D);
	\draw (B) to (E);
\end{tikzpicture}\label{eq:gamkap_2_mu}
\end{align}
From \ref{eq:general_kapdag} we also have

\begin{align}
\begin{tikzpicture}
	\path (0,0) coordinate (A)
	++ (0.3,0) coordinate (copy0)
	++ (0.5,0) node[kernel] (gam) {$\gamma$}
	++ (0.5,0) coordinate (copy1)
	+ (0.5,-0.5) node[kernel] (kap) {$\kappa$}
	+ (0.5,0.5) coordinate (B)
	+ (1.1,-0.5) node (E) {}
	+ (1.1,0.5) node (D) {$\RV{D}$};
	\draw (A) -- (copy0);
	\draw (copy0) -- (gam);
	\draw (copy0) to [bend right] (kap);
	\draw (gam) -- (copy1);
	\draw (copy1) to [bend left] (B);
	\draw (copy1) to [bend right] (kap);
	\draw[-{Rays [n=8]}] (kap) to (E);
	\draw (B) to (D);
\end{tikzpicture}
&=
\begin{tikzpicture}
	\path (0,0) coordinate (A)
	++ (0.3,0) coordinate (copy0)
	++ (0.5,0) node[kernel] (gam) {$\gamma$}
	++ (0.5,0) node[kernel] (kap) {$\kappa$}
	++ (0.5,0) coordinate (copy1)
	+ (0.5,0.5) node[kernel] (kapdag) {$\kappa^*$}
	+ (0.5,-0.5) coordinate (B)
	+ (1.1,-0.5) node (E) {}
	+ (1.1,0.5) node (D) {$\RV{D}$};
	\draw (A) -- (copy0);
	\draw (copy0) -- (gam);
	\draw (copy0) to [bend right=40] (kap);
	\draw (gam) -- (kap);
	\draw (kap) -- (copy1);
	\draw (copy1) to [bend right] (B);
	\draw (copy1) to [bend left] (kapdag);
	\draw (kapdag) to (D);
	\draw[-{Rays [n=8]}] (B) to (E);
\end{tikzpicture} \\
\begin{tikzpicture}
	\path (0,0) coordinate (A)
	++ (0.3,0) coordinate (copy0)
	++ (0.5,0) node[kernel] (gam) {$\gamma$}
	+ (0.5,0) node (D) {$\RV{D}$};
	\draw (A) -- (copy0);
	\draw (copy0) -- (gam);
	\draw (gam) -- (D);
\end{tikzpicture}
&=
\begin{tikzpicture}
	\path (0,0) coordinate (A)
	++ (0.5,0) node[kernel] (mu) {$\mu$}
	+ (0.5,0) node[kernel] (kapdag) {$\kappa^*$}
	+ (1.1,0) node (D) {$\RV{D}$};
	\draw (A) -- (mu);
	\draw (mu) --  (kapdag);
	\draw (kapdag) to (D);
\end{tikzpicture} \label{eq:kappa_cancel}
\end{align}
Where \ref{eq:kappa_cancel} follows from \ref{eq:recoverability}.

The following assumption is a formalisation of the notion that ``we can determine $\mu$ precisely from observation'' (alternatively, that we can find an optimal decision for a classical statistical decision problem). Suppose that $\mu$ is characterised by some kernel $^*\mu$. That is,

\begin{align}
\begin{tikzpicture}
\path (0,0) node (A) {}
++ (0.5,0) node[kernel] (mu0) {$\mu$}
++ (0.5,0) node[kernel] (smu) {$^*\mu$}
++ (0.5,0) coordinate (copy0)
+  (0.5,0.5) coordinate (B)
+  (1,0.5) coordinate (T)
+  (0.5,-0.5) node[kernel] (mu1) {$\mu$}
+  (1,-0.5) coordinate (E);
\draw (A) -- (mu0) -- (smu) -- (copy0);
\draw (copy0) to [bend left] (B);
\draw (B) -- (T);
\draw (copy0) to [bend right] (mu1);
\draw (mu1) -- (E);
\end{tikzpicture}=
\begin{tikzpicture}
\path (0,0) node (A) {}
++ (0.5,0) coordinate (copy0)
++ (0.5,0.5) node[kernel] (mu0) {$\mu$}
++ (0.5,0) node[kernel] (smu) {$^*\mu$}
++ (0.5,0) coordinate (T);
\path (copy0)
++  (0.5,-0.5) node[kernel] (mu1) {$\mu$}
++  (1,0) coordinate (E);
\draw (A) -- (copy0);
\draw (copy0) to [bend left] (mu0);
\draw (mu0) -- (smu) -- (T);
\draw (copy0) to [bend right] (mu1);
\draw (mu1) -- (E);
\end{tikzpicture}\label{eq:starmu}
\end{align}

An equivalent condition to \ref{eq:starmu} is that for all $\theta,\theta'\in\mathscr{T}$, $A\in \mathcal{E}$, we have $\mu(\theta;A) = \mu(\theta';A)$, $\mu^*\mu(\theta;\cdot)$-almost surely. More informally,the support of $\mu^*\mu$ for each input $\theta$ divides $\mathscr{T}$ into equivalence classes such that for all $\theta$ in a given equivalence class, $\mu$ maps to the same probability measure on $\mathscr{E}$. 

Note that as a result of \ref{eq:starmu} we also have $\mu^*\mu\mu=\mu$. This weaker condition is not sufficient for the following result.

\todo[inline]{There is a connection between equation \ref{eq:starmu} and the notion of a sufficient statistic}

% A corollary of Lemma \ref{th:rightleft_inverse} is that left inverses have the following property:

% \begin{align}
% \begin{tikzpicture}
%  \path (0,0) node (A) {}
%  ++(0.5,0) node[kernel] (C) {$\;^\dagger A\;$}
%  ++(0.5,0) coordinate (D)
%  +(0.5,-0.5) node[kernel] (X) {$A$}
%  +(1.2,0.5) node (Y) {}
%  +(1.2,-0.5) node (Z) {};
%  \draw (A)--(C);
%  \draw (C) -- (D);
%  \draw (D) to [bend right] (X);
%  \draw (X) -- (Z);
%  \draw (D) to [bend left] (Y);
% \end{tikzpicture} = \begin{tikzpicture}
%  \path (0,0) node (A) {}
%  ++(0.5,0) coordinate (D)
%  +(0.5,0.5) node[kernel] (X) {$\;^\dagger A\;$}
%  +(1,-0.5) node (Y) {}
%  +(1,0.5) node (Z) {};
%  \draw (A)--(D);
%  \draw (D) to [bend left] (X);
%  \draw (X) -- (Z);
%  \draw (D) to [bend right] (Y);
% \end{tikzpicture}\label{eq:rightinverse_commute}
% \end{align}

% And \citet{fong_causal_2013} has shown that for deterministic $A$ we have

% \begin{align}
% \begin{tikzpicture}
%  \path (0,0) node (A) {}
%  ++(0.5,0) node[kernel] (C) {$A$}
%  ++(0.5,0) coordinate (D)
%  +(1.2,0.5) node (Y) {}
%  +(1.2,-0.5) node (Z) {};
%  \draw (A)--(C);
%  \draw (C) -- (D);
%  \draw (D) to [bend right] (Z);
%   \draw (D) to [bend left] (Y);
% \end{tikzpicture} = \begin{tikzpicture}
%  \path (0,0) node (A) {}
%  ++(0.5,0) coordinate (D)
%  +(0.5,0.5) node[kernel] (A1) {$A$}
%  +(0.5,-0.5) node[kernel] (A2) {$A$}
%  +(1,-0.5) node (Y) {}
%  +(1,0.5) node (Z) {};
%  \draw (A)--(D);
%  \draw (D) to [bend right] (A2);
%  \draw (A2) -- (Y);
%  \draw (D) to [bend left] (A1);
%  \draw (A1) -- (Z);
% \end{tikzpicture}\label{eq:det_commute}
% \end{align}

We then have

\begin{align}
\begin{tikzpicture}
 \path (0,0) node (A) {}
 ++(0.3,0) coordinate (copy0)
 ++(0.5,0) node[kernel] (mu) {$\mu$}
 ++(1,0) node[kernel] (kapdag) {$\kappa^*$}
 ++(0.4,0) coordinate (copy1)
 ++(0.5,0) node[kernel] (kap) {$\kappa$}
 +(1,0) node (E) {}
 +(1,-0.5) node (D) {}
 +(1,0.5) node (T) {};
 \draw (A)--(mu) -- (kapdag) -- (kap) -- (E);
 \draw (copy0) to [bend left] (kap) (copy0) to [bend left] (T);
 \draw (copy1) to [bend right] (D);
\end{tikzpicture}
 & \overset{\ref{eq:general_kapdag}}{=}
\begin{tikzpicture}
 \path (0,0) node (A) {}
 ++(0.3,0) coordinate (copy0)
 ++(0.5,0) node[kernel] (mu) {$\mu$}
 ++(1,0) node[kernel] (dagmumu) {$^*\mu\mu$}
 ++(0.4,0) coordinate (copy1)
 ++(0.5,-0.5) node[kernel] (kapsta) {$\kappa^*$}
 +(1,0.5) node (E) {}
 +(1,0) node (D) {}
 +(1,1) node (T) {};
 \draw (A)--(mu) -- (dagmumu) -- (copy1);
 \draw (copy0) to [bend left] (T);
 \draw (copy1) to [bend right] (kapsta) (copy1) to [bend left] (E);
 \draw (kapsta) -- (D);
\end{tikzpicture}\\
&=
\begin{tikzpicture}
 \path (0,0) node (A) {}
 ++(0.3,0) coordinate (copy0)
 ++(0.7,0) node[kernel] (mudagmu) {$\mu^*\mu$}
 ++(0.7,0) coordinate (copy1)
 ++(0.5,0) node[kernel] (mu) {$\mu$}
 ++(0.5,0) node[kernel] (kapsta0) {$\kappa^*$}
 ++(0.5,0) node[kernel] (kap) {$\kappa$}
 ++(0.4,0) coordinate (copy2)
 ++(0.5,-0.5) node[kernel] (kapsta1) {$\kappa^*$}
 +(1,0.5) node (E) {}
 +(1,0) node (D) {}
 +(1,1	) node (T) {};
 \draw (A)--(mudagmu) -- (mu) -- (kapsta0) -- (kap) -- (copy2);
 \draw (copy0) to [bend left] (T);
 \draw (copy1) to [bend left = 40] (kap);
 \draw (copy2) to [bend right] (kapsta1) (copy2) to [bend left] (E);
 \draw (kapsta1) -- (D);
\end{tikzpicture}\\
&\overset{\ref{eq:general_kapdag}}{=} 
\begin{tikzpicture}
 \path (0,0) node (A) {}
 ++(0.3,0) coordinate (copy0)
 ++(0.7,0) node[kernel] (mudagmu) {$\mu^*\mu$}
 ++(0.7,0) coordinate (copy1)
 ++(0.5,0) node[kernel] (mu) {$\mu$}
 ++(0.5,0) node[kernel] (kapsta0) {$\kappa^*$}
 ++(0.4,0) coordinate (copy2)
 ++(0.5,0) node[kernel] (kap) {$\kappa$}
 +(1,0) node (E) {}
 +(1,-0.5) node (D) {}
 +(1,0.5) node (T) {};
 \draw (A)--(mudagmu) -- (mu) -- (kapsta0) -- (kap) -- (copy2);
 \draw (copy0) to [bend left] (T);
 \draw (copy1) to [bend left =40] (kap);
 \draw (copy2) to [bend right] (D) (kap) -- (E);
\end{tikzpicture}\\
&\overset{\ref{eq:rightinverse_commute}}{=}
\begin{tikzpicture}
 \path (0,0) node (A) {}
 ++(0.3,0) coordinate (copy0)
 ++(0.7,0) node[kernel] (mudagmu) {$\mu$}
 ++(0.7,0) coordinate (copy1)
 +(0.5,0.5) node[kernel] (dagmu) {$^*\mu$}
 ++(0.5,0) node[kernel] (kapsta0) {$\kappa^*$}
 ++(0.4,0) coordinate (copy2)
 ++(0.5,0) node[kernel] (kap) {$\kappa$}
 +(1,0) node (E) {}
 +(1,-0.5) node (D) {}
 +(1,1) node (T) {};
 \draw (A)--(mudagmu) -- (kapsta0) -- (kap) -- (copy2);
 \draw (copy0) to [bend left] (T);
 \draw (copy1) to [bend left] (dagmu);
 \draw (dagmu) to [bend left] (kap);
 \draw (copy2) to [bend right] (D) (kap) -- (E);
\end{tikzpicture}\\
&\overset{\ref{eq:starmu}}{=}
\begin{tikzpicture}
 \path (0,0) node (A) {}
 ++(0.3,0) coordinate (copy0)
 +(0.9,0.5) node[kernel] (mu) {$\mu$}
 +(1.5,0.5) node[kernel] (dagmu) {$^*\mu$}
 ++(1.4,0) node[kernel] (kapsta0) {$\mu \kappa^*$}
 ++(0.4,0) coordinate (copy2)
 ++(0.5,0) node[kernel] (kap) {$\kappa$}
 +(1,0) node (E) {}
 +(1,-0.5) node (D) {}
 +(1,1) node (T) {};
 \draw (A)-- (kapsta0) -- (kap) -- (copy2);
 \draw (copy0) to [bend left=40] (T);
 \draw (copy0) to [bend left] (mu);
 \draw (mu) -- (dagmu) to [bend left] (kap);
 \draw (copy2) to [bend right] (D) (kap) -- (E);
\end{tikzpicture}\label{eq:skipmu}
\end{align}

Equation \ref{eq:skipmu} implies that, given any $\xi\in\Delta(\mathscr{T})$, all distributions of the form

\begin{align}
\begin{tikzpicture}
 \path (0,0) node[dist] (A) {$\xi$}
 ++(0.3,0) coordinate (copy0)
 ++(0.5,0) node[kernel] (mu) {$\mu$}
 ++(1,0) node[kernel] (kapdag) {$\kappa^*$}
 ++(0.4,0) coordinate (copy1)
 ++(0.5,0) node[kernel] (kap) {$\kappa$}
 +(1,0) node (E) {$\RV{E}$}
 +(1,-0.5) node (D) {$\RV{D}$}
 +(1,0.5) node (T) {$\RV{T}$};
 \draw (A)--(mu) -- (kapdag) -- (kap) -- (E);
 \draw (copy0) to [bend left] (kap) (copy0) to [bend left] (T);
 \draw (copy1) to [bend right] (D);\label{eq:reference_dist}
\end{tikzpicture}
\end{align}

admit both \begin{tikzpicture}
\path (0,0) node[kernel] (kap) {$\kappa^{\,}$};
\draw ($(kap.west) +(-0.5,0.15)$) -- ($(kap.west) +(0,0.15)$) ($(kap.west) +(-0.5,-0.15)$) -- ($(kap.west) +(0,-0.15)$);
\draw (kap) -- (0.5,0);
\end{tikzpicture} and \begin{tikzpicture}
\path (0,0) node[kernel] (kap) {$\kappa^{\,}$}
 + (-0.8,0.15) node[kernel] (mdm) {$\mu^*\mu$};
\draw ($(kap.west) +(-1.25,-0.15)$) -- ($(kap.west) +(0,-0.15)$);
\draw (kap) -- (0.5,0);
\draw ($(mdm.west)+(-0.2,0)$) -- (mdm) -- ($(kap.west) +(0,0.15)$);
\end{tikzpicture} as disintegrations from $(\RV{D},\RV{T})\dashrightarrow\RV{E}$. Therefore these two kernels agree almost surely with respect to the distribution \ref{eq:reference_dist} for any prior $\xi$. 

However, also by assumption \ref{eq:starmu}, we have that for $\theta,\theta'\in \mathscr{T}$  either $\mu(\theta;A)=\mu(\theta';A)$ for all $A\in \mathcal{E}$, or for any $A\in\mathcal{E}$ $\mu(\theta;A)=0$ or $\mu(\theta';A)=0$. That is, any two states either have the same probability measure or probability measures with disjoint support. This is problematic, as the distribution \ref{eq:reference_dist} then has no support over much of the space $D\times E\times \mathscr{T}$. If $\mu$ were deterministic, for example, and hence associated with some function $f$, while \ref{eq:starmu} would be guaranteed via a left inverse, \ref{eq:reference_dist} would be supported on a subset of $D\times\{(\theta,f(\theta))|\theta\in\mathscr{T}\}$. In particular, we have no guarantee that the desired equality of kernels holds if we take any decision that doesn't reproduce the observed distribution. This isn't totally trivial: we may live in a world where most actions make things worse, in which case knowing how to keep things the same is valuable.

A stronger result can be found if we assume we have an infinite sequence of RVs $\RV{X}_i:E\to W$ and $\RV{D}_i:D\to V$ such that
\begin{itemize}
	\item $W^\mathbb{N}=E$, $V^\mathbb{N}=D$ (i.e. the sequence of all $\RV{X}_i$'s is identified with $E$ and the sequence of all $\RV{D}_i$'s is identified with $D$)
 	\item For $A:=A_0\times A_1\times ... \in E$, $\mu = \splitter{0.1} \otimes_{i\in \mathbb{N}} \mu F_{\RV{X}_i}$ (the $\RV{X}_i$'s are IID)
 	\item For $y:=(y_0,y_1,...)\in D$, $A\in E$, there exists $\kappa_0$ such that $\kappa = \splitter{0.1}\otimes_{i\in\mathbb{N}} F_{\RV{D}_i} \kappa_0 F_{\RV{X}_i}$ ($\kappa$ is ``IID'')
 \end{itemize} the sequence is IID with respect to $\mu(\theta;\cdot)$ for all $\theta$.


We appear to require additional assumptions in order to support a nontrivial result, however. Given that $\mu$ is deterministic, we might suppose $\kappa^*$ is also likely to be deterministic (or, if it is not, the nondeterminism is not carried through by $\kappa$). Then we will have only a single pair in $D\times \mathscr{T}$ having nonzero measure for each $\theta \in \mathscr{T}$, so the ``almost surely'' condition rules out almost all feasible decision functions. There appear to be two competing demands - we want $\kappa^*$ stochastic in order to determine the results of a wide variety of decision functions, but we want $\mu$ deterministic in order to support statistical inference. One option might be to relax the assumption of determinism on $\mu$ slightly and hope that we ``gain more than we lose''. I suspect, at this point, that this does not work.

An alternative nontrivial case of ``optimisability'' requires the additional assumption of \emph{double exchangeability}. This is exchangeability in the standard statistical sense, not in the sense of the Rubin causal model; a doubly exchangeable kernel is a kernel that remains the same if inputs and outputs are permuted in the same way.

\begin{definition}[Double exchangeability]
A kernel $\kappa:X\to \Delta(\mathcal{Y})$ is \emph{doubly exchangeable} with respect to random variable sets $\{\RV{X}_i\}_{i\in A}$, $\{\RV{Y}_i\}_{i\in A}$ where $A=[n]$ or $A=\mathbb{N}$ and $\RV{X}_i:X\to X_i$, $\RV{Y}_i:Y\to Y_i$ if, given any finite permutation $\sigma$ and its inverse $\sigma^{-1}$ we have both
\begin{itemize}
	\item There exists $\sigma_X:X\to X$ and $\sigma^{-1}_Y:Y\to Y$ such that $F_{\sigma_X}\splitter{0.1}(\otimes_{i\in A} \RV{X}_{a_i}) = \splitter{0.1}(\otimes_{i\in A} \RV{X}_{\sigma(a_i)})$ and similarly for $F_{\sigma^{-1}_Y}$ and
	\item $F_{\sigma_X}\kappa F_{\sigma^{-1}_Y} = \kappa$
\end{itemize}

Double exchangeability is similar to exchangeability for probability distributions, but there is no possible analogoue of the set $\{\RV{X}_i\}$ in that case.
\end{definition}


An example follows. We identify $\mathscr{T}\cong[0,1]\times T$, $E\cong [0,1]^2$ and $D\cong\{0,1\}^{\mathbb{N}}$. For $(\theta,\phi)\in \mathscr{T}$ let $\mu:(\theta;A\times B)\mapsto \delta_{0.5}(A)\delta_{\frac{\theta}{2}}(B)$.

Define $\overline{\RV{D}}^n:\{0,1\}^n\to [0,1]$ by $\overline{\RV{D}}^n:(y_0,...,y_n)\mapsto \frac{1}{n} \sum_{i\in [n]} y_i$ and let $\overline{\RV{D}}:D\to [0,1]$ be the limit $\overline{\RV{D}}=\lim_{n\to\infty} \overline{\RV{D}}^n$. Let $\gamma$ be for all $(\theta,\phi)$ the unique distribution such that $\gamma F_{\overline{\RV{D}}^n} (\theta,\phi;A) = \delta_{0.5}(A)$ (i.e. the distribtion of an infinite sequence of IID RVs with success probability $0.5$) and assert that  $\delta_{(\theta,\phi)} \splitter{0.1}(\text{Id}_{\mathscr{T}}\otimes \gamma)\kappa = \delta_\theta \mu$ almost surely for all $(\theta,\phi)$.  We note that $\kappa^*:(_;A)\mapsto \gamma(\cdot;A)$ satisfies \ref{eq:general_kapdag} for arbitrary $\kappa$.



% From \ref{eq:gamkap_2_mu} and \ref{eq:kappa_cancel} we have

% \begin{align}
% \begin{tikzpicture}
% 	\path (0,0) coordinate (A)
% 	+  (0,0.15) coordinate (copy0)
% 	++ (0,0.5) node[kernel] (mu) {$\mu$}
% 	++ (0,0.5) node[kernel] (kapdag) {$\kappa^{\dagger}$}
% 	++ (0,0.5) coordinate (copy1)
% 	+  (0.6,0.5) node[kernel] (kap) {$\kappa$}
% 	+  (-0.5,0.5) coordinate (B) 
% 	+  (0.6	,1) node (E) {$\RV{E}$}
% 	+  (-0.5,1) node (D) {$\RV{D}$};
% 	\draw (A) -- (mu);
% 	\draw (mu) -- (kapdag);
% 	\draw (kapdag) -- (copy1);
% 	\draw (copy1) to [bend left] (B);
% 	\draw (copy1) to [bend right]  (kap);
% 	\draw (kap) to (E);
% 	\draw (B) to (D);
% 	\draw (copy0) to [bend right] (kap);
% \end{tikzpicture} &= 
% \begin{tikzpicture}
% 	\path (0,0) coordinate (A)
% 	++ (0,0.25) coordinate (copy0)
% 	++ (0,0.5) node[kernel] (gam) {$\gamma$}
% 	++ (0,0.5) coordinate (copy1)
% 	+ (0.5,0.5) node[kernel] (kap) {$\kappa$}
% 	+ (-0.5,0.5) coordinate (B)
% 	+ (0.5,1) node (E) {$\RV{E}$}
% 	+ (-0.5,1) node (D) {$\RV{D}$};
% 	\draw (A) -- (copy0);
% 	\draw (copy0) -- (gam);
% 	\draw (copy0) to [bend right] (kap);
% 	\draw (gam) -- (copy1);
% 	\draw (copy1) to [bend left] (B);
% 	\draw (copy1) to [bend right] (kap);
% 	\draw (kap) to (E);
% 	\draw (B) to (D);
% \end{tikzpicture}
% \end{align}

% Note that, given some prior $\xi\in \Delta(\mathscr{T})$, the ``Bayes result'' of invoking some decision rule $J:E\to \Delta(\mathcal{D})$ is given by

% \begin{align}
% \begin{tikzpicture}
%  \path (0,0) node[dist] (A) {$\xi$}
%  ++(0,0.3) coordinate (copy0)
%  ++(0,0.5) node[kernel] (mu) {$\mu$}
%  ++(0,0.5) node[kernel] (kapdag) {$J$}
%  ++(0,0.7) node[kernel] (kap) {$\kappa$}
%  ++(0,0.5) node (E) {$\RV{E}$};
%  \draw (A)--(mu) -- (kapdag) -- (kap) -- (E);
%  \draw (copy0) to [bend right = 45] (kap);
% \end{tikzpicture}
% \end{align}


% Given these two assumptions, we have
% \begin{align}
% 	\begin{tikzpicture}[scale=0.8]
% 		\path (0,0) node[dist] (A) {$\mu$}
% 		++(0,1) coordinate (C)
% 		+(1,2) node (F) {$*$}
% 		++(-1,1) node[kernel] (E) {$\kappa^*$}
% 		++(0,1) coordinate (G)
% 		+(1,0.5) node[kernel] (D) {$\kappa$}
% 		+(0,1) coordinate (H)
% 		+(1,1) coordinate (I);
% 		\draw (A) -- (C);
% 		\draw (C) to [bend right] (F.center);
% 		\draw (C) to [bend left] (E);
% 		\draw (E) -- (G);
% 		\draw (G) -- (H);
% 		\draw (G) to [bend right] (D);
% 		\draw (D) to (I);
% 	\end{tikzpicture}&=
% 		\begin{tikzpicture}[scale=0.8]
% 		\path (0,0) node[dist] (A) {$\mu$}
% 		++(0,1) node[kernel] (E) {$\kappa^*$}
% 		++(0,0.5) coordinate (G)
% 		+(1,0.5) node[kernel] (D) {$\kappa$}
% 		+(-1,1) coordinate (H)
% 		+(1,1) coordinate (I);
% 		\draw (A) -- (E);
% 		\draw (E) -- (G);
% 		\draw (G) to [bend left] (H);
% 		\draw (G) to [bend right] (D);
% 		\draw (D) to (I);
% 	\end{tikzpicture}\\
% 	&=
% 		\begin{tikzpicture}[scale=0.8]
% 		\path (0,0) node[dist] (A) {$\mu$}
% 		++(0,1) coordinate (G)
% 		+(1,1) node[kernel] (B) {$\kappa^*$}
% 		+(-1,1) node[kernel] (C) {$\kappa^*$}
% 		+(1,2) node[kernel] (D) {$\kappa$}
% 		+(-1,3) coordinate (H)
% 		+(1,3) coordinate (I);
% 		\draw (A) -- (G);
% 		\draw (G) to [bend left] (C);
% 		\draw (G) to [bend right] (B);
% 		\draw (C) to (H);
% 		\draw (B) to (D);
% 		\draw (D) to (I);
% 	\end{tikzpicture}\\
% 	&=
% 	\begin{tikzpicture}[scale=0.8]
% 		\path (0,0) node[dist] (A) {$\mu$}
% 		++(0,1) coordinate (B) 
% 		+(1,2) coordinate (C)
% 		+(-1,1) node[kernel] (D) {$\kappa^*$}
% 		+(-1,2) coordinate (E);
% 		\draw (A) -- (B);
% 		\draw (B) to [bend right] (C);
% 		\draw (B) to [bend left] (D);
% 		\draw (D) -- (E);
% 	\end{tikzpicture}\\
% 	&:= \mu^{\dagger|}
% \end{align}

% Here we recall the definition of a conditional probability: a conditional probability \inkernel{\RV{Y}|\RV{X}}{}{} with respect to $\nu\in \Delta(\mathcal{E})$ and RVs $\RV{X}:E\to X$, $\RV{Y}:E\to Y$ is any Markov kernel $X\to \Delta(\mathcal{Y})$ such that
% \begin{align}
% 	\begin{tikzpicture}[scale=0.7]
% 	\path (0,0) node[dist,inner sep=2pt] (A) {$\nu$}
% 	+ (0.5,1) node (B) {$X$}
% 	+ (-0.5,1) node (C) {$Y$};
% 	\draw (B) -- (B|- A.north);
% 	\draw (C) -- (C|- A.north);
% 	\end{tikzpicture} = 
% 	\begin{tikzpicture}[scale=0.7]
% 	\path (0,0) node[dist,inner sep=2pt] (A) {$\nu$}
% 	+ (-0.5,1) node (C) {$*$}
% 	++ (0.5,1) coordinate (B)
% 	+ (-1,1) node[kernel] (D) {$\RV{Y}|\RV{X}$}
% 	+(0,2) node (E) {$X$}
% 	+(-1,2) node (F) {$Y$};
% 	\draw (B) -- (B|- A.north);
% 	\draw (C.center) -- (C|- A.north);
% 	\draw (B) -- (E);
% 	\draw (B) to [bend left] (D);
% 	\draw (D) -- (F);
% 	\end{tikzpicture}
% \end{align}

% Thus we can see that $\kappa$ is a conditional probability \inkernel{\RV{E}|\RV{D}}{}{} with respect to $\mu^{\dagger|}$ the RVs $\RV{D}:E\times D\to D$ and $\RV{E}:E\times D\to E$ given by the respective projections.

% This result is in line with two familiar cases from the causal literature: an \emph{ignorable} treatment in potential outcomes or a graphical model with no back-door paths between $\RV{D}$ and $\RV{E}$ both imply that the theory's respective version of ``causal effect'' can be determined directly from the conditional probability. This suggests that reproducibility is in fact quite a strong condition, as it is the exception rather than the norm where causal effects can be determined directly from conditional probabilities. There are two important caveats here, however: firstly we've assumed $\kappa$ has a known right-inverse $\kappa^*$, and secondly this result only allows $\kappa$ to be determined $\mu^{\dagger|}$-almost surely.

% \todo[inline]{A notion of ``approximate right-invertibility'' would be very helpful here. Something like $\kappa$ is approximately right invertible if there is some $\tilde{\kappa}^\dagger$ such that $\gamma \kappa\tilde{\kappa}^\dagger$ is always ``close'' to $\gamma$. Intuitively, a consequence that is \emph{not} approximately right invertible has a channel capacity of 0, and \textbf{Conjecture:} a set of kernels that do not collectively admit some approximate right-inverse do not admit preferences between decisions under the minimax rule}

% \section{More Complex Cases}

% Suppose we retain the right invertibility of $\kappa$ via some known $\kappa^*$, which is consistent with (for example) hard interventions and counterfactual variable forcing. A more common way that there fails to be a memoryless function $J_\theta$ such that $J_\theta\kappa=\mu$ arises from the fact that we may often regard the consequence as dynamic. Assign an index $\theta\in \Theta$ to each causal state, which we now denote $(\kappa_\theta,\mu_\theta)\in \mathscr{T}$ and let $\kappa^\Theta:\Theta\times D\to \Delta(\mathcal{E})$ be defined by $\kappa^\Theta:(\theta,y;A)\mapsto \kappa_\theta(y;A)$ which we will assume is also a Markov kernel (noting that this intoduces additional restrictions on $\mathscr{T}$ in the general case). We may assume, then, for some state-dependent generating distribution $\xi_\theta\in \Delta(\Theta)$ and some state dependent ``passive decision map'' $K_\theta:\Theta\to \Delta(\mathcal{D})$, we have $\mu_\theta = \xi_\theta\splitter{0.1}(I_{(\Theta)}\otimes K_\theta)\kappa^\Theta$. 

% \indist{a}{b}

% \inkernel{ab}{a}{b}