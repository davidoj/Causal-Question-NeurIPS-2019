%!TEX root = main.tex

\section{Definitions \& Notation}\label{sec:dfin}

We use the following standard notation: $[n]$ refers to the set of natural numbers $\{1,...,n\}$. Sets are ordinary capital letters $X$, $\sigma$-algebras are script letters $\mathcal{X}$ while random variables are sans serif capitals $\RV{X}:\_\to X$. All sets mentioned are understood to be equipped with measures. The calligraphic $\mathcal{G}$ refers to a directed acyclic graph rather than a $\sigma$-algebra. Probability measures are greek letters $\mu,\xi,\gamma$ and stochastic maps are bold capitals $\mathbf{C},\mathbf{H}$. Sets of probability measures or stochastic maps are script capitals: $\mathscr{H}$, $\mathscr{T}$, $\mathscr{J}$. We write the set of all probability measures on $(X,\mathcal{X})$ as $\Delta(\mathcal{X})$. $\delta_x:\mathcal(X)\to [0,1]$ is the probability measure such that $\delta_x(A)=1$ if $x\in A$ and $0$ otherwise.

A stochastic map or Markov kernel is a map $\mathbf{A}:X\to \Delta(\mathcal{Y})$. We write the first argument of a Markov kernel as a subscript; for $x\in X$, $G\in\mathcal{Y}$, $\mathbf{A}_x$ is a probability measure on $X$ and $A_x(G)\in[0,1]$ is the measure of $G$. For $\mathbf{A}$ to be a Markov kernel we also require that the function $x\mapsto A_x(G)$ must be measurable for all $G\in\mathcal{Y}$. For $\mathbf{C}:X\times Y\to \Delta(\mathcal{Z})$  and $x\in X$ we will write $\mathbf{C}_x$ for the ``curried'' map $y\mapsto \mathbf{C}_{x,y}$.

We can use a notation similar to matrix-vector products to represent relationships with Markov kernels. Probability measures $\mu\in \Delta(\mathcal{X})$ can be read as row vectors, Markov kernels as matrices and measurable functions $\RV{T}:Y\to T$ as column vectors. Defining $\mathbf{B}:Y\to \Delta(\mathcal{Z})$ we have $\mu \mathbf{A} (G) = \int \mathbf{A}_x (G) d\mu(x)$, $\mathbf{A}\mathbf{B}_x(H)=\int \mathbf{B}_y(H)d\mathbf{A}_x(y)$ and $\mathbf{A}\RV{T}(x)=\int \RV{T}(y) d\mathbf{A}_x(y)$. The tensor product is $(mathbf{A}\otimes \mathbf{B})_{x,y}(G,H) = \mathbf{A}_x(G)\mathbf{B}_y(H)$ where the product on the left is scalar multiplication. Kernel products are associative and the product of kernels is always a kernel itself \citep{cinlar_probability_2011}. 

Some elaborate constructions are unwieldly in inline product notation. Here we use string diagrams. String diagrams can always be interpreted as a mixture of kernel products and tensor products of Markov kernels, but we introduce kernels with special notation that helps with interpreting the resulting objects. String diagrams are the subject of a coherence theorem: taking a string diagram and applying a planar deformation or any of a number of graphical rules not used here yields a string diagram that represents the same kernel \citep{selinger_survey_2010}. A kernel $\mathbf{A}:X\to \Delta(\mathcal{Y})$ is written as a box with input and output wires, probability measures $\mu\in \Delta(\mathcal{X})$ are written as triangles ``closed on the left'' and measurable functions $\RV{T}:Y\to T$ as triangles ``closed on the right''. For a thorough definition of version of string diagrams used here, see \citet{cho_disintegration_2019}.

\begin{align}
\begin{tikzpicture}
\path (0,0) coordinate (A)
++(0.5,0) node[kernel] (B) {$\mathbf{A}$}
++(0.5,0) coordinate (C);
\draw (A) -- (B) -- (C);
\end{tikzpicture}\qquad
\begin{tikzpicture}
\path (0,0) node[dist] (B) {$\mu$}
++(0.5,0) coordinate (C);
\draw (B) -- (C);
\end{tikzpicture}\qquad
\begin{tikzpicture}
\path (0,0) coordinate (A)
++(0.5,0) node[expectation] (B) {$\RV{T}$};
\draw (A) -- (B);
\end{tikzpicture}
\end{align}

The identity $\mathbf{Id}:X\to \Delta(X)$ is the Markov kernel $x\mapsto \delta_x$, which we represent with a bare wire. The copy map $\splitter{0.1}:X\to \Delta(X\times X)$ is the Markov kernel $x\mapsto \delta_{(x,x)}$. For $\mathbf{A}:X\to \Delta(Y)$ and $\mathbf{B}:X\to \Delta(Z)$, $\splitter{0.1} (A\otimes B)_x  = A_x \otimes B_x$. The discard map $*$ is the Markov kernel $X\to \{\#\}$ given by $x\mapsto \delta_\#$, where $\#$ is some one element set. 

Given $\mu\in\Delta(X),\mathbf{A}:X\to \Delta(Y)$ as before, the joint distribution on $X\times Y$ that might be informally written $P(\RV{X}) P(\RV{Y}|\RV{X})$ is given in string diagram notation on the left of \ref{eq:jdist}. Marginalisation is accomplished with the discard map $*$; hence $\mu\splitter{0.1}(\mathbf{Id}\otimes \mathbf{A}*) = \mu$; this is shown on the right of \ref{eq:jdist}

\begin{align}
\begin{tikzpicture}
\path (0,0) node[dist] (mu) {$\mu$}
++ (1,0) coordinate (copy0)
+ (1.2,0.5) node (X) {$\RV{X}$}
++ (0.5,-0.5) node[kernel] (A) {$\mathbf{A}$}
++(0.7,0) node (Y) {$\RV{Y}$};
\draw (mu)--(copy0);
\draw (copy0) to [bend left] (X);
\draw (copy0) to [bend right] (A) (A) -- (Y);
\end{tikzpicture}\qquad
\begin{tikzpicture}
\path (0,0) node[dist] (mu) {$\mu$}
++ (1,0) coordinate (copy0)
+ (1.2,0.5) node (X) {$\RV{X}$}
++ (0.5,-0.5) node[kernel] (A) {$\mathbf{A}$}
++(0.7,0) node (Y) {};
\draw (mu)--(copy0);
\draw (copy0) to [bend left] (X);
\draw[-{Rays [n=8]}] (copy0) to [bend right] (A) (A) -- (Y);
\end{tikzpicture}=
\begin{tikzpicture}
\path (0,0) node[dist] (MU) {$\mu$}
+ (0.5,0) node (X) {$\RV{X}$};
\draw (MU) -- (X);
\end{tikzpicture}\label{eq:jdist}
\end{align}
