%!TEX root = main.tex

\section{Definitions \& Notation}\label{sec:dfin}

We use the following standard notation: $[n]$ refers to the set of natural numbers $\{1,...,n\}$. Sets are ordinary capital letters $X$, $\sigma$-algebras are script letters $\mathcal{X}$ while random variables are sans serif capitals $\RV{X}:\_\to X$. The calligraphic $\mathcal{G}$ refers to a directed acyclic graph rather than a $\sigma$-algebra. Probability measures are greek letters $\mu,\xi,\gamma$ and stochastic maps are bold capitals $\mathbf{C},\mathbf{H}$. Sets of probability measures or stochastic maps are script capitals: $\mathscr{H}$, $\mathscr{T}$, $\mathscr{J}$. We write the set of all probability measures on $(X,\mathcal{X})$ as $\Delta(\mathcal{X})$ and the set of all stochastic maps $W\to \Delta(X)$ as $\Delta(\mathcal{X})^W$. $\delta_x:\mathcal(X)\to [0,1]$ is the probability measure such that $\delta_x(A)=1$ if $x\in A$ and $0$ otherwise.

If $X$ is a discrete space, probability measures on $X$ are positive row vectors in $\mathbb{R}^{|X|}$ that sum to $1$, and stochastic maps or Markov kernels $X\to \Delta(Y)$ are $|X|\times |Y|$ positive matrices with row sums of 1. Using the standard notion of associative matrix-vector products, given $\mu\in \Delta(X)$ and $\mathbf{A}:X\to \Delta(Y)$, $\mu \mathbf{A}$ is a probability measure on $Y$. Given a random variable (or equivalently, measurable function) $\RV{T}:Y\to Z$, $\mathbf{A} \RV{T}$ is a measurable function $X\to Z$. Given $\mathbf{A}:X\to \Delta(Y)$ and $\mathbf{B}:Y\to \Delta(Z)$, $\mathbf{A}\mathbf{B}$ is a stochastic map $X\to \Delta(Z)$. We can use this same notation for continuous sets $X$ and $Y$, see \citet{cinlar_probability_2011}. 

Write $\mathbf{A}_x$ for the probability measure given by $\delta_x \mathbf{A}$, for $E\subset X$ write $\mathbf{A}_E$ for $\mathbf{A} \mathds{1}_C$ where $\mathds{1}_E$ is the indicator function on $E$, and write $\mathbf{A}(x;E):=A_x(E):=\delta_x A_E$. The tensor product $\mathbf{A}\otimes\mathbf{B}$ is the stochastic map $X\times Y\to \Delta(Y\times Z)$ given by $(x,y)\mapsto \mathbf{A}_x\mathbf{B}_y$.

Product notation is useful for defining composite kernels and probability measures, but sometimes more elaborate constructions are called for. Here we use string diagrams. String diagrams can always be interpreted as a mixture of matrix products and tensor products of Markov kernels, but we introduce kernels with special notation that helps with interpreting the resulting objects. A kernel $\mathbf{A}:X\to \Delta(\mathcal{Y})$ is written $...$, where the input and output wires are associated with the measureable spaces $(X,\mathcal{X})$ and $(Y,\mathcal{Y})$. Probability measures $\mu\in \Delta(\mathcal{X})$ are written $...$ and measurable functions $X\to Y$ are written $...$. For a thorough definition of string diagrams, see \citet{cho_disintegration_2019}.

The identity $\mathrm{Id}_X:X\to \Delta(X)$ is the Markov kernel $x\mapsto \delta_x$, which we represent with a bare wire, leaving the space implicit. The copy map $\splitter{0.1}:X\to \Delta(X\times X)$ is the Markov kernel $x\mapsto \delta_{(x,x)}$. For $\mathbf{A}:X\to \Delta(Y)$ and $\mathbf{B}:X\to \Delta(Z)$, $\splitter{0.1} (A\otimes B) = \sum_{x\in X} A_x \otimes B_x$. The discard map $\stopper{0.1}$ is the Markov kernel $X\to \{*\}$ given by $x\mapsto \delta_*$, where $*$ is a one element set. 

Given $\mu\in\Delta(X),\mathbf{A}:X\to \Delta(Y)$ as before, the joint distribution on $X\times Y$ that might be informally written $P(\RV{X}) P(\RV{Y}|\RV{X})$ is given in string diagram notation as

\begin{align}
\nu:=\begin{tikzpicture}
\path (0,0) node[dist] (mu) {$\mu$}
++ (1,0) coordinate (copy0)
+ (1.2,0.5) node (X) {$\RV{X}$}
++ (0.5,-0.5) node[kernel] (A) {$\mathbf{A}$}
++(0.7,0) node (Y) {$\RV{Y}$};
\draw (mu)--(copy0);
\draw (copy0) to [bend left] (X);
\draw (copy0) to [bend right] (A) (A) -- (Y);
\end{tikzpicture}\label{eq:jdist}
\end{align}

A string diagram such as \ref{eq:jdist} that is ``capped'' on the left by a probability measure defines a probability space where the sample space is the Cartesian product of the output wires, the measurable sets are the tensor product of the output $\sigma$-algebras and the probability measure is given by the composition of measures in the diagram. The projection map $\pi_X:X\times Y\to X$ is thus a measurable function; following this observation, we overload the notation for the random variable $\RV{X}$ to label wires on the diagram; when used as such, it always refers to the projetion map $\pi_X$. While a random variable technically requires a probability space, we also use this convention for string diagrams representing kernels $X\to \Delta(\mathcal{Y})$ for arbitrary $X,Y$ (such diagrams feature ``free'' wires on the left and right). Using this convention, the measurable function referred to by the wire label $\RV{X}$ is always unambiguous, but we need to define a prior $\xi \in \Delta(\mathcal{X})$ in order for it to have a distribution. 

Finally, if we are given a set of kernels $\{\mathbf{A},\mathbf{B}\}$ where $\mathbf{A}:X\to \Delta(\mathcal{Y})$, $\mathbf{B}:W\to \Delta(\mathcal{Z})$ and a composition defining some kernel $$, we can always construct a 

If we regard $\nu$ as a joint distribution of $\RV{X}$ and $\RV{Y}$, the marginal $\nu_{\RV{X}}$ is given by

\begin{align}
\nu:=\begin{tikzpicture}
\path (0,0) node[dist] (mu) {$\mu$}
++ (1,0) coordinate (copy0)
+ (1.2,0.5) node (X) {$\RV{X}$}
++ (0.5,-0.5) node[kernel] (A) {$\mathbf{A}$}
++(0.7,0) node[{-Rays [n=8]}] (Y) {};
\draw (mu)--(copy0);
\draw (copy0) to [bend left] (X);
\draw (copy0) to [bend right] (A) (A) -- (Y);
\end{tikzpicture}\label{eq:jdist}
\end{align}